{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "%reset -f\n",
    "from helpers import load_all_data, vectorized_flatten, sigmoid, get_log_loss, get_accuracy, sigmoid_derivative, gradient_update, get_loss_plot, plot_loss\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "def prep_data(data_path):\n",
    "    '''\n",
    "    --------------------\n",
    "    Perceptron algorithm\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    --------------------\n",
    "    Output: \n",
    "    \n",
    "    w: trained weights\n",
    "    y_hat: predictions\n",
    "    --------------------\n",
    "    '''\n",
    "    # Get datasets\n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_all_data(data_path)\n",
    "    \n",
    "    # Flatten datasets\n",
    "    X_train_flattened = vectorized_flatten(X_train)\n",
    "    X_dev_flattened = vectorized_flatten(X_dev)\n",
    "    X_test_flattened = vectorized_flatten(X_test)\n",
    "    \n",
    "    # Add extra column to Y_train\n",
    "    y_train = y_train.reshape(1, -1)\n",
    "    y_dev = y_dev.reshape(1, -1)\n",
    "    y_test = y_test.reshape(1, -1)\n",
    "    \n",
    "    return(X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(X, Y, epochs, lr, tolerance):\n",
    "    '''\n",
    "    --------------------\n",
    "    Perceptron algorithm\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/-1) training label (shape = n X 1)\n",
    "    --------------------\n",
    "    Output: \n",
    "    w: trained weights\n",
    "    b: trained biases\n",
    "    y_preds: predictions \n",
    "    --------------------\n",
    "    '''\n",
    "    # Initialize weights and biases\n",
    "    w = np.zeros(X.shape[0])\n",
    "    b = 0\n",
    "    \n",
    "    # History goes here\n",
    "    history = {\n",
    "        \"weights\": [w],\n",
    "        \"losses\": [], \n",
    "        \"biases\": [b],\n",
    "        \"accuracies\": []\n",
    "    }\n",
    "    \n",
    "    convergence_counter = 0\n",
    "    best_accuracy = np.inf\n",
    "    \n",
    "    # Run for a fixed number of epochs\n",
    "    for epoch in range(1, epochs + 1): \n",
    "        \n",
    "        # Do this for each example in the dataset\n",
    "        for i in range(X.shape[1]):\n",
    "            \n",
    "            # Store the sample data\n",
    "            x_i = X[:, i]\n",
    "            y_i = Y[0][i]\n",
    "            \n",
    "            # Compute the prediction with the current weights\n",
    "            if (np.dot(w, x_i) + b > 0): y_hat = 1\n",
    "            else: y_hat = -1\n",
    "            \n",
    "            # Check if the prediction is correct against the labels\n",
    "            # If it is correct we don't need to make any updates: we just move to the next iteration\n",
    "            # If it is not correct then we do the following: \n",
    "            # 1) Update the weights and biases in the direction of the label\n",
    "            if y_hat != y_i:\n",
    "                w += lr*(y_i - y_hat)*x_i\n",
    "                b += lr*(y_i - y_hat)\n",
    "            \n",
    "            # Get predictions\n",
    "            y_preds = np.array([int(np.dot(w, X[:, i]) + b  > 0) for i in range(X.shape[1])])\n",
    "        \n",
    "            # Training accuracy                       \n",
    "            accuracy = get_accuracy(Y, y_preds)\n",
    "                \n",
    "            # Check convergence, keeps a counter of how many epochs it has been without an improvement\n",
    "            # Counter resets whenever there's an improvent            \n",
    "            if accuracy < best_accuracy - tolerance:\n",
    "                best_loss = loss\n",
    "                convergence_counter = 0\n",
    "            else:\n",
    "                convergence_counter += 1\n",
    "                \n",
    "                # Append results to history\n",
    "                history[\"biases\"].append(b)\n",
    "                history[\"weights\"].append(w)\n",
    "                history[\"accuracies\"].append(accuracy)\n",
    "        \n",
    "                # Get training accuracy\n",
    "                print(\"Epoch {}/{}: Training_accuracy = {}\".format(epoch, epochs, accuracy))\n",
    "    \n",
    "    # Return statement\n",
    "    return(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_results(history):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Store results\n",
    "    best_epoch = np.array(history[\"losses\"]).argmin()\n",
    "    best_accuracy = history['accuracies'][best_epoch]\n",
    "    best_loss = history['losses'][best_epoch]\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"best accuracy: {history['accuracies'][best_epoch]}\")\n",
    "    print(f\"best loss: {history['losses'][best_epoch]}\")\n",
    "    print(f\"best epoch: {best_epoch}\")\n",
    "    \n",
    "    return(best_epoch, best_accuracy, best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(X_dev, y_dev, history, best_epoch, label=\"dev\"):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    w = history[\"weights\"][best_epoch]\n",
    "    b = history[\"biases\"][best_epoch]\n",
    "    activations = forward_pass(X_dev, w, b)\n",
    "\n",
    "    y_dev_prob = activations[-1]\n",
    "    y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "\n",
    "    loss = get_log_loss(y_dev, y_dev_prob)\n",
    "    accuracy = get_accuracy(y_dev, y_dev_pred)\n",
    "    print(f\"{label} set accuracy: {accuracy}\")\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceptron_predictions(w, b, X, Y): \n",
    "    '''\n",
    "    --------------------\n",
    "    Run perceptron algorithm to get a base-line\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    --------------------\n",
    "    Output: \n",
    "    w: trained weights\n",
    "    y_preds: predictions\n",
    "    --------------------\n",
    "    '''\n",
    "    # Get predictions\n",
    "    y_preds = np.array([int(np.dot(w, X[:, i]) + b  > 0) for i in range(X.shape[1])])\n",
    "\n",
    "    # Return statement\n",
    "    return(y_preds, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceptron_baseline(data_path, epochs):\n",
    "    '''\n",
    "    --------------------\n",
    "    Run perceptron algorithm to get a base-line\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    --------------------\n",
    "    Output: \n",
    "    w: trained weights\n",
    "    y_preds: predictions\n",
    "    --------------------\n",
    "    '''\n",
    "    # Set the random seed for np.random number generator\n",
    "    # This will make sure results are reproducible\n",
    "    np.random.seed(132089)\n",
    "    \n",
    "    # Prepare data for the perceptron\n",
    "    X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test = prep_data(data_path)\n",
    "    \n",
    "    # Call the perceptron training with the given epochs\n",
    "    w, b = train_perceptron(X_train_flattened, y_train, epochs)\n",
    "    \n",
    "    # Get train set performance\n",
    "    train_preds, train_accuracy = get_perceptron_performance(w, b, X_train_flattened, y_train)\n",
    "    \n",
    "    # Get dev set performance\n",
    "    dev_preds, dev_accuracy = get_perceptron_performance(w, b, X_dev_flattened, y_dev)\n",
    "    \n",
    "    # Return statement\n",
    "    return(w, b, train_preds, dev_preds, train_accuracy, dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Training_accuracy = 0.5\n",
      "Epoch 1/100: Training_accuracy = 0.68025\n",
      "Epoch 2/100: Training_accuracy = 0.6936666666666667\n",
      "Epoch 2/100: Training_accuracy = 0.7846666666666666\n",
      "Epoch 3/100: Training_accuracy = 0.837\n",
      "Epoch 3/100: Training_accuracy = 0.8585\n",
      "Epoch 4/100: Training_accuracy = 0.5176666666666667\n",
      "Epoch 4/100: Training_accuracy = 0.8005\n",
      "Epoch 5/100: Training_accuracy = 0.64025\n",
      "Epoch 5/100: Training_accuracy = 0.9178333333333333\n",
      "Epoch 6/100: Training_accuracy = 0.7600833333333333\n",
      "Epoch 6/100: Training_accuracy = 0.8029166666666666\n",
      "Epoch 7/100: Training_accuracy = 0.7224166666666667\n",
      "Epoch 7/100: Training_accuracy = 0.898\n",
      "Epoch 8/100: Training_accuracy = 0.6161666666666666\n",
      "Epoch 8/100: Training_accuracy = 0.8725\n",
      "Epoch 9/100: Training_accuracy = 0.7235833333333334\n",
      "Epoch 9/100: Training_accuracy = 0.6359166666666667\n",
      "Epoch 10/100: Training_accuracy = 0.5351666666666667\n",
      "Epoch 10/100: Training_accuracy = 0.8911666666666667\n",
      "Epoch 11/100: Training_accuracy = 0.698\n",
      "Epoch 11/100: Training_accuracy = 0.8545\n",
      "Epoch 12/100: Training_accuracy = 0.5701666666666667\n",
      "Epoch 12/100: Training_accuracy = 0.8728333333333333\n",
      "Epoch 13/100: Training_accuracy = 0.8358333333333333\n",
      "Epoch 13/100: Training_accuracy = 0.7110833333333333\n",
      "Epoch 14/100: Training_accuracy = 0.51025\n",
      "Epoch 14/100: Training_accuracy = 0.6879166666666666\n",
      "Epoch 15/100: Training_accuracy = 0.9270833333333334\n",
      "Epoch 15/100: Training_accuracy = 0.9098333333333334\n",
      "Epoch 16/100: Training_accuracy = 0.5384166666666667\n",
      "Epoch 16/100: Training_accuracy = 0.8946666666666667\n",
      "Epoch 17/100: Training_accuracy = 0.5271666666666667\n",
      "Epoch 17/100: Training_accuracy = 0.9048333333333334\n",
      "Epoch 18/100: Training_accuracy = 0.5261666666666667\n",
      "Epoch 18/100: Training_accuracy = 0.8843333333333333\n",
      "Epoch 19/100: Training_accuracy = 0.6326666666666667\n",
      "Epoch 19/100: Training_accuracy = 0.9216666666666666\n",
      "Epoch 20/100: Training_accuracy = 0.77325\n",
      "Epoch 20/100: Training_accuracy = 0.9318333333333333\n",
      "Epoch 21/100: Training_accuracy = 0.538\n",
      "Epoch 21/100: Training_accuracy = 0.7665833333333333\n",
      "Epoch 22/100: Training_accuracy = 0.7888333333333334\n",
      "Epoch 22/100: Training_accuracy = 0.9295\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "data_path = '../setup/data'\n",
    "epochs = 100\n",
    "\n",
    "# Call training function\n",
    "w, b, train_preds, dev_preds, train_accuracy, dev_accuracy = get_perceptron_baseline(data_path, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "train_accuracy, dev_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
