{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from helpers import load_all_data, vectorized_flatten, sigmoid, get_log_loss, get_accuracy, sigmoid_derivative, gradient_update, get_loss_plot, plot_loss\n",
    "from helpers import sgd_with_momentum_update\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data_path):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    Use vectorized flatten\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Load\n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_all_data(data_path)\n",
    "    \n",
    "    # Flatten\n",
    "    X_train_flattened = vectorized_flatten(X_train)\n",
    "    X_dev_flattened = vectorized_flatten(X_dev)\n",
    "    X_test_flattened = vectorized_flatten(X_test)\n",
    "    \n",
    "    # Reshape labels\n",
    "    y_train = y_train.reshape(1, -1)\n",
    "    y_dev = y_dev.reshape(1, -1)\n",
    "    y_test = y_test.reshape(1, -1)\n",
    "    \n",
    "    # Return\n",
    "    return(X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X): \n",
    "    '''\n",
    "    --------------------\n",
    "    Parameter Initialization\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X [n = 12000])\n",
    "    --------------------\n",
    "    Output: \n",
    "    weights: Weight terms initialized as random normals\n",
    "    biases: Bias terms initialized to zero\n",
    "    --------------------\n",
    "    '''\n",
    "    dim1 = 1/np.sqrt(X.shape[0])\n",
    "    W1 = dim1 * np.random.randn(h1, 28**2)\n",
    "    \n",
    "    dim2 = 1/np.sqrt(W1.shape[1])\n",
    "    W2 = dim2 * np.random.randn(1, h1)\n",
    "    \n",
    "    b1 = np.zeros((h1, 1))\n",
    "    b2 = np.zeros((1, 1))\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, weights, biases):\n",
    "    '''\n",
    "    ----------------------------------\n",
    "    Forward propogation:\n",
    "    Send inputs through the network to\n",
    "    generate output\n",
    "    ----------------------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    weights: Binary (1/0) training label (shape = n X 1)\n",
    "    biases:\n",
    "    --------------------\n",
    "    Output: \n",
    "    activations: vector of results from passing\n",
    "    inputs through each neuron\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    z1 = W1 @ X + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    activations = (z1, a1, z2, a2)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, weights, biases, activations):\n",
    "    '''\n",
    "    --------------------\n",
    "    Backpropagation\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    activations: Current set of activations\n",
    "    --------------------\n",
    "    Output: \n",
    "    Derivatives required\n",
    "    for optimization update\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    z1, a1, z2, a2 = activations\n",
    "    m = y.shape[1]\n",
    "    #print(m)   \n",
    "    dz2 = a2 - y\n",
    "    #print(\"dz3\", dz3.shape)\n",
    "    \n",
    "   \n",
    "    dW2 = np.dot(dz2, a1.T)/m\n",
    "    #print(\"dW2\", dW2.shape)\n",
    "    \n",
    "    db2 = np.sum(dz2, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db2\", db2.shape)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    #print(\"da1\", da1.shape)\n",
    "    \n",
    "    dz1 = da1 * sigmoid_derivative(z1)\n",
    "    #print(\"dz1\", dz1.shape)\n",
    "    \n",
    "    dW1 = np.dot(dz1, X.T)/m\n",
    "    #print(\"dW1\", dW1.shape)\n",
    "    \n",
    "    db1 = np.sum(dz1, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db1\", db1.shape)\n",
    "    \n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    '''\n",
    "    --------------------\n",
    "    Update parameters\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    \n",
    "    W1 = gradient_update(W1, learning_rate, dW1)\n",
    "    W2 = gradient_update(W2, learning_rate, dW2)\n",
    "   \n",
    "    b1 = gradient_update(b1, learning_rate, db1)\n",
    "    b2 = gradient_update(b2, learning_rate, db2)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(weights,biases,gradients,learning_rate,velocity,momentum):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    vw1,vw2,vb1,vb2 = velocity\n",
    "    W1,vw1 = sgd_with_momentum_update(W1, learning_rate, dW1,vw1,momentum)\n",
    "    W2,vw2 = sgd_with_momentum_update(W2, learning_rate, dW2,vw2,momentum)\n",
    "   \n",
    "    b1,vb1 = sgd_with_momentum_update(b1, learning_rate, db1,vb1,momentum)\n",
    "    b2,vb2 = sgd_with_momentum_update(b2, learning_rate, db2,vb2,momentum)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    velocity = (vw1,vw2,vb1,vb2)\n",
    "    return weights ,biases,velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training_without_momentum(batch_size, weights, biases, epochs, lr, X, y):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    history = {\n",
    "    \"weights\": [weights],\n",
    "    \"losses\": [], \n",
    "    \"biases\": [biases],\n",
    "    \"accuracies\": []\n",
    "    \n",
    "    }\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        offset = 0\n",
    "        weights = history['weights'][epoch]\n",
    "        biases = history['biases'][epoch]\n",
    "        \n",
    "        while offset < max(y.shape):\n",
    "            \n",
    "            if offset%1000==0 :\n",
    "                print(\"epoch :\",epoch,\" batch:\",offset)\n",
    "            else :\n",
    "                a=1\n",
    "            if offset+batch_size >=max(y.shape):\n",
    "                X_batch = X[:,offset:]\n",
    "                y_batch = y[:,offset:]\n",
    "            else :    \n",
    "                X_batch = X[:,offset:offset+batch_size]\n",
    "                y_batch = y[:,offset:offset+batch_size]\n",
    "            \n",
    "            \n",
    "            activations = forward_pass(X_batch, weights, biases)\n",
    "            gradients = backpropagation(X_batch, y_batch, weights, biases, activations)\n",
    "\n",
    "            weights, biases = update_parameters(weights, biases, gradients, lr)\n",
    "            offset = offset+batch_size\n",
    "        \n",
    "        activations_full = forward_pass(X, weights, biases)\n",
    "        y_prob = activations_full[-1]\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    \n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "\n",
    "        history[\"weights\"].append(weights)\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(biases)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "\n",
    "        if np.isnan(loss):\n",
    "            break\n",
    "\n",
    "        print(\"loss after epoch: \",epoch,\": \",loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(weights,biases):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    vw1 = np.zeros(W1.shape)\n",
    "    vw2 = np.zeros(W2.shape)\n",
    "    vb1 = np.zeros(b1.shape)\n",
    "    vb2 = np.zeros(b2.shape)\n",
    "    return vw1,vw2,vb1,vb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training_with_momentum(batch_size,weights,biases,history,X,y,momentum,lr,X_dev,y_dev):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    \n",
    "    history = {\n",
    "        \"weights\": [weights],\n",
    "        \"losses\": [], \n",
    "        \"biases\": [biases],\n",
    "        \"accuracies\": [],\n",
    "        \"velocity\":[velocity],\n",
    "        \"dev_accuracies\" :[],\n",
    "        \"dev_loss\":[]\n",
    "    }\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        offset = 0\n",
    "        weights = history['weights'][epoch]\n",
    "        biases = history['biases'][epoch]\n",
    "        velocity = history['velocity'][epoch]\n",
    "        \n",
    "        while offset <max(y_train.shape):\n",
    "            if offset%1000==0 :\n",
    "                print(\"epoch :\",epoch,\" batch:\",offset)\n",
    "            else :\n",
    "                a=1\n",
    "            if offset+batch_size >=max(y_train.shape):\n",
    "                X_batch = X[:,offset:]\n",
    "                y_batch = y[:,offset:]\n",
    "            else :    \n",
    "                X_batch = X[:,offset:offset+batch_size]\n",
    "                y_batch = y[:,offset:offset+batch_size]\n",
    "            \n",
    "            activations = forward_pass(X_batch, weights, biases)\n",
    "            gradients = backpropagation(X_batch, y_batch, weights, biases, activations)\n",
    "\n",
    "            \n",
    "            weights, biases,velocity = update_parameters_with_momentum(weights, biases, gradients, \n",
    "                                                                       lr,velocity,momentum)\n",
    "            offset = offset+batch_size\n",
    "        \n",
    "        activations_full = forward_pass(X, weights, biases)\n",
    "        y_prob = activations_full[-1]\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "\n",
    "        #print(y,y_prob)\n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "        \n",
    "        activations_dev = forward_pass(X_dev,weights,biases)\n",
    "        y_dev_prob =  activations_dev[-1]\n",
    "        y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "        loss_dev = get_log_loss(y_dev,y_dev_prob)\n",
    "        accuracy_dev = get_accuracy(y_dev,y_dev_pred)\n",
    "\n",
    "        history[\"weights\"].append(weights)\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(biases)\n",
    "        history[\"velocity\"].append(velocity)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "        history[\"dev_accuracies\"].append(accuracy_dev)\n",
    "        history['dev_loss'].append(loss_dev)\n",
    "\n",
    "        if np.isnan(loss):\n",
    "            break\n",
    "\n",
    "        print(\"loss after epoch: \",epoch,\": \",loss)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_epoch(history):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Store results\n",
    "    best_epoch = np.array(history[\"losses\"]).argmin()\n",
    "    best_accuracy = history['accuracies'][best_epoch]\n",
    "    best_loss = history['losses'][best_epoch]\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"best accuracy: {history['accuracies'][best_epoch]}\")\n",
    "    print(f\"best loss: {history['losses'][best_epoch]}\")\n",
    "    print(f\"best epoch: {best_epoch}\")\n",
    "    \n",
    "    return(best_epoch, best_accuracy, best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(X_dev, y_dev, history, best_epoch, label=\"dev\"):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    w = history[\"weights\"][best_epoch]\n",
    "    b = history[\"biases\"][best_epoch]\n",
    "    activations = forward_pass(X_dev, w, b)\n",
    "\n",
    "    y_dev_prob = activations[-1]\n",
    "    y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "\n",
    "    loss = get_log_loss(y_dev, y_dev_prob)\n",
    "    accuracy = get_accuracy(y_dev, y_dev_pred)\n",
    "    print(f\"{label} set accuracy: {accuracy}\")\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(data_path, epochs, model_name, h1, lr, mode, batch_size):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Set seed for reproducible results\n",
    "    np.random.seed(1252908)\n",
    "    \n",
    "    # Get data\n",
    "    X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test = prep_data(data_path)\n",
    "    \n",
    "    # Assign batch size\n",
    "    # If a batch size parameter is specified we use that otherwise we default to full batch gradient descent\n",
    "    if mode == 'full': batch_size = max(y_train.shape)\n",
    "    elif mode == 'stochastic': batch_size = 1\n",
    "        \n",
    "    print(\"The batch size is...{}\".format(batch_size))\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    weights, biases = initialize(X_train_flattened)\n",
    "    \n",
    "    # Enter training loop\n",
    "    history = batch_training_without_momentum(batch_size, weights, biases, epochs, lr, X_train_flattened, y_train)\n",
    "    \n",
    "    # Get best epoch from history object\n",
    "    best_epoch = get_best_epoch(history)\n",
    "    \n",
    "    # Plot loss and save\n",
    "    plot_loss(\"loss_{}.png\".format(model_name), history[\"losses\"][:-2])\n",
    "    \n",
    "    # Plot accuracy and save\n",
    "    plot_loss(\"accuracy_{}.png\".format(model_name), history[\"accuracies\"][:-2], label='Training Accuracy')\n",
    "    \n",
    "    return(history, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch size is...10\n",
      "epoch : 0  batch: 0\n",
      "epoch : 0  batch: 1000\n",
      "epoch : 0  batch: 2000\n",
      "epoch : 0  batch: 3000\n",
      "epoch : 0  batch: 4000\n",
      "epoch : 0  batch: 5000\n",
      "epoch : 0  batch: 6000\n",
      "epoch : 0  batch: 7000\n",
      "epoch : 0  batch: 8000\n",
      "epoch : 0  batch: 9000\n",
      "epoch : 0  batch: 10000\n",
      "epoch : 0  batch: 11000\n",
      "loss after epoch:  0 :  1447.3387075541198\n",
      "epoch : 1  batch: 0\n",
      "epoch : 1  batch: 1000\n",
      "epoch : 1  batch: 2000\n",
      "epoch : 1  batch: 3000\n",
      "epoch : 1  batch: 4000\n",
      "epoch : 1  batch: 5000\n",
      "epoch : 1  batch: 6000\n",
      "epoch : 1  batch: 7000\n",
      "epoch : 1  batch: 8000\n",
      "epoch : 1  batch: 9000\n",
      "epoch : 1  batch: 10000\n",
      "epoch : 1  batch: 11000\n",
      "loss after epoch:  1 :  1196.5432035333915\n",
      "epoch : 2  batch: 0\n",
      "epoch : 2  batch: 1000\n",
      "epoch : 2  batch: 2000\n",
      "epoch : 2  batch: 3000\n",
      "epoch : 2  batch: 4000\n",
      "epoch : 2  batch: 5000\n",
      "epoch : 2  batch: 6000\n",
      "epoch : 2  batch: 7000\n",
      "epoch : 2  batch: 8000\n",
      "epoch : 2  batch: 9000\n",
      "epoch : 2  batch: 10000\n",
      "epoch : 2  batch: 11000\n",
      "loss after epoch:  2 :  1051.8023216287245\n",
      "epoch : 3  batch: 0\n",
      "epoch : 3  batch: 1000\n",
      "epoch : 3  batch: 2000\n",
      "epoch : 3  batch: 3000\n",
      "epoch : 3  batch: 4000\n",
      "epoch : 3  batch: 5000\n",
      "epoch : 3  batch: 6000\n",
      "epoch : 3  batch: 7000\n",
      "epoch : 3  batch: 8000\n",
      "epoch : 3  batch: 9000\n",
      "epoch : 3  batch: 10000\n",
      "epoch : 3  batch: 11000\n",
      "loss after epoch:  3 :  953.9101292244763\n",
      "epoch : 4  batch: 0\n",
      "epoch : 4  batch: 1000\n",
      "epoch : 4  batch: 2000\n",
      "epoch : 4  batch: 3000\n",
      "epoch : 4  batch: 4000\n",
      "epoch : 4  batch: 5000\n",
      "epoch : 4  batch: 6000\n",
      "epoch : 4  batch: 7000\n",
      "epoch : 4  batch: 8000\n",
      "epoch : 4  batch: 9000\n",
      "epoch : 4  batch: 10000\n",
      "epoch : 4  batch: 11000\n",
      "loss after epoch:  4 :  875.9786132260431\n",
      "epoch : 5  batch: 0\n",
      "epoch : 5  batch: 1000\n",
      "epoch : 5  batch: 2000\n",
      "epoch : 5  batch: 3000\n",
      "epoch : 5  batch: 4000\n",
      "epoch : 5  batch: 5000\n",
      "epoch : 5  batch: 6000\n",
      "epoch : 5  batch: 7000\n",
      "epoch : 5  batch: 8000\n",
      "epoch : 5  batch: 9000\n",
      "epoch : 5  batch: 10000\n",
      "epoch : 5  batch: 11000\n",
      "loss after epoch:  5 :  808.2530944322718\n",
      "epoch : 6  batch: 0\n",
      "epoch : 6  batch: 1000\n",
      "epoch : 6  batch: 2000\n",
      "epoch : 6  batch: 3000\n",
      "epoch : 6  batch: 4000\n",
      "epoch : 6  batch: 5000\n",
      "epoch : 6  batch: 6000\n",
      "epoch : 6  batch: 7000\n",
      "epoch : 6  batch: 8000\n",
      "epoch : 6  batch: 9000\n",
      "epoch : 6  batch: 10000\n",
      "epoch : 6  batch: 11000\n",
      "loss after epoch:  6 :  748.684550848272\n",
      "epoch : 7  batch: 0\n",
      "epoch : 7  batch: 1000\n",
      "epoch : 7  batch: 2000\n",
      "epoch : 7  batch: 3000\n",
      "epoch : 7  batch: 4000\n",
      "epoch : 7  batch: 5000\n",
      "epoch : 7  batch: 6000\n",
      "epoch : 7  batch: 7000\n",
      "epoch : 7  batch: 8000\n",
      "epoch : 7  batch: 9000\n",
      "epoch : 7  batch: 10000\n",
      "epoch : 7  batch: 11000\n",
      "loss after epoch:  7 :  696.5496019621673\n",
      "epoch : 8  batch: 0\n",
      "epoch : 8  batch: 1000\n",
      "epoch : 8  batch: 2000\n",
      "epoch : 8  batch: 3000\n",
      "epoch : 8  batch: 4000\n",
      "epoch : 8  batch: 5000\n",
      "epoch : 8  batch: 6000\n",
      "epoch : 8  batch: 7000\n",
      "epoch : 8  batch: 8000\n",
      "epoch : 8  batch: 9000\n",
      "epoch : 8  batch: 10000\n",
      "epoch : 8  batch: 11000\n",
      "loss after epoch:  8 :  651.1282774930796\n",
      "epoch : 9  batch: 0\n",
      "epoch : 9  batch: 1000\n",
      "epoch : 9  batch: 2000\n",
      "epoch : 9  batch: 3000\n",
      "epoch : 9  batch: 4000\n",
      "epoch : 9  batch: 5000\n",
      "epoch : 9  batch: 6000\n",
      "epoch : 9  batch: 7000\n",
      "epoch : 9  batch: 8000\n",
      "epoch : 9  batch: 9000\n",
      "epoch : 9  batch: 10000\n",
      "epoch : 9  batch: 11000\n",
      "loss after epoch:  9 :  611.4642097273997\n",
      "best accuracy: 0.98475\n",
      "best loss: 611.4642097273997\n",
      "best epoch: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'weights': [(array([[ 0.01619872,  0.01801601, -0.03417228, ..., -0.04823096,\n",
       "            -0.05604477,  0.00901734],\n",
       "           [ 0.01149911,  0.01923254, -0.02097324, ..., -0.03188668,\n",
       "            -0.00640128,  0.07425657],\n",
       "           [ 0.01154735,  0.01350113, -0.00265027, ...,  0.09964181,\n",
       "             0.01167598,  0.00294111],\n",
       "           ...,\n",
       "           [-0.00108249,  0.00230341,  0.02563485, ...,  0.0009959 ,\n",
       "             0.02456301,  0.01430508],\n",
       "           [ 0.01832175,  0.00118309, -0.03305243, ..., -0.005914  ,\n",
       "            -0.03218962,  0.07038516],\n",
       "           [-0.00086598,  0.05116875,  0.08850713, ...,  0.01915647,\n",
       "             0.02992496,  0.02984515]]),\n",
       "    array([[-0.01311009, -0.06386502, -0.01692045, -0.00956199, -0.03516135,\n",
       "             0.02304771,  0.00525049, -0.02400982,  0.02400189,  0.09434391,\n",
       "             0.03044473, -0.02063809, -0.01385631, -0.08354234, -0.01564368,\n",
       "            -0.01535444, -0.0118825 ,  0.03852521,  0.01543301, -0.01806274,\n",
       "            -0.03375803, -0.01176581,  0.02887136,  0.02203382,  0.04440253,\n",
       "            -0.03918601, -0.0066844 ,  0.00180321, -0.00964843,  0.034028  ,\n",
       "             0.01684728,  0.0255217 ]])),\n",
       "   (array([[ 0.01575901,  0.01757666, -0.03460951, ..., -0.0485287 ,\n",
       "            -0.05642938,  0.00857232],\n",
       "           [ 0.02658609,  0.03431295, -0.00621541, ..., -0.020943  ,\n",
       "             0.00806089,  0.08934329],\n",
       "           [ 0.02236454,  0.02431261,  0.0079228 , ...,  0.10740434,\n",
       "             0.02224009,  0.01377471],\n",
       "           ...,\n",
       "           [-0.01085327, -0.007465  ,  0.01604719, ..., -0.00593835,\n",
       "             0.01499669,  0.00451382],\n",
       "           [-0.00229552, -0.01944235, -0.05323242, ..., -0.02141583,\n",
       "            -0.05299966,  0.04970486],\n",
       "           [-0.01319883,  0.03884354,  0.07642818, ...,  0.0104481 ,\n",
       "             0.01815686,  0.0174917 ]]),\n",
       "    array([[ 0.04143581, -0.71502365, -0.55485902,  0.38109416, -0.94675148,\n",
       "             1.07674897,  0.645065  , -0.42582476, -0.21929713,  1.14266469,\n",
       "             0.81228712, -0.28902002, -0.83725451, -0.963758  , -0.64970804,\n",
       "            -1.01885506, -0.10724743, -0.0548257 , -0.14203362, -0.77204456,\n",
       "            -0.94533817, -0.43562099, -0.20932236,  0.42424458,  0.88248564,\n",
       "            -0.79171264,  0.4036528 , -0.21637055, -0.5226332 ,  0.56285997,\n",
       "             1.09039363,  0.63216618]])),\n",
       "   (array([[ 0.01517277,  0.0169898 , -0.0351949 , ..., -0.04889089,\n",
       "            -0.05699908,  0.00797917],\n",
       "           [ 0.03766142,  0.04539205,  0.00474894, ..., -0.01416014,\n",
       "             0.01899416,  0.10060954],\n",
       "           [ 0.02997489,  0.03192413,  0.01548602, ...,  0.11246143,\n",
       "             0.03011024,  0.02151896],\n",
       "           ...,\n",
       "           [-0.01821874, -0.01483236,  0.008719  , ..., -0.01079977,\n",
       "             0.00726789, -0.00300873],\n",
       "           [-0.01090292, -0.0280632 , -0.06183696, ..., -0.02568768,\n",
       "            -0.06155143,  0.04097579],\n",
       "           [-0.02308531,  0.02895016,  0.0666074 , ...,  0.00458594,\n",
       "             0.0084163 ,  0.00740784]]),\n",
       "    array([[ 0.05315751, -0.78211967, -0.65024533,  0.44776125, -1.06078059,\n",
       "             1.1936745 ,  0.73828546, -0.48209467, -0.25287467,  1.31528184,\n",
       "             0.92547885, -0.3600242 , -0.93213832, -1.07052037, -0.74355714,\n",
       "            -1.18607973, -0.13903833, -0.06958819, -0.15821909, -0.95370995,\n",
       "            -1.06350076, -0.46898061, -0.25746798,  0.5101211 ,  1.04796251,\n",
       "            -0.89559086,  0.45506631, -0.26502868, -0.5875458 ,  0.68574945,\n",
       "             1.28440937,  0.70349797]])),\n",
       "   (array([[ 0.01458344,  0.01639951, -0.03578628, ..., -0.04920872,\n",
       "            -0.05756953,  0.00738034],\n",
       "           [ 0.04608286,  0.05381542,  0.01310834, ..., -0.009504  ,\n",
       "             0.02729523,  0.10921142],\n",
       "           [ 0.03621746,  0.03816689,  0.02170836, ...,  0.11632265,\n",
       "             0.03643941,  0.02784434],\n",
       "           ...,\n",
       "           [-0.02444408, -0.02105861,  0.00251118, ..., -0.01457316,\n",
       "             0.0007739 , -0.00935103],\n",
       "           [-0.01505548, -0.03221411, -0.06596313, ..., -0.02502459,\n",
       "            -0.06467625,  0.03685718],\n",
       "           [-0.0309572 ,  0.02106791,  0.05873235, ...,  0.00069032,\n",
       "             0.00089045, -0.00066607]]),\n",
       "    array([[ 0.06390869, -0.83899872, -0.74826376,  0.49339888, -1.1453697 ,\n",
       "             1.26182527,  0.79636394, -0.5244457 , -0.27536979,  1.44604879,\n",
       "             1.00305236, -0.41811866, -1.00496573, -1.14406085, -0.83252741,\n",
       "            -1.31853032, -0.16052571, -0.07894071, -0.16797607, -1.13142504,\n",
       "            -1.14194702, -0.49784055, -0.29326937,  0.57594647,  1.21480547,\n",
       "            -0.97064669,  0.47785538, -0.30592359, -0.64454065,  0.79139656,\n",
       "             1.42317629,  0.75230301]])),\n",
       "   (array([[ 0.01399605,  0.015811  , -0.0363786 , ..., -0.04948009,\n",
       "            -0.05813131,  0.00678186],\n",
       "           [ 0.05301672,  0.06075004,  0.02000536, ..., -0.00624709,\n",
       "             0.03383231,  0.11628978],\n",
       "           [ 0.04167209,  0.04362151,  0.02714889, ...,  0.11921988,\n",
       "             0.04195601,  0.03336766],\n",
       "           ...,\n",
       "           [-0.02982294, -0.02643806, -0.00285625, ..., -0.01731314,\n",
       "            -0.00486169, -0.01483358],\n",
       "           [-0.01752122, -0.03467621, -0.06839413, ..., -0.02247375,\n",
       "            -0.06590875,  0.0344486 ],\n",
       "           [-0.03748715,  0.01452668,  0.05215516, ..., -0.00193964,\n",
       "            -0.00511659, -0.00740903]]),\n",
       "    array([[ 0.07726334, -0.9082304 , -0.84190409,  0.53203915, -1.21904191,\n",
       "             1.30462747,  0.84083532, -0.56027497, -0.29319685,  1.55413908,\n",
       "             1.06408769, -0.469261  , -1.06316082, -1.1952938 , -0.91603655,\n",
       "            -1.43179475, -0.17561194, -0.08482369, -0.17535233, -1.28899853,\n",
       "            -1.1940448 , -0.5287903 , -0.32259225,  0.63527225,  1.36638327,\n",
       "            -1.02681629,  0.49391035, -0.34108462, -0.69173994,  0.88582433,\n",
       "             1.53310538,  0.7920695 ]])),\n",
       "   (array([[ 0.01341104,  0.01522472, -0.03697091, ..., -0.04970329,\n",
       "            -0.05867902,  0.00618526],\n",
       "           [ 0.05930335,  0.06703673,  0.02626189, ..., -0.00375469,\n",
       "             0.03948719,  0.12272313],\n",
       "           [ 0.04644159,  0.04839089,  0.0319068 , ...,  0.1212573 ,\n",
       "             0.04670351,  0.03819471],\n",
       "           ...,\n",
       "           [-0.03449542, -0.031111  , -0.00751994, ..., -0.01918459,\n",
       "            -0.00970844, -0.01959449],\n",
       "           [-0.01917668, -0.03632766, -0.07000934, ..., -0.01925056,\n",
       "            -0.06627563,  0.03286044],\n",
       "           [-0.04303533,  0.00896704,  0.04653165, ..., -0.00370637,\n",
       "            -0.00996311, -0.01316741]]),\n",
       "    array([[ 0.09232497, -0.98954931, -0.9280173 ,  0.56829114, -1.28367482,\n",
       "             1.33277463,  0.88174884, -0.59225816, -0.30900419,  1.6492174 ,\n",
       "             1.11644689, -0.51605395, -1.11436296, -1.23146074, -0.99188393,\n",
       "            -1.53613093, -0.18761117, -0.08900812, -0.18249963, -1.42773593,\n",
       "            -1.22847167, -0.55892031, -0.34847321,  0.69156795,  1.50237199,\n",
       "            -1.07027142,  0.5099051 , -0.37268764, -0.73018544,  0.97083055,\n",
       "             1.62721453,  0.82769571]])),\n",
       "   (array([[ 0.0128315 ,  0.01464378, -0.03755957, ..., -0.04987774,\n",
       "            -0.0592056 ,  0.00559444],\n",
       "           [ 0.06516549,  0.07289829,  0.03209558, ..., -0.00169764,\n",
       "             0.04448175,  0.12874744],\n",
       "           [ 0.05066047,  0.05260957,  0.03611511, ...,  0.12265265,\n",
       "             0.0507773 ,  0.04246001],\n",
       "           ...,\n",
       "           [-0.03860729, -0.03522321, -0.01162414, ..., -0.02042485,\n",
       "            -0.01387232, -0.0237793 ],\n",
       "           [-0.02039883, -0.03754545, -0.0711847 , ..., -0.01595773,\n",
       "            -0.06623957,  0.03171143],\n",
       "           [-0.04785109,  0.00413972,  0.04162111, ..., -0.00488079,\n",
       "            -0.01391476, -0.01818151]]),\n",
       "    array([[ 0.10824145, -1.08161559, -1.00643947,  0.6050362 , -1.33972954,\n",
       "             1.35579184,  0.93508537, -0.622724  , -0.32443602,  1.73542597,\n",
       "             1.16338083, -0.55953529, -1.16746682, -1.25994318, -1.06098569,\n",
       "            -1.63387448, -0.19886967, -0.09282533, -0.19099599, -1.55045734,\n",
       "            -1.25381398, -0.58988486, -0.37294148,  0.74645801,  1.62604318,\n",
       "            -1.10742308,  0.53016276, -0.40225333, -0.76378208,  1.04903074,\n",
       "             1.7112811 ,  0.86556939]])),\n",
       "   (array([[ 1.22548858e-02,  1.40656008e-02, -3.81465125e-02, ...,\n",
       "            -5.00139447e-02, -5.97152314e-02,  5.00707094e-03],\n",
       "           [ 7.06854636e-02,  7.84172718e-02,  3.75871518e-02, ...,\n",
       "             1.23362407e-05,  4.88514399e-02,  1.34430069e-01],\n",
       "           [ 5.44332027e-02,  5.63820146e-02,  3.98772788e-02, ...,\n",
       "             1.23598296e-01,  5.43014570e-02,  4.62706802e-02],\n",
       "           ...,\n",
       "           [-4.22586159e-02, -3.88747845e-02, -1.52681568e-02, ...,\n",
       "            -2.12252724e-02, -1.74710399e-02, -2.74909691e-02],\n",
       "           [-2.13587177e-02, -3.85006632e-02, -7.20888404e-02, ...,\n",
       "            -1.28398153e-02, -6.60299070e-02,  3.08266894e-02],\n",
       "           [-5.21794256e-02, -2.00565786e-04,  3.71831280e-02, ...,\n",
       "            -5.66835082e-03, -1.72579380e-02, -2.26930529e-02]]),\n",
       "    array([[ 0.12494662, -1.18207267, -1.07679811,  0.64215106, -1.38792754,\n",
       "             1.37822203,  1.01228172, -0.65304955, -0.33975951,  1.81398457,\n",
       "             1.20627728, -0.59923585, -1.22707212, -1.28458036, -1.123555  ,\n",
       "            -1.72353661, -0.2101724 , -0.09669717, -0.20102483, -1.6586784 ,\n",
       "            -1.27580278, -0.62445328, -0.39638809,  0.79937669,  1.73864846,\n",
       "            -1.14233083,  0.55494706, -0.42982803, -0.79623951,  1.12102289,\n",
       "             1.7878498 ,  0.90842218]])),\n",
       "   (array([[ 0.01167524,  0.01348422, -0.03873673, ..., -0.05012458,\n",
       "            -0.06021506,  0.00441724],\n",
       "           [ 0.07588736,  0.08361791,  0.04275968, ...,  0.00134099,\n",
       "             0.05259319,  0.13977931],\n",
       "           [ 0.05783419,  0.05978262,  0.04326681, ...,  0.12423052,\n",
       "             0.0573785 ,  0.04970361],\n",
       "           ...,\n",
       "           [-0.04551865, -0.04213497, -0.01852059, ..., -0.02172131,\n",
       "            -0.02060219, -0.03080153],\n",
       "           [-0.02211703, -0.03925394, -0.07278043, ..., -0.00993655,\n",
       "            -0.06573143,  0.03014223],\n",
       "           [-0.0562491 , -0.00428299,  0.03300491, ..., -0.00628361,\n",
       "            -0.02025203, -0.02693114]]),\n",
       "    array([[ 0.14304074, -1.28786463, -1.13930645,  0.67984879, -1.42846187,\n",
       "             1.40247592,  1.10585451, -0.68464127, -0.35468126,  1.88526107,\n",
       "             1.24597231, -0.63467856, -1.293604  , -1.30688085, -1.18017756,\n",
       "            -1.80263825, -0.22149647, -0.10051663, -0.21200472, -1.75420691,\n",
       "            -1.2979041 , -0.66331226, -0.41860484,  0.85074902,  1.84253412,\n",
       "            -1.17693414,  0.58350698, -0.45505082, -0.83015754,  1.18825912,\n",
       "             1.85855903,  0.95656697]])),\n",
       "   (array([[ 0.01108101,  0.0128881 , -0.03934059, ..., -0.05022696,\n",
       "            -0.06071796,  0.00381302],\n",
       "           [ 0.08079475,  0.08852384,  0.04763632, ...,  0.00226727,\n",
       "             0.05574632,  0.14481363],\n",
       "           [ 0.06092018,  0.06286811,  0.04633987, ...,  0.12464179,\n",
       "             0.0600846 ,  0.05281816],\n",
       "           ...,\n",
       "           [-0.04844001, -0.04505638, -0.02143369, ..., -0.02200173,\n",
       "            -0.02333878, -0.03376699],\n",
       "           [-0.02268395, -0.03981529, -0.0732686 , ..., -0.00720882,\n",
       "            -0.06534896,  0.02964521],\n",
       "           [-0.06022227, -0.00827022,  0.02894276, ..., -0.00686515,\n",
       "            -0.02306238, -0.03106174]]),\n",
       "    array([[ 0.16267876, -1.39581857, -1.19461535,  0.71734979, -1.46267747,\n",
       "             1.42961745,  1.20694362, -0.71941498, -0.36886285,  1.94984534,\n",
       "             1.28319171, -0.66585336, -1.36628976, -1.32836912, -1.23204397,\n",
       "            -1.87167869, -0.23263246, -0.10428155, -0.22334401, -1.83900418,\n",
       "            -1.32295929, -0.70503105, -0.43936915,  0.90017444,  1.9389903 ,\n",
       "            -1.21304818,  0.61425608, -0.47762105, -0.86729264,  1.25120896,\n",
       "             1.92447042,  1.008327  ]])),\n",
       "   (array([[ 0.010461  ,  0.012266  , -0.03996797, ..., -0.05034096,\n",
       "            -0.06123936,  0.00318251],\n",
       "           [ 0.08541784,  0.09314531,  0.05222709, ...,  0.0027824 ,\n",
       "             0.05836536,  0.14954583],\n",
       "           [ 0.06373416,  0.06568148,  0.04913915, ...,  0.1248976 ,\n",
       "             0.06248493,  0.05565974],\n",
       "           ...,\n",
       "           [-0.05106374, -0.04768005, -0.02404823, ..., -0.02212929,\n",
       "            -0.02574807, -0.03643185],\n",
       "           [-0.02305985, -0.04018512, -0.07355252, ..., -0.00461549,\n",
       "            -0.06487573,  0.02933305],\n",
       "           [-0.06411723, -0.01218105,  0.02499061, ..., -0.0073911 ,\n",
       "            -0.02571695, -0.03510978]]),\n",
       "    array([[ 0.18357278, -1.50326917, -1.24367451,  0.75330769, -1.49250496,\n",
       "             1.45896927,  1.31081101, -0.75930461, -0.38208932,  2.00854923,\n",
       "             1.31848912, -0.69314292, -1.44275842, -1.34983512, -1.28098179,\n",
       "            -1.93314574, -0.24337668, -0.1080149 , -0.23456309, -1.91490326,\n",
       "            -1.35258305, -0.74752745, -0.45857906,  0.94661434,  2.02826555,\n",
       "            -1.25139695,  0.64548115, -0.49750887, -0.90747948,  1.30953055,\n",
       "             1.98622251,  1.06109794]]))],\n",
       "  'losses': [1447.3387075541198,\n",
       "   1196.5432035333915,\n",
       "   1051.8023216287245,\n",
       "   953.9101292244763,\n",
       "   875.9786132260431,\n",
       "   808.2530944322718,\n",
       "   748.684550848272,\n",
       "   696.5496019621673,\n",
       "   651.1282774930796,\n",
       "   611.4642097273997],\n",
       "  'biases': [(array([[0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.],\n",
       "           [0.]]),\n",
       "    array([[0.]])),\n",
       "   (array([[-0.0041014 ],\n",
       "           [ 0.05676123],\n",
       "           [ 0.03294176],\n",
       "           [-0.01252824],\n",
       "           [ 0.08376222],\n",
       "           [-0.08827797],\n",
       "           [-0.03980768],\n",
       "           [ 0.02009211],\n",
       "           [ 0.00486345],\n",
       "           [-0.09370218],\n",
       "           [-0.05332986],\n",
       "           [ 0.00525786],\n",
       "           [ 0.0710068 ],\n",
       "           [ 0.07809799],\n",
       "           [ 0.04602919],\n",
       "           [ 0.08975436],\n",
       "           [-0.00179798],\n",
       "           [-0.00381246],\n",
       "           [-0.00146813],\n",
       "           [ 0.05534267],\n",
       "           [ 0.08047759],\n",
       "           [ 0.02867543],\n",
       "           [-0.00353631],\n",
       "           [-0.01172936],\n",
       "           [-0.05651789],\n",
       "           [ 0.06138432],\n",
       "           [-0.01568245],\n",
       "           [ 0.00416725],\n",
       "           [ 0.03603487],\n",
       "           [-0.03078593],\n",
       "           [-0.08222091],\n",
       "           [-0.04542613]]),\n",
       "    array([[0.09965613]])),\n",
       "   (array([[-0.00581996],\n",
       "           [ 0.06571358],\n",
       "           [ 0.03989187],\n",
       "           [-0.01750541],\n",
       "           [ 0.10710349],\n",
       "           [-0.11218141],\n",
       "           [-0.05115165],\n",
       "           [ 0.02592237],\n",
       "           [ 0.00620785],\n",
       "           [-0.11086673],\n",
       "           [-0.06364218],\n",
       "           [ 0.00770295],\n",
       "           [ 0.08928145],\n",
       "           [ 0.09923109],\n",
       "           [ 0.05366869],\n",
       "           [ 0.10906143],\n",
       "           [-0.00178604],\n",
       "           [-0.00459405],\n",
       "           [-0.00120486],\n",
       "           [ 0.06649932],\n",
       "           [ 0.0987865 ],\n",
       "           [ 0.03672176],\n",
       "           [-0.00307626],\n",
       "           [-0.01747412],\n",
       "           [-0.06828159],\n",
       "           [ 0.07703477],\n",
       "           [-0.01994329],\n",
       "           [ 0.00595248],\n",
       "           [ 0.04394007],\n",
       "           [-0.0401696 ],\n",
       "           [-0.09475481],\n",
       "           [-0.0566708 ]]),\n",
       "    array([[0.10844651]])),\n",
       "   (array([[-0.00721526],\n",
       "           [ 0.06416259],\n",
       "           [ 0.0444599 ],\n",
       "           [-0.02200895],\n",
       "           [ 0.12049257],\n",
       "           [-0.11866812],\n",
       "           [-0.05330885],\n",
       "           [ 0.02838399],\n",
       "           [ 0.00679097],\n",
       "           [-0.121085  ],\n",
       "           [-0.0704021 ],\n",
       "           [ 0.01021367],\n",
       "           [ 0.09349715],\n",
       "           [ 0.10412249],\n",
       "           [ 0.05529104],\n",
       "           [ 0.12265896],\n",
       "           [-0.00161402],\n",
       "           [-0.00515275],\n",
       "           [-0.00129551],\n",
       "           [ 0.07035485],\n",
       "           [ 0.10183569],\n",
       "           [ 0.03921641],\n",
       "           [-0.00231398],\n",
       "           [-0.02255648],\n",
       "           [-0.07228056],\n",
       "           [ 0.08027917],\n",
       "           [-0.0211836 ],\n",
       "           [ 0.007961  ],\n",
       "           [ 0.04378877],\n",
       "           [-0.04652099],\n",
       "           [-0.10633859],\n",
       "           [-0.05753154]]),\n",
       "    array([[0.10170709]])),\n",
       "   (array([[-0.0082752 ],\n",
       "           [ 0.06155514],\n",
       "           [ 0.04556629],\n",
       "           [-0.02555066],\n",
       "           [ 0.12866092],\n",
       "           [-0.11611869],\n",
       "           [-0.05079002],\n",
       "           [ 0.02947252],\n",
       "           [ 0.00710754],\n",
       "           [-0.13310608],\n",
       "           [-0.07741692],\n",
       "           [ 0.01219941],\n",
       "           [ 0.08878228],\n",
       "           [ 0.10104236],\n",
       "           [ 0.05331425],\n",
       "           [ 0.13681705],\n",
       "           [-0.00145846],\n",
       "           [-0.00573615],\n",
       "           [-0.00176722],\n",
       "           [ 0.06723904],\n",
       "           [ 0.09775515],\n",
       "           [ 0.0370049 ],\n",
       "           [-0.0013931 ],\n",
       "           [-0.02524229],\n",
       "           [-0.06979837],\n",
       "           [ 0.07642506],\n",
       "           [-0.02093755],\n",
       "           [ 0.0097271 ],\n",
       "           [ 0.03937411],\n",
       "           [-0.0483017 ],\n",
       "           [-0.11955454],\n",
       "           [-0.05300448]]),\n",
       "    array([[0.09269395]])),\n",
       "   (array([[-0.00902853],\n",
       "           [ 0.05765258],\n",
       "           [ 0.0439563 ],\n",
       "           [-0.02811152],\n",
       "           [ 0.13569766],\n",
       "           [-0.11034261],\n",
       "           [-0.04794882],\n",
       "           [ 0.02995547],\n",
       "           [ 0.00745426],\n",
       "           [-0.14718089],\n",
       "           [-0.08506756],\n",
       "           [ 0.01339442],\n",
       "           [ 0.08006061],\n",
       "           [ 0.09525861],\n",
       "           [ 0.0487167 ],\n",
       "           [ 0.15142565],\n",
       "           [-0.00123034],\n",
       "           [-0.00637096],\n",
       "           [-0.00247844],\n",
       "           [ 0.05984554],\n",
       "           [ 0.09129397],\n",
       "           [ 0.03282692],\n",
       "           [-0.00024004],\n",
       "           [-0.02558833],\n",
       "           [-0.06339848],\n",
       "           [ 0.06933226],\n",
       "           [-0.01986718],\n",
       "           [ 0.01117888],\n",
       "           [ 0.03280352],\n",
       "           [-0.04681763],\n",
       "           [-0.13384042],\n",
       "           [-0.04604636]]),\n",
       "    array([[0.0848499]])),\n",
       "   (array([[-0.00952496],\n",
       "           [ 0.05217692],\n",
       "           [ 0.04101913],\n",
       "           [-0.0301223 ],\n",
       "           [ 0.14343496],\n",
       "           [-0.10448254],\n",
       "           [-0.04879083],\n",
       "           [ 0.03050478],\n",
       "           [ 0.00806   ],\n",
       "           [-0.16340186],\n",
       "           [-0.09376846],\n",
       "           [ 0.01398911],\n",
       "           [ 0.07114736],\n",
       "           [ 0.08986102],\n",
       "           [ 0.04299846],\n",
       "           [ 0.16699409],\n",
       "           [-0.00079415],\n",
       "           [-0.00702634],\n",
       "           [-0.00322517],\n",
       "           [ 0.05037178],\n",
       "           [ 0.08498989],\n",
       "           [ 0.02828322],\n",
       "           [ 0.00131842],\n",
       "           [-0.02449195],\n",
       "           [-0.05498665],\n",
       "           [ 0.06165207],\n",
       "           [-0.01874337],\n",
       "           [ 0.01240266],\n",
       "           [ 0.02541816],\n",
       "           [-0.04368879],\n",
       "           [-0.1493421 ],\n",
       "           [-0.0386533 ]]),\n",
       "    array([[0.08013891]])),\n",
       "   (array([[-9.82086799e-03],\n",
       "           [ 4.53493621e-02],\n",
       "           [ 3.73774889e-02],\n",
       "           [-3.18639649e-02],\n",
       "           [ 1.52183116e-01],\n",
       "           [-9.94800913e-02],\n",
       "           [-5.41961382e-02],\n",
       "           [ 3.13432689e-02],\n",
       "           [ 8.94439278e-03],\n",
       "           [-1.81105615e-01],\n",
       "           [-1.03400318e-01],\n",
       "           [ 1.41879594e-02],\n",
       "           [ 6.40336211e-02],\n",
       "           [ 8.57938300e-02],\n",
       "           [ 3.66529064e-02],\n",
       "           [ 1.83679122e-01],\n",
       "           [-1.23665258e-04],\n",
       "           [-7.67284211e-03],\n",
       "           [-3.86233319e-03],\n",
       "           [ 3.98621444e-02],\n",
       "           [ 7.93368327e-02],\n",
       "           [ 2.45743950e-02],\n",
       "           [ 3.25359200e-03],\n",
       "           [-2.25816730e-02],\n",
       "           [-4.53984790e-02],\n",
       "           [ 5.46518008e-02],\n",
       "           [-1.81245885e-02],\n",
       "           [ 1.34162472e-02],\n",
       "           [ 1.78456492e-02],\n",
       "           [-3.97419699e-02],\n",
       "           [-1.65664456e-01],\n",
       "           [-3.18645046e-02]]),\n",
       "    array([[0.07891593]])),\n",
       "   (array([[-0.00992412],\n",
       "           [ 0.03731001],\n",
       "           [ 0.03321513],\n",
       "           [-0.03343129],\n",
       "           [ 0.16154319],\n",
       "           [-0.09539933],\n",
       "           [-0.06291387],\n",
       "           [ 0.03211819],\n",
       "           [ 0.01001794],\n",
       "           [-0.19949555],\n",
       "           [-0.11349448],\n",
       "           [ 0.01410434],\n",
       "           [ 0.05929978],\n",
       "           [ 0.08313982],\n",
       "           [ 0.0296233 ],\n",
       "           [ 0.20092673],\n",
       "           [ 0.00072796],\n",
       "           [-0.00830627],\n",
       "           [-0.00433893],\n",
       "           [ 0.02873417],\n",
       "           [ 0.07390234],\n",
       "           [ 0.02212601],\n",
       "           [ 0.00542162],\n",
       "           [-0.02013839],\n",
       "           [-0.03494776],\n",
       "           [ 0.04887825],\n",
       "           [-0.01805528],\n",
       "           [ 0.0142279 ],\n",
       "           [ 0.01033873],\n",
       "           [-0.03533121],\n",
       "           [-0.18220293],\n",
       "           [-0.02583253]]),\n",
       "    array([[0.08190305]])),\n",
       "   (array([[-0.00985633],\n",
       "           [ 0.02794608],\n",
       "           [ 0.02853176],\n",
       "           [-0.03481034],\n",
       "           [ 0.17104728],\n",
       "           [-0.09192113],\n",
       "           [-0.07202357],\n",
       "           [ 0.03256119],\n",
       "           [ 0.01118983],\n",
       "           [-0.21793356],\n",
       "           [-0.12353293],\n",
       "           [ 0.01378523],\n",
       "           [ 0.05658159],\n",
       "           [ 0.081575  ],\n",
       "           [ 0.02148618],\n",
       "           [ 0.21787244],\n",
       "           [ 0.00169818],\n",
       "           [-0.00891177],\n",
       "           [-0.00463029],\n",
       "           [ 0.01709331],\n",
       "           [ 0.06798616],\n",
       "           [ 0.02096637],\n",
       "           [ 0.00769065],\n",
       "           [-0.01725009],\n",
       "           [-0.02371194],\n",
       "           [ 0.0444503 ],\n",
       "           [-0.01838346],\n",
       "           [ 0.01483316],\n",
       "           [ 0.00320941],\n",
       "           [-0.03058216],\n",
       "           [-0.19823394],\n",
       "           [-0.02055496]]),\n",
       "    array([[0.088462]])),\n",
       "   (array([[-0.00966437],\n",
       "           [ 0.01700304],\n",
       "           [ 0.02338652],\n",
       "           [-0.035984  ],\n",
       "           [ 0.18027525],\n",
       "           [-0.08858207],\n",
       "           [-0.07855559],\n",
       "           [ 0.03263164],\n",
       "           [ 0.01240299],\n",
       "           [-0.23584788],\n",
       "           [-0.13304459],\n",
       "           [ 0.01327932],\n",
       "           [ 0.05522512],\n",
       "           [ 0.08056168],\n",
       "           [ 0.01173931],\n",
       "           [ 0.23362575],\n",
       "           [ 0.00273585],\n",
       "           [-0.00947059],\n",
       "           [-0.00474016],\n",
       "           [ 0.00500519],\n",
       "           [ 0.06185887],\n",
       "           [ 0.02103761],\n",
       "           [ 0.00996745],\n",
       "           [-0.01399059],\n",
       "           [-0.01179234],\n",
       "           [ 0.04112996],\n",
       "           [-0.0189236 ],\n",
       "           [ 0.01525238],\n",
       "           [-0.00339303],\n",
       "           [-0.0256042 ],\n",
       "           [-0.21319832],\n",
       "           [-0.01603652]]),\n",
       "    array([[0.09700875]]))],\n",
       "  'accuracies': [0.963,\n",
       "   0.9698333333333333,\n",
       "   0.97325,\n",
       "   0.9755,\n",
       "   0.9776666666666667,\n",
       "   0.9790833333333333,\n",
       "   0.9806666666666667,\n",
       "   0.9828333333333333,\n",
       "   0.98375,\n",
       "   0.98475]},\n",
       " (9, 0.98475, 611.4642097273997))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set path to data\n",
    "data_path = '../setup/data'\n",
    "epochs = 10\n",
    "model_name = 'full_batch_without_momentum'\n",
    "h1 = 32\n",
    "lr = 0.1\n",
    "batch_size = 10\n",
    "mode = \"full\"\n",
    "\n",
    "\n",
    "run_training(data_path, epochs, model_name, h1, lr, mode, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4.2 Hyperparameter tuning using gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(momentum, lr, batch_size):\n",
    "\n",
    "\n",
    "    best_params = {}\n",
    "    best_params['loss'] = np.inf\n",
    "    best_params['momentum'] = 0\n",
    "    best_params['lr'] = 0\n",
    "    best_params['batch_size'] = 0\n",
    "    best_params[\"weights\"] = 0\n",
    "    best_params[\"biases\"] = 0\n",
    "    \n",
    "    for m in momentum :\n",
    "        for l in lr :\n",
    "            for bs in batch_size :\n",
    "                \n",
    "                weights, biases = initialize(X_train_flattened)\n",
    "                epochs = 50\n",
    "                velocity = initialize_velocity(weights,biases)\n",
    "                \n",
    "                history = {\n",
    "                    \"weights\": [weights],\n",
    "                    \"losses\": [], \n",
    "                    \"biases\": [biases],\n",
    "                    \"accuracies\": [],\n",
    "                    \"velocity\":[velocity]\n",
    "                }\n",
    "                \n",
    "                history = batch_training_with_momentum(bs,weights,biases,history,X_train_flattened,y_train,m,l)\n",
    "                best_epoch = get_best_results(history)\n",
    "                print(history['accuracies'][best_epoch],best_epoch)\n",
    "                \n",
    "                activations_dev = forward_pass(X_dev_flattened, history[\"weights\"][best_epoch], history[\"biases\"][best_epoch])\n",
    "                y_prob = activations_dev[-1]\n",
    "                dev_loss = get_log_loss(y_dev,y_prob)\n",
    "                \n",
    "                if dev_loss < best_params['loss'] :\n",
    "                    best_params['momentum'] = m\n",
    "                    best_params['lr'] = l\n",
    "                    best_params['batch_size'] = bs\n",
    "                    best_params['loss'] = dev_loss\n",
    "                    best_params[\"weights\"] = history[\"weights\"][best_epoch]\n",
    "                    best_params[\"biases\"] = history[\"biases\"][best_epoch]\n",
    "            \n",
    "    return(best_params)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store initial parameters\n",
    "h1 = 32\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store grids for grid search\n",
    "momentum = np.arange(0.001,0.01,0.001)\n",
    "lr = np.arange(0.01,0.1,0.01)\n",
    "batch_size = [16,64,256,1024,4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search and get best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with the best parameters\n",
    "history = execute_pipeline(**best_params)\n",
    "plot_loss(\"loss.png\", history[\"dev_loss\"][:-2],label = \"validation loss\")\n",
    "plot_loss(\"accuracy.png\", history[\"dev_accuracies\"][:-2], label='validation Accuracy')\n",
    "history[\"dev_accuracies\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
