{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from helpers import load_all_data, vectorized_flatten, sigmoid, get_log_loss, get_accuracy, sigmoid_derivative, gradient_update, get_loss_plot, plot_loss\n",
    "from helpers import sgd_with_momentum_update\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data_path):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    Use vectorized flatten\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Load\n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_all_data(data_path)\n",
    "    \n",
    "    # Flatten\n",
    "    X_train_flattened = vectorized_flatten(X_train)\n",
    "    X_dev_flattened = vectorized_flatten(X_dev)\n",
    "    X_test_flattened = vectorized_flatten(X_test)\n",
    "    \n",
    "    # Reshape labels\n",
    "    y_train = y_train.reshape(1, -1)\n",
    "    y_dev = y_dev.reshape(1, -1)\n",
    "    y_test = y_test.reshape(1, -1)\n",
    "    \n",
    "    # Return\n",
    "    return(X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X, h1): \n",
    "    '''\n",
    "    --------------------\n",
    "    Parameter Initialization\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X [n = 12000])\n",
    "    --------------------\n",
    "    Output: \n",
    "    weights: Weight terms initialized as random normals\n",
    "    biases: Bias terms initialized to zero\n",
    "    --------------------\n",
    "    '''\n",
    "    dim1 = 1/np.sqrt(X.shape[0])\n",
    "    W1 = dim1 * np.random.randn(h1, 28**2)\n",
    "    \n",
    "    dim2 = 1/np.sqrt(W1.shape[1])\n",
    "    W2 = dim2 * np.random.randn(1, h1)\n",
    "    \n",
    "    b1 = np.zeros((h1, 1))\n",
    "    b2 = np.zeros((1, 1))\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, weights, biases):\n",
    "    '''\n",
    "    ----------------------------------\n",
    "    Forward propogation:\n",
    "    Send inputs through the network to\n",
    "    generate output\n",
    "    ----------------------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    weights: Binary (1/0) training label (shape = n X 1)\n",
    "    biases:\n",
    "    --------------------\n",
    "    Output: \n",
    "    activations: vector of results from passing\n",
    "    inputs through each neuron\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    z1 = W1 @ X + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    activations = (z1, a1, z2, a2)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, weights, biases, activations):\n",
    "    '''\n",
    "    --------------------\n",
    "    Backpropagation\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    activations: Current set of activations\n",
    "    --------------------\n",
    "    Output: \n",
    "    Derivatives required\n",
    "    for optimization update\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    z1, a1, z2, a2 = activations\n",
    "    m = y.shape[1]\n",
    "    #print(m)   \n",
    "    dz2 = a2 - y\n",
    "    #print(\"dz3\", dz3.shape)\n",
    "    \n",
    "   \n",
    "    dW2 = np.dot(dz2, a1.T)/m\n",
    "    #print(\"dW2\", dW2.shape)\n",
    "    \n",
    "    db2 = np.sum(dz2, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db2\", db2.shape)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    #print(\"da1\", da1.shape)\n",
    "    \n",
    "    dz1 = da1 * sigmoid_derivative(z1)\n",
    "    #print(\"dz1\", dz1.shape)\n",
    "    \n",
    "    dW1 = np.dot(dz1, X.T)/m\n",
    "    #print(\"dW1\", dW1.shape)\n",
    "    \n",
    "    db1 = np.sum(dz1, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db1\", db1.shape)\n",
    "    \n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    '''\n",
    "    --------------------\n",
    "    Update parameters\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    \n",
    "    W1 = gradient_update(W1, learning_rate, dW1)\n",
    "    W2 = gradient_update(W2, learning_rate, dW2)\n",
    "   \n",
    "    b1 = gradient_update(b1, learning_rate, db1)\n",
    "    b2 = gradient_update(b2, learning_rate, db2)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(weights,biases,gradients,learning_rate,velocity,momentum):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    vw1,vw2,vb1,vb2 = velocity\n",
    "    W1,vw1 = sgd_with_momentum_update(W1, learning_rate, dW1,vw1,momentum)\n",
    "    W2,vw2 = sgd_with_momentum_update(W2, learning_rate, dW2,vw2,momentum)\n",
    "   \n",
    "    b1,vb1 = sgd_with_momentum_update(b1, learning_rate, db1,vb1,momentum)\n",
    "    b2,vb2 = sgd_with_momentum_update(b2, learning_rate, db2,vb2,momentum)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    velocity = (vw1,vw2,vb1,vb2)\n",
    "    return weights ,biases,velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training_without_momentum(batch_size, weights, biases, epochs, lr, X, y):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    history = {\n",
    "    \"weights\": [weights],\n",
    "    \"losses\": [], \n",
    "    \"biases\": [biases],\n",
    "    \"accuracies\": []\n",
    "    \n",
    "    }\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        offset = 0\n",
    "        weights = history['weights'][epoch]\n",
    "        biases = history['biases'][epoch]\n",
    "        \n",
    "        while offset < max(y.shape):\n",
    "            \n",
    "            if offset%1000==0 :\n",
    "                print(\"epoch :\",epoch,\" batch:\",offset)\n",
    "            else :\n",
    "                a=1\n",
    "            if offset+batch_size >=max(y.shape):\n",
    "                X_batch = X[:,offset:]\n",
    "                y_batch = y[:,offset:]\n",
    "            else :    \n",
    "                X_batch = X[:,offset:offset+batch_size]\n",
    "                y_batch = y[:,offset:offset+batch_size]\n",
    "            \n",
    "            \n",
    "            activations = forward_pass(X_batch, weights, biases)\n",
    "            gradients = backpropagation(X_batch, y_batch, weights, biases, activations)\n",
    "\n",
    "            weights, biases = update_parameters(weights, biases, gradients, lr)\n",
    "            offset = offset+batch_size\n",
    "        \n",
    "        activations_full = forward_pass(X, weights, biases)\n",
    "        y_prob = activations_full[-1]\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    \n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "\n",
    "        history[\"weights\"].append(weights)\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(biases)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "\n",
    "        if np.isnan(loss):\n",
    "            break\n",
    "\n",
    "        print(\"loss after epoch: \",epoch,\": \",loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(weights,biases):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    vw1 = np.zeros(W1.shape)\n",
    "    vw2 = np.zeros(W2.shape)\n",
    "    vb1 = np.zeros(b1.shape)\n",
    "    vb2 = np.zeros(b2.shape)\n",
    "    return vw1,vw2,vb1,vb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training_with_momentum(batch_size,weights,biases, epochs, X,y,momentum, lr, X_dev,y_dev, velocity):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    history = {\n",
    "        \"weights\": [weights],\n",
    "        \"losses\": [], \n",
    "        \"biases\": [biases],\n",
    "        \"accuracies\": [],\n",
    "        \"velocity\":[velocity],\n",
    "        \"dev_accuracies\" :[],\n",
    "        \"dev_loss\":[]\n",
    "    }\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        offset = 0\n",
    "        weights = history['weights'][epoch]\n",
    "        biases = history['biases'][epoch]\n",
    "        velocity = history['velocity'][epoch]\n",
    "        \n",
    "        while offset <max(y.shape):\n",
    "            if offset%1000==0 :\n",
    "                print(\"epoch :\",epoch,\" batch:\",offset)\n",
    "            else :\n",
    "                a=1\n",
    "            if offset+batch_size >=max(y.shape):\n",
    "                X_batch = X[:,offset:]\n",
    "                y_batch = y[:,offset:]\n",
    "            else :    \n",
    "                X_batch = X[:,offset:offset+batch_size]\n",
    "                y_batch = y[:,offset:offset+batch_size]\n",
    "            \n",
    "            activations = forward_pass(X_batch, weights, biases)\n",
    "            gradients = backpropagation(X_batch, y_batch, weights, biases, activations)\n",
    "\n",
    "            \n",
    "            weights, biases,velocity = update_parameters_with_momentum(weights, biases, gradients, \n",
    "                                                                       lr,velocity,momentum)\n",
    "            offset = offset+batch_size\n",
    "        \n",
    "        activations_full = forward_pass(X, weights, biases)\n",
    "        y_prob = activations_full[-1]\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "\n",
    "        #print(y,y_prob)\n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "        \n",
    "        activations_dev = forward_pass(X_dev,weights,biases)\n",
    "        y_dev_prob =  activations_dev[-1]\n",
    "        y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "        loss_dev = get_log_loss(y_dev,y_dev_prob)\n",
    "        accuracy_dev = get_accuracy(y_dev,y_dev_pred)\n",
    "\n",
    "        history[\"weights\"].append(weights)\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(biases)\n",
    "        history[\"velocity\"].append(velocity)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "        history[\"dev_accuracies\"].append(accuracy_dev)\n",
    "        history['dev_loss'].append(loss_dev)\n",
    "\n",
    "        if np.isnan(loss):\n",
    "            break\n",
    "\n",
    "        print(\"loss after epoch: \",epoch,\": \",loss)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_epoch(history):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Store results\n",
    "    best_epoch = np.array(history[\"losses\"]).argmin()\n",
    "    best_accuracy = history['accuracies'][best_epoch]\n",
    "    best_loss = history['losses'][best_epoch]\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"best accuracy: {history['accuracies'][best_epoch]}\")\n",
    "    print(f\"best loss: {history['losses'][best_epoch]}\")\n",
    "    print(f\"best epoch: {best_epoch}\")\n",
    "    \n",
    "    return(best_epoch, best_accuracy, best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(X_dev, y_dev, history, best_epoch, label=\"dev\"):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    w = history[\"weights\"][best_epoch]\n",
    "    b = history[\"biases\"][best_epoch]\n",
    "    activations = forward_pass(X_dev, w, b)\n",
    "\n",
    "    y_dev_prob = activations[-1]\n",
    "    y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "\n",
    "    loss = get_log_loss(y_dev, y_dev_prob)\n",
    "    accuracy = get_accuracy(y_dev, y_dev_pred)\n",
    "    print(f\"{label} set accuracy: {accuracy}\")\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(data_path, epochs, model_name, h1, lr, mode, batch_size, momentum):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Set seed for reproducible results\n",
    "    np.random.seed(1252908)\n",
    "    \n",
    "    # Get data\n",
    "    X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test = prep_data(data_path)\n",
    "    \n",
    "    # Assign batch size\n",
    "    # If a batch size parameter is specified we use that otherwise we default to full batch gradient descent\n",
    "    if mode == 'full': \n",
    "        batch_size = max(y_train.shape)\n",
    "    \n",
    "    elif mode == 'stochastic': \n",
    "        batch_size = 1\n",
    "    \n",
    "    # Print as a check to make sure batch size is correctly set\n",
    "    print(\"The batch size is...{}\".format(batch_size))\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    weights, biases = initialize(X_train_flattened, h1)\n",
    "    \n",
    "    # Depe\n",
    "    if momentum:\n",
    "        \n",
    "        velocity = initialize_velocity(weights, biases)\n",
    "        history = batch_training_with_momentum(batch_size, weights, biases, \n",
    "                                               epochs, X_train_flattened, y_train, \n",
    "                                               momentum, lr, X_dev_flattened, y_dev, velocity)\n",
    "    else:\n",
    "        history = batch_training_without_momentum(batch_size, weights, biases, epochs, \n",
    "                                                  lr, X_train_flattened, y_train)\n",
    "    \n",
    "    # Get best epoch from history object\n",
    "    best_epoch = get_best_epoch(history)\n",
    "    \n",
    "    # Plot loss and save\n",
    "    plot_loss(\"loss_{}.png\".format(model_name), history[\"losses\"][:-2])\n",
    "    \n",
    "    # Plot accuracy and save\n",
    "    plot_loss(\"accuracy_{}.png\".format(model_name), history[\"accuracies\"][:-2], label='Training Accuracy')\n",
    "    \n",
    "    return(history, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch size is...12000\n",
      "epoch : 0  batch: 0\n",
      "loss after epoch:  0 :  8271.200264586612\n",
      "epoch : 1  batch: 0\n",
      "loss after epoch:  1 :  8256.260205105082\n",
      "epoch : 2  batch: 0\n",
      "loss after epoch:  2 :  8233.468651728763\n",
      "epoch : 3  batch: 0\n",
      "loss after epoch:  3 :  8202.179984444345\n",
      "epoch : 4  batch: 0\n",
      "loss after epoch:  4 :  8161.264002432089\n",
      "epoch : 5  batch: 0\n",
      "loss after epoch:  5 :  8108.938527195727\n",
      "epoch : 6  batch: 0\n",
      "loss after epoch:  6 :  8042.6119360384755\n",
      "epoch : 7  batch: 0\n",
      "loss after epoch:  7 :  7958.7627828105815\n",
      "epoch : 8  batch: 0\n",
      "loss after epoch:  8 :  7852.898781291593\n",
      "epoch : 9  batch: 0\n",
      "loss after epoch:  9 :  7719.639209828403\n",
      "epoch : 10  batch: 0\n",
      "loss after epoch:  10 :  7552.945413279511\n",
      "epoch : 11  batch: 0\n",
      "loss after epoch:  11 :  7346.525731881606\n",
      "epoch : 12  batch: 0\n",
      "loss after epoch:  12 :  7094.554016532295\n",
      "epoch : 13  batch: 0\n",
      "loss after epoch:  13 :  6793.086713972756\n",
      "epoch : 14  batch: 0\n",
      "loss after epoch:  14 :  6442.63771704984\n",
      "epoch : 15  batch: 0\n",
      "loss after epoch:  15 :  6051.488860671147\n",
      "epoch : 16  batch: 0\n",
      "loss after epoch:  16 :  5637.184108257285\n",
      "epoch : 17  batch: 0\n",
      "loss after epoch:  17 :  5222.962882344012\n",
      "epoch : 18  batch: 0\n",
      "loss after epoch:  18 :  4830.489557207957\n",
      "epoch : 19  batch: 0\n",
      "loss after epoch:  19 :  4475.185994773086\n",
      "epoch : 20  batch: 0\n",
      "loss after epoch:  20 :  4165.978139924\n",
      "epoch : 21  batch: 0\n",
      "loss after epoch:  21 :  3904.576737221796\n",
      "epoch : 22  batch: 0\n",
      "loss after epoch:  22 :  3683.477436263258\n",
      "epoch : 23  batch: 0\n",
      "loss after epoch:  23 :  3487.871973649782\n",
      "epoch : 24  batch: 0\n",
      "loss after epoch:  24 :  3302.9602186703623\n",
      "epoch : 25  batch: 0\n",
      "loss after epoch:  25 :  3121.3825040033744\n",
      "epoch : 26  batch: 0\n",
      "loss after epoch:  26 :  2944.5747748663443\n",
      "epoch : 27  batch: 0\n",
      "loss after epoch:  27 :  2777.0308168224046\n",
      "epoch : 28  batch: 0\n",
      "loss after epoch:  28 :  2620.319353943225\n",
      "epoch : 29  batch: 0\n",
      "loss after epoch:  29 :  2475.216354337569\n",
      "epoch : 30  batch: 0\n",
      "loss after epoch:  30 :  2349.009001932297\n",
      "epoch : 31  batch: 0\n",
      "loss after epoch:  31 :  2256.2684277773324\n",
      "epoch : 32  batch: 0\n",
      "loss after epoch:  32 :  2209.2115376447664\n",
      "epoch : 33  batch: 0\n",
      "loss after epoch:  33 :  2206.442257867966\n",
      "epoch : 34  batch: 0\n",
      "loss after epoch:  34 :  2231.6521871498708\n",
      "epoch : 35  batch: 0\n",
      "loss after epoch:  35 :  2264.9637066718415\n",
      "epoch : 36  batch: 0\n",
      "loss after epoch:  36 :  2296.574771709934\n",
      "epoch : 37  batch: 0\n",
      "loss after epoch:  37 :  2329.1215520226706\n",
      "epoch : 38  batch: 0\n",
      "loss after epoch:  38 :  2367.2091076707106\n",
      "epoch : 39  batch: 0\n",
      "loss after epoch:  39 :  2406.054091651191\n",
      "epoch : 40  batch: 0\n",
      "loss after epoch:  40 :  2431.4889813871537\n",
      "epoch : 41  batch: 0\n",
      "loss after epoch:  41 :  2431.144508682264\n",
      "epoch : 42  batch: 0\n",
      "loss after epoch:  42 :  2404.8277792060253\n",
      "epoch : 43  batch: 0\n",
      "loss after epoch:  43 :  2363.2963739880106\n",
      "epoch : 44  batch: 0\n",
      "loss after epoch:  44 :  2317.401861856728\n",
      "epoch : 45  batch: 0\n",
      "loss after epoch:  45 :  2269.463663965592\n",
      "epoch : 46  batch: 0\n",
      "loss after epoch:  46 :  2214.568087557454\n",
      "epoch : 47  batch: 0\n",
      "loss after epoch:  47 :  2148.4551768054703\n",
      "epoch : 48  batch: 0\n",
      "loss after epoch:  48 :  2073.628864416323\n",
      "epoch : 49  batch: 0\n",
      "loss after epoch:  49 :  1998.9695158838504\n",
      "epoch : 50  batch: 0\n",
      "loss after epoch:  50 :  1934.5895899024842\n",
      "epoch : 51  batch: 0\n",
      "loss after epoch:  51 :  1886.474325700568\n",
      "epoch : 52  batch: 0\n",
      "loss after epoch:  52 :  1854.097629976292\n",
      "epoch : 53  batch: 0\n",
      "loss after epoch:  53 :  1831.8714369826703\n",
      "epoch : 54  batch: 0\n",
      "loss after epoch:  54 :  1813.040794515845\n",
      "epoch : 55  batch: 0\n",
      "loss after epoch:  55 :  1793.4863170865192\n",
      "epoch : 56  batch: 0\n",
      "loss after epoch:  56 :  1773.4076558200902\n",
      "epoch : 57  batch: 0\n",
      "loss after epoch:  57 :  1756.2841570707005\n",
      "epoch : 58  batch: 0\n",
      "loss after epoch:  58 :  1746.0669648537205\n",
      "epoch : 59  batch: 0\n",
      "loss after epoch:  59 :  1744.39012534585\n",
      "epoch : 60  batch: 0\n",
      "loss after epoch:  60 :  1749.3585674541218\n",
      "epoch : 61  batch: 0\n",
      "loss after epoch:  61 :  1756.5082605678879\n",
      "epoch : 62  batch: 0\n",
      "loss after epoch:  62 :  1761.3425056637993\n",
      "epoch : 63  batch: 0\n",
      "loss after epoch:  63 :  1761.9318042461214\n",
      "epoch : 64  batch: 0\n",
      "loss after epoch:  64 :  1759.9397693462765\n",
      "epoch : 65  batch: 0\n",
      "loss after epoch:  65 :  1759.3086808067933\n",
      "epoch : 66  batch: 0\n",
      "loss after epoch:  66 :  1763.2613159916348\n",
      "epoch : 67  batch: 0\n",
      "loss after epoch:  67 :  1771.4336154960374\n",
      "epoch : 68  batch: 0\n",
      "loss after epoch:  68 :  1779.114032293487\n",
      "epoch : 69  batch: 0\n",
      "loss after epoch:  69 :  1779.3804886996013\n",
      "epoch : 70  batch: 0\n",
      "loss after epoch:  70 :  1767.126110390812\n",
      "epoch : 71  batch: 0\n",
      "loss after epoch:  71 :  1742.6854600939482\n",
      "epoch : 72  batch: 0\n",
      "loss after epoch:  72 :  1712.56973520186\n",
      "epoch : 73  batch: 0\n",
      "loss after epoch:  73 :  1686.273582417737\n",
      "epoch : 74  batch: 0\n",
      "loss after epoch:  74 :  1670.8417014341253\n",
      "epoch : 75  batch: 0\n",
      "loss after epoch:  75 :  1666.7442677023175\n",
      "epoch : 76  batch: 0\n",
      "loss after epoch:  76 :  1667.8984188616096\n",
      "epoch : 77  batch: 0\n",
      "loss after epoch:  77 :  1665.7202103313855\n",
      "epoch : 78  batch: 0\n",
      "loss after epoch:  78 :  1654.4623298331649\n",
      "epoch : 79  batch: 0\n",
      "loss after epoch:  79 :  1634.5824465012815\n",
      "epoch : 80  batch: 0\n",
      "loss after epoch:  80 :  1612.2824736341818\n",
      "epoch : 81  batch: 0\n",
      "loss after epoch:  81 :  1595.581177832234\n",
      "epoch : 82  batch: 0\n",
      "loss after epoch:  82 :  1589.151522371555\n",
      "epoch : 83  batch: 0\n",
      "loss after epoch:  83 :  1591.1935001770403\n",
      "epoch : 84  batch: 0\n",
      "loss after epoch:  84 :  1594.4676580039586\n",
      "epoch : 85  batch: 0\n",
      "loss after epoch:  85 :  1590.7322780490053\n",
      "epoch : 86  batch: 0\n",
      "loss after epoch:  86 :  1575.8780185172773\n",
      "epoch : 87  batch: 0\n",
      "loss after epoch:  87 :  1552.469350910326\n",
      "epoch : 88  batch: 0\n",
      "loss after epoch:  88 :  1527.9768447128636\n",
      "epoch : 89  batch: 0\n",
      "loss after epoch:  89 :  1510.1934429861867\n",
      "epoch : 90  batch: 0\n",
      "loss after epoch:  90 :  1503.077018748601\n",
      "epoch : 91  batch: 0\n",
      "loss after epoch:  91 :  1505.3140938242504\n",
      "epoch : 92  batch: 0\n",
      "loss after epoch:  92 :  1511.7881732581632\n",
      "epoch : 93  batch: 0\n",
      "loss after epoch:  93 :  1516.622608878408\n",
      "epoch : 94  batch: 0\n",
      "loss after epoch:  94 :  1515.9956857701202\n",
      "epoch : 95  batch: 0\n",
      "loss after epoch:  95 :  1509.3084704306516\n",
      "epoch : 96  batch: 0\n",
      "loss after epoch:  96 :  1498.5721268972716\n",
      "epoch : 97  batch: 0\n",
      "loss after epoch:  97 :  1487.1044079292255\n",
      "epoch : 98  batch: 0\n",
      "loss after epoch:  98 :  1478.2935509735883\n",
      "epoch : 99  batch: 0\n",
      "loss after epoch:  99 :  1474.3872001748864\n",
      "best accuracy: 0.9600833333333333\n",
      "best loss: 1474.3872001748864\n",
      "best epoch: 99\n",
      "The batch size is...1\n",
      "epoch : 0  batch: 0\n",
      "epoch : 0  batch: 1000\n",
      "epoch : 0  batch: 2000\n",
      "epoch : 0  batch: 3000\n",
      "epoch : 0  batch: 4000\n",
      "epoch : 0  batch: 5000\n",
      "epoch : 0  batch: 6000\n",
      "epoch : 0  batch: 7000\n",
      "epoch : 0  batch: 8000\n",
      "epoch : 0  batch: 9000\n",
      "epoch : 0  batch: 10000\n",
      "epoch : 0  batch: 11000\n",
      "loss after epoch:  0 :  96563.51044399134\n",
      "epoch : 1  batch: 0\n",
      "epoch : 1  batch: 1000\n",
      "epoch : 1  batch: 2000\n",
      "epoch : 1  batch: 3000\n",
      "epoch : 1  batch: 4000\n",
      "epoch : 1  batch: 5000\n",
      "epoch : 1  batch: 6000\n",
      "epoch : 1  batch: 7000\n",
      "epoch : 1  batch: 8000\n",
      "epoch : 1  batch: 9000\n",
      "epoch : 1  batch: 10000\n",
      "epoch : 1  batch: 11000\n",
      "loss after epoch:  1 :  96708.57330574996\n",
      "epoch : 2  batch: 0\n",
      "epoch : 2  batch: 1000\n",
      "epoch : 2  batch: 2000\n",
      "epoch : 2  batch: 3000\n",
      "epoch : 2  batch: 4000\n",
      "epoch : 2  batch: 5000\n",
      "epoch : 2  batch: 6000\n",
      "epoch : 2  batch: 7000\n",
      "epoch : 2  batch: 8000\n",
      "epoch : 2  batch: 9000\n",
      "epoch : 2  batch: 10000\n",
      "epoch : 2  batch: 11000\n",
      "loss after epoch:  2 :  96563.51044399134\n",
      "epoch : 3  batch: 0\n",
      "epoch : 3  batch: 1000\n",
      "epoch : 3  batch: 2000\n",
      "epoch : 3  batch: 3000\n",
      "epoch : 3  batch: 4000\n",
      "epoch : 3  batch: 5000\n",
      "epoch : 3  batch: 6000\n",
      "epoch : 3  batch: 7000\n",
      "epoch : 3  batch: 8000\n",
      "epoch : 3  batch: 9000\n",
      "epoch : 3  batch: 10000\n",
      "epoch : 3  batch: 11000\n",
      "loss after epoch:  3 :  96676.33711424805\n",
      "epoch : 4  batch: 0\n",
      "epoch : 4  batch: 1000\n",
      "epoch : 4  batch: 2000\n",
      "epoch : 4  batch: 3000\n",
      "epoch : 4  batch: 4000\n",
      "epoch : 4  batch: 5000\n",
      "epoch : 4  batch: 6000\n",
      "epoch : 4  batch: 7000\n",
      "epoch : 4  batch: 8000\n",
      "epoch : 4  batch: 9000\n",
      "epoch : 4  batch: 10000\n",
      "epoch : 4  batch: 11000\n",
      "loss after epoch:  4 :  96708.57330574996\n",
      "epoch : 5  batch: 0\n",
      "epoch : 5  batch: 1000\n",
      "epoch : 5  batch: 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5  batch: 3000\n",
      "epoch : 5  batch: 4000\n",
      "epoch : 5  batch: 5000\n",
      "epoch : 5  batch: 6000\n",
      "epoch : 5  batch: 7000\n",
      "epoch : 5  batch: 8000\n",
      "epoch : 5  batch: 9000\n",
      "epoch : 5  batch: 10000\n",
      "epoch : 5  batch: 11000\n",
      "loss after epoch:  5 :  96708.57330574996\n",
      "epoch : 6  batch: 0\n",
      "epoch : 6  batch: 1000\n",
      "epoch : 6  batch: 2000\n",
      "epoch : 6  batch: 3000\n",
      "epoch : 6  batch: 4000\n",
      "epoch : 6  batch: 5000\n",
      "epoch : 6  batch: 6000\n",
      "epoch : 6  batch: 7000\n",
      "epoch : 6  batch: 8000\n",
      "epoch : 6  batch: 9000\n",
      "epoch : 6  batch: 10000\n",
      "epoch : 6  batch: 11000\n",
      "loss after epoch:  6 :  96708.57330574996\n",
      "epoch : 7  batch: 0\n",
      "epoch : 7  batch: 1000\n",
      "epoch : 7  batch: 2000\n",
      "epoch : 7  batch: 3000\n",
      "epoch : 7  batch: 4000\n",
      "epoch : 7  batch: 5000\n",
      "epoch : 7  batch: 6000\n",
      "epoch : 7  batch: 7000\n",
      "epoch : 7  batch: 8000\n",
      "epoch : 7  batch: 9000\n",
      "epoch : 7  batch: 10000\n",
      "epoch : 7  batch: 11000\n",
      "loss after epoch:  7 :  96563.51044399134\n",
      "epoch : 8  batch: 0\n",
      "epoch : 8  batch: 1000\n",
      "epoch : 8  batch: 2000\n",
      "epoch : 8  batch: 3000\n",
      "epoch : 8  batch: 4000\n",
      "epoch : 8  batch: 5000\n",
      "epoch : 8  batch: 6000\n",
      "epoch : 8  batch: 7000\n",
      "epoch : 8  batch: 8000\n",
      "epoch : 8  batch: 9000\n",
      "epoch : 8  batch: 10000\n",
      "epoch : 8  batch: 11000\n",
      "loss after epoch:  8 :  96676.33711424805\n",
      "epoch : 9  batch: 0\n",
      "epoch : 9  batch: 1000\n",
      "epoch : 9  batch: 2000\n",
      "epoch : 9  batch: 3000\n",
      "epoch : 9  batch: 4000\n",
      "epoch : 9  batch: 5000\n",
      "epoch : 9  batch: 6000\n",
      "epoch : 9  batch: 7000\n",
      "epoch : 9  batch: 8000\n",
      "epoch : 9  batch: 9000\n",
      "epoch : 9  batch: 10000\n",
      "epoch : 9  batch: 11000\n",
      "loss after epoch:  9 :  96708.57330574996\n",
      "epoch : 10  batch: 0\n",
      "epoch : 10  batch: 1000\n",
      "epoch : 10  batch: 2000\n",
      "epoch : 10  batch: 3000\n",
      "epoch : 10  batch: 4000\n",
      "epoch : 10  batch: 5000\n",
      "epoch : 10  batch: 6000\n",
      "epoch : 10  batch: 7000\n",
      "epoch : 10  batch: 8000\n",
      "epoch : 10  batch: 9000\n",
      "epoch : 10  batch: 10000\n",
      "epoch : 10  batch: 11000\n",
      "loss after epoch:  10 :  96563.51044399134\n",
      "epoch : 11  batch: 0\n",
      "epoch : 11  batch: 1000\n",
      "epoch : 11  batch: 2000\n",
      "epoch : 11  batch: 3000\n",
      "epoch : 11  batch: 4000\n",
      "epoch : 11  batch: 5000\n",
      "epoch : 11  batch: 6000\n",
      "epoch : 11  batch: 7000\n",
      "epoch : 11  batch: 8000\n",
      "epoch : 11  batch: 9000\n",
      "epoch : 11  batch: 10000\n",
      "epoch : 11  batch: 11000\n",
      "loss after epoch:  11 :  96563.51044399134\n",
      "epoch : 12  batch: 0\n",
      "epoch : 12  batch: 1000\n",
      "epoch : 12  batch: 2000\n",
      "epoch : 12  batch: 3000\n",
      "epoch : 12  batch: 4000\n",
      "epoch : 12  batch: 5000\n",
      "epoch : 12  batch: 6000\n",
      "epoch : 12  batch: 7000\n",
      "epoch : 12  batch: 8000\n",
      "epoch : 12  batch: 9000\n",
      "epoch : 12  batch: 10000\n",
      "epoch : 12  batch: 11000\n",
      "loss after epoch:  12 :  96563.51044399134\n",
      "epoch : 13  batch: 0\n",
      "epoch : 13  batch: 1000\n",
      "epoch : 13  batch: 2000\n",
      "epoch : 13  batch: 3000\n",
      "epoch : 13  batch: 4000\n",
      "epoch : 13  batch: 5000\n",
      "epoch : 13  batch: 6000\n",
      "epoch : 13  batch: 7000\n",
      "epoch : 13  batch: 8000\n",
      "epoch : 13  batch: 9000\n",
      "epoch : 13  batch: 10000\n",
      "epoch : 13  batch: 11000\n",
      "loss after epoch:  13 :  96563.51044399134\n",
      "epoch : 14  batch: 0\n",
      "epoch : 14  batch: 1000\n",
      "epoch : 14  batch: 2000\n",
      "epoch : 14  batch: 3000\n",
      "epoch : 14  batch: 4000\n",
      "epoch : 14  batch: 5000\n",
      "epoch : 14  batch: 6000\n",
      "epoch : 14  batch: 7000\n",
      "epoch : 14  batch: 8000\n",
      "epoch : 14  batch: 9000\n",
      "epoch : 14  batch: 10000\n",
      "epoch : 14  batch: 11000\n",
      "loss after epoch:  14 :  96708.57330574996\n",
      "epoch : 15  batch: 0\n",
      "epoch : 15  batch: 1000\n",
      "epoch : 15  batch: 2000\n",
      "epoch : 15  batch: 3000\n",
      "epoch : 15  batch: 4000\n",
      "epoch : 15  batch: 5000\n",
      "epoch : 15  batch: 6000\n",
      "epoch : 15  batch: 7000\n",
      "epoch : 15  batch: 8000\n",
      "epoch : 15  batch: 9000\n",
      "epoch : 15  batch: 10000\n",
      "epoch : 15  batch: 11000\n",
      "loss after epoch:  15 :  96708.57330574996\n",
      "epoch : 16  batch: 0\n",
      "epoch : 16  batch: 1000\n",
      "epoch : 16  batch: 2000\n",
      "epoch : 16  batch: 3000\n",
      "epoch : 16  batch: 4000\n",
      "epoch : 16  batch: 5000\n",
      "epoch : 16  batch: 6000\n",
      "epoch : 16  batch: 7000\n",
      "epoch : 16  batch: 8000\n",
      "epoch : 16  batch: 9000\n",
      "epoch : 16  batch: 10000\n",
      "epoch : 16  batch: 11000\n",
      "loss after epoch:  16 :  96563.51044399134\n",
      "epoch : 17  batch: 0\n",
      "epoch : 17  batch: 1000\n",
      "epoch : 17  batch: 2000\n",
      "epoch : 17  batch: 3000\n",
      "epoch : 17  batch: 4000\n",
      "epoch : 17  batch: 5000\n",
      "epoch : 17  batch: 6000\n",
      "epoch : 17  batch: 7000\n",
      "epoch : 17  batch: 8000\n",
      "epoch : 17  batch: 9000\n",
      "epoch : 17  batch: 10000\n",
      "epoch : 17  batch: 11000\n",
      "loss after epoch:  17 :  96563.51044399134\n",
      "epoch : 18  batch: 0\n",
      "epoch : 18  batch: 1000\n",
      "epoch : 18  batch: 2000\n",
      "epoch : 18  batch: 3000\n",
      "epoch : 18  batch: 4000\n",
      "epoch : 18  batch: 5000\n",
      "epoch : 18  batch: 6000\n",
      "epoch : 18  batch: 7000\n",
      "epoch : 18  batch: 8000\n",
      "epoch : 18  batch: 9000\n",
      "epoch : 18  batch: 10000\n",
      "epoch : 18  batch: 11000\n",
      "loss after epoch:  18 :  96708.57330574996\n",
      "epoch : 19  batch: 0\n",
      "epoch : 19  batch: 1000\n",
      "epoch : 19  batch: 2000\n",
      "epoch : 19  batch: 3000\n",
      "epoch : 19  batch: 4000\n",
      "epoch : 19  batch: 5000\n",
      "epoch : 19  batch: 6000\n",
      "epoch : 19  batch: 7000\n",
      "epoch : 19  batch: 8000\n",
      "epoch : 19  batch: 9000\n",
      "epoch : 19  batch: 10000\n",
      "epoch : 19  batch: 11000\n",
      "loss after epoch:  19 :  96708.57330574996\n",
      "epoch : 20  batch: 0\n",
      "epoch : 20  batch: 1000\n",
      "epoch : 20  batch: 2000\n",
      "epoch : 20  batch: 3000\n",
      "epoch : 20  batch: 4000\n",
      "epoch : 20  batch: 5000\n",
      "epoch : 20  batch: 6000\n",
      "epoch : 20  batch: 7000\n",
      "epoch : 20  batch: 8000\n",
      "epoch : 20  batch: 9000\n",
      "epoch : 20  batch: 10000\n",
      "epoch : 20  batch: 11000\n",
      "loss after epoch:  20 :  96708.57330574996\n",
      "epoch : 21  batch: 0\n",
      "epoch : 21  batch: 1000\n",
      "epoch : 21  batch: 2000\n",
      "epoch : 21  batch: 3000\n",
      "epoch : 21  batch: 4000\n",
      "epoch : 21  batch: 5000\n",
      "epoch : 21  batch: 6000\n",
      "epoch : 21  batch: 7000\n",
      "epoch : 21  batch: 8000\n",
      "epoch : 21  batch: 9000\n",
      "epoch : 21  batch: 10000\n",
      "epoch : 21  batch: 11000\n",
      "loss after epoch:  21 :  96708.57330574996\n",
      "epoch : 22  batch: 0\n",
      "epoch : 22  batch: 1000\n",
      "epoch : 22  batch: 2000\n",
      "epoch : 22  batch: 3000\n",
      "epoch : 22  batch: 4000\n",
      "epoch : 22  batch: 5000\n",
      "epoch : 22  batch: 6000\n",
      "epoch : 22  batch: 7000\n",
      "epoch : 22  batch: 8000\n",
      "epoch : 22  batch: 9000\n",
      "epoch : 22  batch: 10000\n",
      "epoch : 22  batch: 11000\n",
      "loss after epoch:  22 :  96708.57330574996\n",
      "epoch : 23  batch: 0\n",
      "epoch : 23  batch: 1000\n",
      "epoch : 23  batch: 2000\n",
      "epoch : 23  batch: 3000\n",
      "epoch : 23  batch: 4000\n",
      "epoch : 23  batch: 5000\n",
      "epoch : 23  batch: 6000\n",
      "epoch : 23  batch: 7000\n",
      "epoch : 23  batch: 8000\n",
      "epoch : 23  batch: 9000\n",
      "epoch : 23  batch: 10000\n",
      "epoch : 23  batch: 11000\n",
      "loss after epoch:  23 :  96563.51044399134\n",
      "epoch : 24  batch: 0\n",
      "epoch : 24  batch: 1000\n",
      "epoch : 24  batch: 2000\n",
      "epoch : 24  batch: 3000\n",
      "epoch : 24  batch: 4000\n",
      "epoch : 24  batch: 5000\n",
      "epoch : 24  batch: 6000\n",
      "epoch : 24  batch: 7000\n",
      "epoch : 24  batch: 8000\n",
      "epoch : 24  batch: 9000\n",
      "epoch : 24  batch: 10000\n",
      "epoch : 24  batch: 11000\n",
      "loss after epoch:  24 :  96563.51044399134\n",
      "epoch : 25  batch: 0\n",
      "epoch : 25  batch: 1000\n",
      "epoch : 25  batch: 2000\n",
      "epoch : 25  batch: 3000\n",
      "epoch : 25  batch: 4000\n",
      "epoch : 25  batch: 5000\n",
      "epoch : 25  batch: 6000\n",
      "epoch : 25  batch: 7000\n",
      "epoch : 25  batch: 8000\n",
      "epoch : 25  batch: 9000\n",
      "epoch : 25  batch: 10000\n",
      "epoch : 25  batch: 11000\n",
      "loss after epoch:  25 :  96563.51044399134\n",
      "epoch : 26  batch: 0\n",
      "epoch : 26  batch: 1000\n",
      "epoch : 26  batch: 2000\n",
      "epoch : 26  batch: 3000\n",
      "epoch : 26  batch: 4000\n",
      "epoch : 26  batch: 5000\n",
      "epoch : 26  batch: 6000\n",
      "epoch : 26  batch: 7000\n",
      "epoch : 26  batch: 8000\n",
      "epoch : 26  batch: 9000\n",
      "epoch : 26  batch: 10000\n",
      "epoch : 26  batch: 11000\n",
      "loss after epoch:  26 :  96563.51044399134\n",
      "epoch : 27  batch: 0\n",
      "epoch : 27  batch: 1000\n",
      "epoch : 27  batch: 2000\n",
      "epoch : 27  batch: 3000\n",
      "epoch : 27  batch: 4000\n",
      "epoch : 27  batch: 5000\n",
      "epoch : 27  batch: 6000\n",
      "epoch : 27  batch: 7000\n",
      "epoch : 27  batch: 8000\n",
      "epoch : 27  batch: 9000\n",
      "epoch : 27  batch: 10000\n",
      "epoch : 27  batch: 11000\n",
      "loss after epoch:  27 :  96563.51044399134\n",
      "epoch : 28  batch: 0\n",
      "epoch : 28  batch: 1000\n",
      "epoch : 28  batch: 2000\n",
      "epoch : 28  batch: 3000\n",
      "epoch : 28  batch: 4000\n",
      "epoch : 28  batch: 5000\n",
      "epoch : 28  batch: 6000\n",
      "epoch : 28  batch: 7000\n",
      "epoch : 28  batch: 8000\n",
      "epoch : 28  batch: 9000\n",
      "epoch : 28  batch: 10000\n",
      "epoch : 28  batch: 11000\n",
      "loss after epoch:  28 :  96821.39997600667\n",
      "epoch : 29  batch: 0\n",
      "epoch : 29  batch: 1000\n",
      "epoch : 29  batch: 2000\n",
      "epoch : 29  batch: 3000\n",
      "epoch : 29  batch: 4000\n",
      "epoch : 29  batch: 5000\n",
      "epoch : 29  batch: 6000\n",
      "epoch : 29  batch: 7000\n",
      "epoch : 29  batch: 8000\n",
      "epoch : 29  batch: 9000\n",
      "epoch : 29  batch: 10000\n",
      "epoch : 29  batch: 11000\n",
      "loss after epoch:  29 :  96708.57330574996\n",
      "epoch : 30  batch: 0\n",
      "epoch : 30  batch: 1000\n",
      "epoch : 30  batch: 2000\n",
      "epoch : 30  batch: 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 30  batch: 4000\n",
      "epoch : 30  batch: 5000\n",
      "epoch : 30  batch: 6000\n",
      "epoch : 30  batch: 7000\n",
      "epoch : 30  batch: 8000\n",
      "epoch : 30  batch: 9000\n",
      "epoch : 30  batch: 10000\n",
      "epoch : 30  batch: 11000\n",
      "loss after epoch:  30 :  96563.51044399134\n",
      "epoch : 31  batch: 0\n",
      "epoch : 31  batch: 1000\n",
      "epoch : 31  batch: 2000\n",
      "epoch : 31  batch: 3000\n",
      "epoch : 31  batch: 4000\n",
      "epoch : 31  batch: 5000\n",
      "epoch : 31  batch: 6000\n",
      "epoch : 31  batch: 7000\n",
      "epoch : 31  batch: 8000\n",
      "epoch : 31  batch: 9000\n",
      "epoch : 31  batch: 10000\n",
      "epoch : 31  batch: 11000\n",
      "loss after epoch:  31 :  96708.57330574996\n",
      "epoch : 32  batch: 0\n",
      "epoch : 32  batch: 1000\n",
      "epoch : 32  batch: 2000\n",
      "epoch : 32  batch: 3000\n",
      "epoch : 32  batch: 4000\n",
      "epoch : 32  batch: 5000\n",
      "epoch : 32  batch: 6000\n",
      "epoch : 32  batch: 7000\n",
      "epoch : 32  batch: 8000\n",
      "epoch : 32  batch: 9000\n",
      "epoch : 32  batch: 10000\n",
      "epoch : 32  batch: 11000\n",
      "loss after epoch:  32 :  96563.51044399134\n",
      "epoch : 33  batch: 0\n",
      "epoch : 33  batch: 1000\n",
      "epoch : 33  batch: 2000\n",
      "epoch : 33  batch: 3000\n",
      "epoch : 33  batch: 4000\n",
      "epoch : 33  batch: 5000\n",
      "epoch : 33  batch: 6000\n",
      "epoch : 33  batch: 7000\n",
      "epoch : 33  batch: 8000\n",
      "epoch : 33  batch: 9000\n",
      "epoch : 33  batch: 10000\n",
      "epoch : 33  batch: 11000\n",
      "loss after epoch:  33 :  96563.51044399134\n",
      "epoch : 34  batch: 0\n",
      "epoch : 34  batch: 1000\n",
      "epoch : 34  batch: 2000\n",
      "epoch : 34  batch: 3000\n",
      "epoch : 34  batch: 4000\n",
      "epoch : 34  batch: 5000\n",
      "epoch : 34  batch: 6000\n",
      "epoch : 34  batch: 7000\n",
      "epoch : 34  batch: 8000\n",
      "epoch : 34  batch: 9000\n",
      "epoch : 34  batch: 10000\n",
      "epoch : 34  batch: 11000\n",
      "loss after epoch:  34 :  96563.51044399134\n",
      "epoch : 35  batch: 0\n",
      "epoch : 35  batch: 1000\n",
      "epoch : 35  batch: 2000\n",
      "epoch : 35  batch: 3000\n",
      "epoch : 35  batch: 4000\n",
      "epoch : 35  batch: 5000\n",
      "epoch : 35  batch: 6000\n",
      "epoch : 35  batch: 7000\n",
      "epoch : 35  batch: 8000\n",
      "epoch : 35  batch: 9000\n",
      "epoch : 35  batch: 10000\n",
      "epoch : 35  batch: 11000\n",
      "loss after epoch:  35 :  96563.51044399134\n",
      "epoch : 36  batch: 0\n",
      "epoch : 36  batch: 1000\n",
      "epoch : 36  batch: 2000\n",
      "epoch : 36  batch: 3000\n",
      "epoch : 36  batch: 4000\n",
      "epoch : 36  batch: 5000\n",
      "epoch : 36  batch: 6000\n",
      "epoch : 36  batch: 7000\n",
      "epoch : 36  batch: 8000\n",
      "epoch : 36  batch: 9000\n",
      "epoch : 36  batch: 10000\n",
      "epoch : 36  batch: 11000\n",
      "loss after epoch:  36 :  96708.57330574996\n",
      "epoch : 37  batch: 0\n",
      "epoch : 37  batch: 1000\n",
      "epoch : 37  batch: 2000\n",
      "epoch : 37  batch: 3000\n",
      "epoch : 37  batch: 4000\n",
      "epoch : 37  batch: 5000\n",
      "epoch : 37  batch: 6000\n",
      "epoch : 37  batch: 7000\n",
      "epoch : 37  batch: 8000\n",
      "epoch : 37  batch: 9000\n",
      "epoch : 37  batch: 10000\n",
      "epoch : 37  batch: 11000\n",
      "loss after epoch:  37 :  96563.51044399134\n",
      "epoch : 38  batch: 0\n",
      "epoch : 38  batch: 1000\n",
      "epoch : 38  batch: 2000\n",
      "epoch : 38  batch: 3000\n",
      "epoch : 38  batch: 4000\n",
      "epoch : 38  batch: 5000\n",
      "epoch : 38  batch: 6000\n",
      "epoch : 38  batch: 7000\n",
      "epoch : 38  batch: 8000\n",
      "epoch : 38  batch: 9000\n",
      "epoch : 38  batch: 10000\n",
      "epoch : 38  batch: 11000\n",
      "loss after epoch:  38 :  96563.51044399134\n",
      "epoch : 39  batch: 0\n",
      "epoch : 39  batch: 1000\n",
      "epoch : 39  batch: 2000\n",
      "epoch : 39  batch: 3000\n",
      "epoch : 39  batch: 4000\n",
      "epoch : 39  batch: 5000\n",
      "epoch : 39  batch: 6000\n",
      "epoch : 39  batch: 7000\n",
      "epoch : 39  batch: 8000\n",
      "epoch : 39  batch: 9000\n",
      "epoch : 39  batch: 10000\n",
      "epoch : 39  batch: 11000\n",
      "loss after epoch:  39 :  96563.51044399134\n",
      "epoch : 40  batch: 0\n",
      "epoch : 40  batch: 1000\n",
      "epoch : 40  batch: 2000\n",
      "epoch : 40  batch: 3000\n",
      "epoch : 40  batch: 4000\n",
      "epoch : 40  batch: 5000\n",
      "epoch : 40  batch: 6000\n",
      "epoch : 40  batch: 7000\n",
      "epoch : 40  batch: 8000\n",
      "epoch : 40  batch: 9000\n",
      "epoch : 40  batch: 10000\n",
      "epoch : 40  batch: 11000\n",
      "loss after epoch:  40 :  96563.51044399134\n",
      "epoch : 41  batch: 0\n",
      "epoch : 41  batch: 1000\n",
      "epoch : 41  batch: 2000\n",
      "epoch : 41  batch: 3000\n",
      "epoch : 41  batch: 4000\n",
      "epoch : 41  batch: 5000\n",
      "epoch : 41  batch: 6000\n",
      "epoch : 41  batch: 7000\n",
      "epoch : 41  batch: 8000\n",
      "epoch : 41  batch: 9000\n",
      "epoch : 41  batch: 10000\n",
      "epoch : 41  batch: 11000\n",
      "loss after epoch:  41 :  96563.51044399134\n",
      "epoch : 42  batch: 0\n",
      "epoch : 42  batch: 1000\n",
      "epoch : 42  batch: 2000\n",
      "epoch : 42  batch: 3000\n",
      "epoch : 42  batch: 4000\n",
      "epoch : 42  batch: 5000\n",
      "epoch : 42  batch: 6000\n",
      "epoch : 42  batch: 7000\n",
      "epoch : 42  batch: 8000\n",
      "epoch : 42  batch: 9000\n",
      "epoch : 42  batch: 10000\n"
     ]
    }
   ],
   "source": [
    "# Set path to data\n",
    "def unit_test_training(momentum):\n",
    "    \n",
    "    modes = [\"full\", \"stochastic\", \"mini\"]\n",
    "    data_path = '../setup/data'\n",
    "    epochs = 100\n",
    "    h1 = 32\n",
    "    lr = 0.1\n",
    "    batch_size = 10\n",
    "    momentum = True\n",
    "    model_name = '{}_with_momentum' if momentum==True else '{}_without_momentum'\n",
    "\n",
    "    for mode in modes:\n",
    "        \n",
    "        model_name = model_name.format(mode)\n",
    "        run_training(data_path, epochs, model_name, h1, lr, mode, batch_size, momentum)\n",
    "\n",
    "\n",
    "call_training(momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4.2 Hyperparameter tuning using gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(momentum_grid, lr_grid, batch_size_grid, h1):\n",
    "\n",
    "\n",
    "    best_params = {}\n",
    "    best_params['loss'] = np.inf\n",
    "    best_params['momentum'] = 0\n",
    "    best_params['lr'] = 0\n",
    "    best_params['batch_size'] = 0\n",
    "    best_params[\"weights\"] = 0\n",
    "    best_params[\"biases\"] = 0\n",
    "    \n",
    "    for m in momentum_grid:\n",
    "        for l in lr_grid:\n",
    "            for bs in batch_size_grid:\n",
    "                \n",
    "                weights, biases = initialize(X_train_flattened, h1)\n",
    "                velocity = initialize_velocity(weights,biases)\n",
    "                epochs = 50\n",
    "                \n",
    "                history = {\n",
    "                    \"weights\": [weights],\n",
    "                    \"losses\": [], \n",
    "                    \"biases\": [biases],\n",
    "                    \"accuracies\": [],\n",
    "                    \"velocity\":[velocity]\n",
    "                }\n",
    "                \n",
    "                history = batch_training_with_momentum(bs,weights,biases,history,X_train_flattened,y_train,m,l)\n",
    "                best_epoch = get_best_results(history)\n",
    "                print(history['accuracies'][best_epoch],best_epoch)\n",
    "                \n",
    "                activations_dev = forward_pass(X_dev_flattened, \n",
    "                                               history[\"weights\"][best_epoch], \n",
    "                                               history[\"biases\"][best_epoch])\n",
    "                y_prob = activations_dev[-1]\n",
    "                dev_loss = get_log_loss(y_dev,y_prob)\n",
    "                \n",
    "                if dev_loss < best_params['loss'] :\n",
    "                    best_params['momentum'] = m\n",
    "                    best_params['lr'] = l\n",
    "                    best_params['batch_size'] = bs\n",
    "                    best_params['loss'] = dev_loss\n",
    "                    best_params[\"weights\"] = history[\"weights\"][best_epoch]\n",
    "                    best_params[\"biases\"] = history[\"biases\"][best_epoch]\n",
    "            \n",
    "    return(best_params)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store initial parameters\n",
    "h1 = 32\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store grids for grid search\n",
    "momentum = np.arange(0.001,0.01,0.001)\n",
    "lr = np.arange(0.01,0.1,0.01)\n",
    "batch_size = [16,64,256,1024,4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search and get best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with the best parameters\n",
    "history = execute_pipeline(**best_params)\n",
    "plot_loss(\"loss.png\", history[\"dev_loss\"][:-2],label = \"validation loss\")\n",
    "plot_loss(\"accuracy.png\", history[\"dev_accuracies\"][:-2], label='validation Accuracy')\n",
    "history[\"dev_accuracies\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
