{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from helpers import load_all_data, vectorized_flatten, sigmoid, get_log_loss, get_accuracy, sigmoid_derivative, gradient_update, plot_loss\n",
    "from helpers import sgd_with_momentum_update, prep_data,  get_best_epoch, get_results\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_two_layers(X, h1): \n",
    "    '''\n",
    "    --------------------\n",
    "    Parameter Initialization\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X [n = 12000])\n",
    "    --------------------\n",
    "    Output: \n",
    "    weights: Weight terms initialized as random normals\n",
    "    biases: Bias terms initialized to zero\n",
    "    --------------------\n",
    "    '''\n",
    "    dim1 = 1/np.sqrt(X.shape[0])\n",
    "    W1 = dim1 * np.random.randn(h1, 28**2)\n",
    "    \n",
    "    dim2 = 1/np.sqrt(W1.shape[1])\n",
    "    W2 = dim2 * np.random.randn(1, h1)\n",
    "    \n",
    "    b1 = np.zeros((h1, 1))\n",
    "    b2 = np.zeros((1, 1))\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_two_layers(X, weights, biases):\n",
    "    '''\n",
    "    ----------------------------------\n",
    "    Forward propogation:\n",
    "    Send inputs through the network to\n",
    "    generate output\n",
    "    ----------------------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    weights: Binary (1/0) training label (shape = n X 1)\n",
    "    biases:\n",
    "    --------------------\n",
    "    Output: \n",
    "    activations: vector of results from passing\n",
    "    inputs through each neuron\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    z1 = W1 @ X + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    activations = (z1, a1, z2, a2)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation_two_layers(X, y, weights, biases, activations):\n",
    "    '''\n",
    "    --------------------\n",
    "    Backpropagation\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    activations: Current set of activations\n",
    "    --------------------\n",
    "    Output: \n",
    "    Derivatives required\n",
    "    for optimization update\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    z1, a1, z2, a2 = activations\n",
    "    m = y.shape[1]\n",
    "    #print(m)   \n",
    "    \n",
    "    dz2 = a2 - y\n",
    "    #print(\"dz3\", dz3.shape)\n",
    "    \n",
    "    dW2 = np.dot(dz2, a1.T)/m\n",
    "    #print(\"dW2\", dW2.shape)\n",
    "    \n",
    "    db2 = np.sum(dz2, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db2\", db2.shape)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    #print(\"da1\", da1.shape)\n",
    "    \n",
    "    dz1 = da1 * sigmoid_derivative(z1)\n",
    "    #print(\"dz1\", dz1.shape)\n",
    "    \n",
    "    dW1 = np.dot(dz1, X.T)/m\n",
    "    #print(\"dW1\", dW1.shape)\n",
    "    \n",
    "    db1 = np.sum(dz1, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db1\", db1.shape)\n",
    "    \n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_differences(example, truth, weights, biases, delta_h=1e-9):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    I, J = W2.shape # Change here\n",
    "    \n",
    "    deltaW = np.zeros((I, J))\n",
    "    \n",
    "    activations = forward_pass_two_layers(example, weights, biases)\n",
    "    db1, dW1, db2, dW2 = backpropagation_two_layers(example, truth, weights, biases, activations)\n",
    "    \n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "    \n",
    "            W_plus = np.copy(W2) # Change here\n",
    "            W_minus = np.copy(W2) # Change here\n",
    "            \n",
    "            W_plus[i][j] += delta_h\n",
    "            W_minus[i][j] -= delta_h\n",
    "            \n",
    "            weights_plus = [W1, W_plus] # Change here\n",
    "            weights_minus = [W1, W_minus] # Change here\n",
    "            \n",
    "            activations_plus = forward_pass_two_layers(example, weights_plus, biases)\n",
    "            activations_minus = forward_pass_two_layers(example, weights_minus, biases)\n",
    "\n",
    "            loss_plus = get_log_loss(truth, activations_plus[-1])\n",
    "            loss_minus =  get_log_loss(truth, activations_minus[-1])\n",
    "\n",
    "            deltaW[i][j] = (loss_plus - loss_minus)/(2 * delta_h)\n",
    "\n",
    "    difference = np.linalg.norm(dW2 - deltaW) # Change here\n",
    "    \n",
    "    return dW2, deltaW, difference, activations_plus[-1], activations_minus[-1] # Change here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check finite differences\n",
    "def run_finite_differences(data_path, h1 = 8, idx=10):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "   \n",
    "    X_train_flattened, _, _, y_train, _, _ = prep_data(data_path)\n",
    "    w, b = initialize_two_layers(X_train_flattened, h1)\n",
    "    dW, deltaW, difference, activations_plus, activations_minus = finite_differences(X_train_flattened[:, idx].reshape(-1, 1), \n",
    "                                                                  y_train[:, idx].reshape(-1, 1), w, b)\n",
    "    \n",
    "    print(\"dW\", dW)\n",
    "    print(\"deltaW\", deltaW)\n",
    "    print(\"difference\", difference)\n",
    "    print(activations_plus, activations_minus)\n",
    "    \n",
    "    return(dW, deltaW, difference, activations_plus, activations_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_without_momentum(weights, biases, gradients, learning_rate):\n",
    "    '''\n",
    "    --------------------\n",
    "    Update parameters\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    \n",
    "    W1 = gradient_update(W1, learning_rate, dW1)\n",
    "    W2 = gradient_update(W2, learning_rate, dW2)\n",
    "   \n",
    "    b1 = gradient_update(b1, learning_rate, db1)\n",
    "    b2 = gradient_update(b2, learning_rate, db2)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(weights,biases):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    vw1 = np.zeros(W1.shape)\n",
    "    vw2 = np.zeros(W2.shape)\n",
    "    vb1 = np.zeros(b1.shape)\n",
    "    vb2 = np.zeros(b2.shape)\n",
    "    return vw1,vw2,vb1,vb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(weights,biases,gradients,learning_rate,velocity,momentum):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    vw1,vw2,vb1,vb2 = velocity\n",
    "    W1,vw1 = sgd_with_momentum_update(W1, learning_rate, dW1,vw1,momentum)\n",
    "    W2,vw2 = sgd_with_momentum_update(W2, learning_rate, dW2,vw2,momentum)\n",
    "   \n",
    "    b1,vb1 = sgd_with_momentum_update(b1, learning_rate, db1,vb1,momentum)\n",
    "    b2,vb2 = sgd_with_momentum_update(b2, learning_rate, db2,vb2,momentum)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    velocity = (vw1,vw2,vb1,vb2)\n",
    "    return weights ,biases,velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer(activations_full):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    y_prob = activations_full[-1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    \n",
    "    return(y_prob, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(X):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training(batch_size, weights, biases, epochs,\n",
    "                   X, y, momentum_param, lr, X_dev,y_dev, velocity):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    history = {\n",
    "        \"weights\": [weights],\n",
    "        \"losses\": [], \n",
    "        \"biases\": [biases],\n",
    "        \"accuracies\": [],\n",
    "        \"velocity\":[velocity],\n",
    "        \"dev_accuracies\" :[],\n",
    "        \"dev_loss\":[]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        offset = 0\n",
    "        weights = history['weights'][epoch]\n",
    "        biases = history['biases'][epoch]\n",
    "        velocity = history['velocity'][epoch]\n",
    "        \n",
    "        while offset <max(y.shape):\n",
    "            if offset%1000==0 :\n",
    "                print(\"epoch :\",epoch,\" batch:\",offset)\n",
    "            else :\n",
    "                a=1\n",
    "            if offset+batch_size >=max(y.shape):\n",
    "                X_batch = X[:,offset:]\n",
    "                y_batch = y[:,offset:]\n",
    "            else :    \n",
    "                X_batch = X[:,offset:offset+batch_size]\n",
    "                y_batch = y[:,offset:offset+batch_size]\n",
    "            \n",
    "            activations = forward_pass_two_layers(X_batch, weights, biases)\n",
    "            gradients = backpropagation_two_layers(X_batch, y_batch, weights, biases, activations)\n",
    "            weights, biases,velocity = update_parameters_with_momentum(weights, biases, gradients, \n",
    "                                                                       lr, velocity, momentum_param)\n",
    "            offset = offset+batch_size\n",
    "        \n",
    "        activations_full = forward_pass_two_layers(X, weights, biases)\n",
    "        y_prob = activations_full[-1]\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "\n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "        \n",
    "        activations_dev = forward_pass_two_layers(X_dev,weights,biases)\n",
    "        y_dev_prob =  activations_dev[-1]\n",
    "        y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "        \n",
    "        loss_dev = get_log_loss(y_dev,y_dev_prob)\n",
    "        accuracy_dev = get_accuracy(y_dev,y_dev_pred)\n",
    "\n",
    "        history[\"weights\"].append(weights)\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(biases)\n",
    "        history[\"velocity\"].append(velocity)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "        history[\"dev_accuracies\"].append(accuracy_dev)\n",
    "        history['dev_loss'].append(loss_dev)\n",
    "\n",
    "        if np.isnan(loss):\n",
    "            break\n",
    "        print(\"loss after epoch: \",epoch,\": \",loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(data_path, epochs, mode, model_name, h1, lr, batch_size, momentum_param, grid_search = False):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    np.random.seed(1252908)\n",
    "    \n",
    "    X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test = prep_data(data_path)\n",
    "    \n",
    "    if mode == 'full': batch_size = max(y_train.shape)\n",
    "    elif mode == 'stochastic': batch_size = 1\n",
    "    \n",
    "    weights, biases = initialize_two_layers(X_train_flattened, h1)\n",
    "    velocity = initialize_velocity(weights, biases)\n",
    "    \n",
    "    history = batch_training(batch_size, weights, biases, \n",
    "                             epochs, X_train_flattened, y_train, \n",
    "                             momentum_param, lr, X_dev_flattened, y_dev, \n",
    "                             velocity)\n",
    "    \n",
    "    best_epoch, _, _ = get_best_epoch(history)\n",
    "    best_dev_epoch, _, _ = get_best_dev_epoch(history)\n",
    "\n",
    "    # Plots\n",
    "    plot_loss(\"{}_loss.png\".format(model_name), history[\"losses\"][:-2])\n",
    "    plot_loss(\"{}_accuracy.png\".format(model_name), history[\"accuracies\"][:-2], label='Training Accuracy')\n",
    "    \n",
    "    # Plot dev loss\n",
    "    plot_loss(\"{}_dev_loss.png\".format(model_name), history[\"dev_loss\"][:-2])\n",
    "    plot_loss(\"{}_dev_accuracy.png\".format(model_name), history[\"dev_accuracies\"][:-2])\n",
    "    \n",
    "    # If we are running grid search return the dev. loss to calling function to compare \n",
    "    if grid_search:\n",
    "        activations_dev = forward_pass_two_layers(X_dev_flattened, history[\"weights\"][best_dev_epoch], \n",
    "                                                  history[\"biases\"][best_dev_epoch])\n",
    "        y_prob = activations_dev[-1]\n",
    "        dev_loss = get_log_loss(y_dev,y_prob)\n",
    "        return(history, best_epoch, dev_loss)  \n",
    "        \n",
    "    else:\n",
    "        return(history, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grid_search(data_path, epochs, mode, model_name, h1, \n",
    "                lr_grid, batch_size_grid, momentum_grid, sampling_frac=0.1):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    best_params = {}\n",
    "    best_params['loss'] = np.inf\n",
    "    best_params['momentum'] = 0\n",
    "    best_params['lr'] = 0\n",
    "    best_params['batch_size'] = 0\n",
    "    best_params[\"weights\"] = 0\n",
    "    best_params[\"biases\"] = 0\n",
    "    \n",
    "    sampled_grid = [np.random.choice(grid, int(sampling_frac*len(grid)), replace = False) \n",
    "                    for grid in [momentum_grid, lr_grid, batch_size_grid]]\n",
    "    \n",
    "    momentum_grid, lr_grid, batch_size_grid = *sampled_grid\n",
    "\n",
    "    total_iters = len(momentum_grid)*len(lr_grid)*len(batch_size_grid)\n",
    "    i = 0\n",
    "                                     \n",
    "    for m in momentum_grid:\n",
    "        for lr in lr_grid:\n",
    "            for bs in batch_size_grid:\n",
    "                \n",
    "                print(\"We are {} % done!\".format(i/total_iters))\n",
    "                \n",
    "                history, best_epoch, dev_loss = run_training(data_path, \n",
    "                                                             epochs, mode, model_name, \n",
    "                                                             h1, lr, bs, m, grid_search = True)\n",
    "                \n",
    "                if dev_loss < best_params['loss'] :\n",
    "                    best_params['momentum'] = m\n",
    "                    best_params['lr'] = lr\n",
    "                    best_params['batch_size'] = bs\n",
    "                    best_params['loss'] = dev_loss\n",
    "                    best_params[\"weights\"] = history[\"weights\"][best_epoch]\n",
    "                    best_params[\"biases\"] = history[\"biases\"][best_epoch]\n",
    "                \n",
    "                i+=1\n",
    "            \n",
    "    return(best_params)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for momentum and grid search demonstration\n",
    "modes = ['full', 'stochastic', 'mini']\n",
    "data_path = '../setup/data'\n",
    "batch_size = 10\n",
    "epochs = 100 \n",
    "h1 = 8 \n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we train without momentum\n",
    "# This is why we set the momentum_param to 0\n",
    "# This is equivalent to running gradient descent\n",
    "results_without_momentum = {'full':'', 'stochastic':'', 'batch': ''}\n",
    "model_name = '../figs/{}_without_momentum'\n",
    "momentum_param = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We go through each mode with the training call\n",
    "for mode in modes:\n",
    "    fig_name = model_name.format(mode)\n",
    "    results_without_momentum[mode] = run_training(data_path, \n",
    "                                                  epochs, mode, fig_name, h1, \n",
    "                                                  lr, batch_size, momentum_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we conduct the same rounds of training with momentum\n",
    "results_with_momentum = {'full':'', 'stochastic':'', 'batch': ''}\n",
    "model_name = '../figs/{}_with_momentum'\n",
    "momentum_param = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call with momentum\n",
    "for mode in modes:\n",
    "    fig_name = model_name.format(mode)\n",
    "    results_without_momentum[mode] = run_training(data_path, \n",
    "                                                  epochs, mode, fig_name, h1, \n",
    "                                                  lr, batch_size, momentum_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use grid search for hyper-parameter optimization\n",
    "mode = 'mini'\n",
    "model_name = 'grid_search_result'\n",
    "epochs = 2 \n",
    "\n",
    "# These are the grids that we will search over\n",
    "lr_grid = np.arange(0.1, 0.2, 0.1)\n",
    "batch_size_grid = np.arange(1, 12000, 5999)\n",
    "momentum_grid = np.arange(0.1, 0.9, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we call the function\n",
    "grid_search(data_path, epochs, mode, model_name, h1, lr_grid, batch_size_grid, momentum_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
