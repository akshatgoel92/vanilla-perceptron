{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from helpers import load_all_data, vectorized_flatten, sigmoid, get_log_loss, get_accuracy, sigmoid_derivative, gradient_update, get_loss_plot, plot_loss\n",
    "from helpers import sgd_with_momentum_update\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data_path):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    Use vectorized flatten\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Load\n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_all_data(data_path)\n",
    "    \n",
    "    # Flatten\n",
    "    X_train_flattened = vectorized_flatten(X_train)\n",
    "    X_dev_flattened = vectorized_flatten(X_dev)\n",
    "    X_test_flattened = vectorized_flatten(X_test)\n",
    "    \n",
    "    # Reshape labels\n",
    "    y_train = y_train.reshape(1, -1)\n",
    "    y_dev = y_dev.reshape(1, -1)\n",
    "    y_test = y_test.reshape(1, -1)\n",
    "    \n",
    "    # Return\n",
    "    return(X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X): \n",
    "    '''\n",
    "    --------------------\n",
    "    Parameter Initialization\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X [n = 12000])\n",
    "    --------------------\n",
    "    Output: \n",
    "    weights: Weight terms initialized as random normals\n",
    "    biases: Bias terms initialized to zero\n",
    "    --------------------\n",
    "    '''\n",
    "    dim1 = 1/np.sqrt(X.shape[0])\n",
    "    W1 = dim1 * np.random.randn(h1, 28**2)\n",
    "    \n",
    "    dim2 = 1/np.sqrt(W1.shape[1])\n",
    "    W2 = dim2 * np.random.randn(1, h1)\n",
    "    \n",
    "    b1 = np.zeros((h1, 1))\n",
    "    b2 = np.zeros((1, 1))\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, weights, biases):\n",
    "    '''\n",
    "    ----------------------------------\n",
    "    Forward propogation:\n",
    "    Send inputs through the network to\n",
    "    generate output\n",
    "    ----------------------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    weights: Binary (1/0) training label (shape = n X 1)\n",
    "    biases:\n",
    "    --------------------\n",
    "    Output: \n",
    "    activations: vector of results from passing\n",
    "    inputs through each neuron\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    z1 = W1 @ X + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    activations = (z1, a1, z2, a2)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, weights, biases, activations):\n",
    "    '''\n",
    "    --------------------\n",
    "    Backpropagation\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    activations: Current set of activations\n",
    "    --------------------\n",
    "    Output: \n",
    "    Derivatives required\n",
    "    for optimization update\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    z1, a1, z2, a2 = activations\n",
    "    m = y.shape[1]\n",
    "    #print(m)   \n",
    "    dz2 = a2 - y\n",
    "    #print(\"dz3\", dz3.shape)\n",
    "    \n",
    "   \n",
    "    dW2 = np.dot(dz2, a1.T)/m\n",
    "    #print(\"dW2\", dW2.shape)\n",
    "    \n",
    "    db2 = np.sum(dz2, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db2\", db2.shape)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    #print(\"da1\", da1.shape)\n",
    "    \n",
    "    dz1 = da1 * sigmoid_derivative(z1)\n",
    "    #print(\"dz1\", dz1.shape)\n",
    "    \n",
    "    dW1 = np.dot(dz1, X.T)/m\n",
    "    #print(\"dW1\", dW1.shape)\n",
    "    \n",
    "    db1 = np.sum(dz1, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db1\", db1.shape)\n",
    "    \n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    '''\n",
    "    --------------------\n",
    "    Update parameters\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    \n",
    "    W1 = gradient_update(W1, learning_rate, dW1)\n",
    "    W2 = gradient_update(W2, learning_rate, dW2)\n",
    "   \n",
    "    b1 = gradient_update(b1, learning_rate, db1)\n",
    "    b2 = gradient_update(b2, learning_rate, db2)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(weights,biases,gradients,learning_rate,velocity,momentum):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    vw1,vw2,vb1,vb2 = velocity\n",
    "    W1,vw1 = sgd_with_momentum_update(W1, learning_rate, dW1,vw1,momentum)\n",
    "    W2,vw2 = sgd_with_momentum_update(W2, learning_rate, dW2,vw2,momentum)\n",
    "   \n",
    "    b1,vb1 = sgd_with_momentum_update(b1, learning_rate, db1,vb1,momentum)\n",
    "    b2,vb2 = sgd_with_momentum_update(b2, learning_rate, db2,vb2,momentum)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    velocity = (vw1,vw2,vb1,vb2)\n",
    "    return weights ,biases,velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training_without_momentum(batch_size, weights,biases, epochs, X,y):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    history = {\n",
    "    \"weights\": [weights],\n",
    "    \"losses\": [], \n",
    "    \"biases\": [biases],\n",
    "    \"accuracies\": []\n",
    "    \n",
    "    }\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        offset = 0\n",
    "        weights = history['weights'][epoch]\n",
    "        biases = history['biases'][epoch]\n",
    "        \n",
    "        while offset < max(y_train.shape):\n",
    "            \n",
    "            if offset%1000==0 :\n",
    "                print(\"epoch :\",epoch,\" batch:\",offset)\n",
    "            else :\n",
    "                a=1\n",
    "            if offset+batch_size >=max(y_train.shape):\n",
    "                X_batch = X[:,offset:]\n",
    "                y_batch = y[:,offset:]\n",
    "            else :    \n",
    "                X_batch = X[:,offset:offset+batch_size]\n",
    "                y_batch = y[:,offset:offset+batch_size]\n",
    "            \n",
    "            \n",
    "            activations = forward_pass(X_batch, weights, biases)\n",
    "            gradients = backpropagation(X_batch, y_batch, weights, biases, activations)\n",
    "\n",
    "            weights, biases = update_parameters(weights, biases, gradients, lr)\n",
    "            offset = offset+batch_size\n",
    "        \n",
    "        activations_full = forward_pass(X, weights, biases)\n",
    "        y_prob = activations_full[-1]\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    \n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "\n",
    "        history[\"weights\"].append(weights)\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(biases)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "\n",
    "        if np.isnan(loss):\n",
    "            break\n",
    "\n",
    "        print(\"loss after epoch: \",epoch,\": \",loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(weights,biases):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    vw1 = np.zeros(W1.shape)\n",
    "    vw2 = np.zeros(W2.shape)\n",
    "    vb1 = np.zeros(b1.shape)\n",
    "    vb2 = np.zeros(b2.shape)\n",
    "    return vw1,vw2,vb1,vb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training_with_momentum(batch_size,weights,biases,history,X,y,momentum,lr,X_dev,y_dev):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    \n",
    "    history = {\n",
    "        \"weights\": [weights],\n",
    "        \"losses\": [], \n",
    "        \"biases\": [biases],\n",
    "        \"accuracies\": [],\n",
    "        \"velocity\":[velocity],\n",
    "        \"dev_accuracies\" :[],\n",
    "        \"dev_loss\":[]\n",
    "    }\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        offset = 0\n",
    "        weights = history['weights'][epoch]\n",
    "        biases = history['biases'][epoch]\n",
    "        velocity = history['velocity'][epoch]\n",
    "        \n",
    "        while offset <max(y_train.shape):\n",
    "            if offset%1000==0 :\n",
    "                print(\"epoch :\",epoch,\" batch:\",offset)\n",
    "            else :\n",
    "                a=1\n",
    "            if offset+batch_size >=max(y_train.shape):\n",
    "                X_batch = X[:,offset:]\n",
    "                y_batch = y[:,offset:]\n",
    "            else :    \n",
    "                X_batch = X[:,offset:offset+batch_size]\n",
    "                y_batch = y[:,offset:offset+batch_size]\n",
    "            \n",
    "            activations = forward_pass(X_batch, weights, biases)\n",
    "            gradients = backpropagation(X_batch, y_batch, weights, biases, activations)\n",
    "\n",
    "            \n",
    "            weights, biases,velocity = update_parameters_with_momentum(weights, biases, gradients, \n",
    "                                                                       lr,velocity,momentum)\n",
    "            offset = offset+batch_size\n",
    "        \n",
    "        activations_full = forward_pass(X, weights, biases)\n",
    "        y_prob = activations_full[-1]\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "\n",
    "        #print(y,y_prob)\n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "        \n",
    "        activations_dev = forward_pass(X_dev,weights,biases)\n",
    "        y_dev_prob =  activations_dev[-1]\n",
    "        y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "        loss_dev = get_log_loss(y_dev,y_dev_prob)\n",
    "        accuracy_dev = get_accuracy(y_dev,y_dev_pred)\n",
    "\n",
    "        history[\"weights\"].append(weights)\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(biases)\n",
    "        history[\"velocity\"].append(velocity)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "        history[\"dev_accuracies\"].append(accuracy_dev)\n",
    "        history['dev_loss'].append(loss_dev)\n",
    "\n",
    "        if np.isnan(loss):\n",
    "            break\n",
    "\n",
    "        print(\"loss after epoch: \",epoch,\": \",loss)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_epoch(history):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Store results\n",
    "    best_epoch = np.array(history[\"losses\"]).argmin()\n",
    "    best_accuracy = history['accuracies'][best_epoch]\n",
    "    best_loss = history['losses'][best_epoch]\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"best accuracy: {history['accuracies'][best_epoch]}\")\n",
    "    print(f\"best loss: {history['losses'][best_epoch]}\")\n",
    "    print(f\"best epoch: {best_epoch}\")\n",
    "    \n",
    "    return(best_epoch, best_accuracy, best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(X_dev, y_dev, history, best_epoch, label=\"dev\"):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    w = history[\"weights\"][best_epoch]\n",
    "    b = history[\"biases\"][best_epoch]\n",
    "    activations = forward_pass(X_dev, w, b)\n",
    "\n",
    "    y_dev_prob = activations[-1]\n",
    "    y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "\n",
    "    loss = get_log_loss(y_dev, y_dev_prob)\n",
    "    accuracy = get_accuracy(y_dev, y_dev_pred)\n",
    "    print(f\"{label} set accuracy: {accuracy}\")\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    weights, biases = initialize(X_train_flattened)\n",
    "    epochs = 200\n",
    "    \n",
    "    history = batch_training_without_momentum(max(y_train.shape),weights,biases,history,X_train_flattened,y_train)\n",
    "    best_epoch = get_best_epoch(history)\n",
    "    plot_loss(\"loss.png\", history[\"losses\"][:-2])\n",
    "    plot_loss(\"accuracy.png\", history[\"accuracies\"][:-2], label='Training Accuracy')\n",
    "    \n",
    "    return(history, best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4.2 Hyperparameter tuning using gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(momentum, lr, batch_size):\n",
    "\n",
    "\n",
    "    best_params = {}\n",
    "    best_params['loss'] = np.inf\n",
    "    best_params['momentum'] = 0\n",
    "    best_params['lr'] = 0\n",
    "    best_params['batch_size'] = 0\n",
    "    best_params[\"weights\"] = 0\n",
    "    best_params[\"biases\"] = 0\n",
    "    \n",
    "    for m in momentum :\n",
    "        for l in lr :\n",
    "            for bs in batch_size :\n",
    "                \n",
    "                weights, biases = initialize(X_train_flattened)\n",
    "                epochs = 50\n",
    "                velocity = initialize_velocity(weights,biases)\n",
    "                \n",
    "                history = {\n",
    "                    \"weights\": [weights],\n",
    "                    \"losses\": [], \n",
    "                    \"biases\": [biases],\n",
    "                    \"accuracies\": [],\n",
    "                    \"velocity\":[velocity]\n",
    "                }\n",
    "                \n",
    "                history = batch_training_with_momentum(bs,weights,biases,history,X_train_flattened,y_train,m,l)\n",
    "                best_epoch = get_best_results(history)\n",
    "                print(history['accuracies'][best_epoch],best_epoch)\n",
    "                \n",
    "                activations_dev = forward_pass(X_dev_flattened, history[\"weights\"][best_epoch], history[\"biases\"][best_epoch])\n",
    "                y_prob = activations_dev[-1]\n",
    "                dev_loss = get_log_loss(y_dev,y_prob)\n",
    "                \n",
    "                if dev_loss < best_params['loss'] :\n",
    "                    best_params['momentum'] = m\n",
    "                    best_params['lr'] = l\n",
    "                    best_params['batch_size'] = bs\n",
    "                    best_params['loss'] = dev_loss\n",
    "                    best_params[\"weights\"] = history[\"weights\"][best_epoch]\n",
    "                    best_params[\"biases\"] = history[\"biases\"][best_epoch]\n",
    "            \n",
    "    return(best_params)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store initial parameters\n",
    "h1 = 32\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store grids for grid search\n",
    "momentum = np.arange(0.001,0.01,0.001)\n",
    "lr = np.arange(0.01,0.1,0.01)\n",
    "batch_size = [16,64,256,1024,4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search and get best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with the best parameters\n",
    "history = execute_pipeline(**best_params)\n",
    "plot_loss(\"loss.png\", history[\"dev_loss\"][:-2],label = \"validation loss\")\n",
    "plot_loss(\"accuracy.png\", history[\"dev_accuracies\"][:-2], label='validation Accuracy')\n",
    "history[\"dev_accuracies\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
