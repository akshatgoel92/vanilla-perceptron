{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from helpers import load_all_data, vectorized_flatten, sigmoid, get_log_loss, get_accuracy, sigmoid_derivative, gradient_update, plot_loss\n",
    "from helpers import sgd_with_momentum_update, prep_data,  get_best_epoch, get_results\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_two_layers(X, h1): \n",
    "    '''\n",
    "    --------------------\n",
    "    Parameter Initialization\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X [n = 12000])\n",
    "    --------------------\n",
    "    Output: \n",
    "    weights: Weight terms initialized as random normals\n",
    "    biases: Bias terms initialized to zero\n",
    "    --------------------\n",
    "    '''\n",
    "    dim1 = 1/np.sqrt(X.shape[0])\n",
    "    W1 = dim1 * np.random.randn(h1, 28**2)\n",
    "    \n",
    "    dim2 = 1/np.sqrt(W1.shape[1])\n",
    "    W2 = dim2 * np.random.randn(1, h1)\n",
    "    \n",
    "    b1 = np.zeros((h1, 1))\n",
    "    b2 = np.zeros((1, 1))\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_two_layers(X, weights, biases):\n",
    "    '''\n",
    "    ----------------------------------\n",
    "    Forward propogation:\n",
    "    Send inputs through the network to\n",
    "    generate output\n",
    "    ----------------------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    weights: Binary (1/0) training label (shape = n X 1)\n",
    "    biases:\n",
    "    --------------------\n",
    "    Output: \n",
    "    activations: vector of results from passing\n",
    "    inputs through each neuron\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    z1 = W1 @ X + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    activations = (z1, a1, z2, a2)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation_two_layers(X, y, weights, biases, activations):\n",
    "    '''\n",
    "    --------------------\n",
    "    Backpropagation\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    activations: Current set of activations\n",
    "    --------------------\n",
    "    Output: \n",
    "    Derivatives required\n",
    "    for optimization update\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    z1, a1, z2, a2 = activations\n",
    "    m = y.shape[1]\n",
    "    #print(m)   \n",
    "    \n",
    "    dz2 = a2 - y\n",
    "    #print(\"dz3\", dz3.shape)\n",
    "    \n",
    "    dW2 = np.dot(dz2, a1.T)/m\n",
    "    #print(\"dW2\", dW2.shape)\n",
    "    \n",
    "    db2 = np.sum(dz2, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db2\", db2.shape)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    #print(\"da1\", da1.shape)\n",
    "    \n",
    "    dz1 = da1 * sigmoid_derivative(z1)\n",
    "    #print(\"dz1\", dz1.shape)\n",
    "    \n",
    "    dW1 = np.dot(dz1, X.T)/m\n",
    "    #print(\"dW1\", dW1.shape)\n",
    "    \n",
    "    db1 = np.sum(dz1, axis=1).reshape(-1, 1)/m\n",
    "    #print(\"db1\", db1.shape)\n",
    "    \n",
    "    return db1, dW1, db2, dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_differences():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_without_momentum(weights, biases, gradients, learning_rate):\n",
    "    '''\n",
    "    --------------------\n",
    "    Update parameters\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    \n",
    "    W1 = gradient_update(W1, learning_rate, dW1)\n",
    "    W2 = gradient_update(W2, learning_rate, dW2)\n",
    "   \n",
    "    b1 = gradient_update(b1, learning_rate, db1)\n",
    "    b2 = gradient_update(b2, learning_rate, db2)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(weights,biases):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    vw1 = np.zeros(W1.shape)\n",
    "    vw2 = np.zeros(W2.shape)\n",
    "    vb1 = np.zeros(b1.shape)\n",
    "    vb2 = np.zeros(b2.shape)\n",
    "    return vw1,vw2,vb1,vb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(weights,biases,gradients,learning_rate,velocity,momentum):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2 = weights\n",
    "    b1, b2 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2 = gradients\n",
    "    vw1,vw2,vb1,vb2 = velocity\n",
    "    W1,vw1 = sgd_with_momentum_update(W1, learning_rate, dW1,vw1,momentum)\n",
    "    W2,vw2 = sgd_with_momentum_update(W2, learning_rate, dW2,vw2,momentum)\n",
    "   \n",
    "    b1,vb1 = sgd_with_momentum_update(b1, learning_rate, db1,vb1,momentum)\n",
    "    b2,vb2 = sgd_with_momentum_update(b2, learning_rate, db2,vb2,momentum)\n",
    "    \n",
    "    weights = (W1, W2)\n",
    "    biases = (b1, b2)\n",
    "    velocity = (vw1,vw2,vb1,vb2)\n",
    "    return weights ,biases,velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer():\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    y_prob = activations_full[-1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    \n",
    "    return(y_prob, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(X):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_training(batch_size, weights, biases, epochs,\n",
    "                   X, y, momentum_param, lr, X_dev,y_dev, velocity):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    history = {\n",
    "        \"weights\": [weights],\n",
    "        \"losses\": [], \n",
    "        \"biases\": [biases],\n",
    "        \"accuracies\": [],\n",
    "        \"velocity\":[velocity],\n",
    "        \"dev_accuracies\" :[],\n",
    "        \"dev_loss\":[]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        offset = 0\n",
    "        weights = history['weights'][epoch]\n",
    "        biases = history['biases'][epoch]\n",
    "        velocity = history['velocity'][epoch]\n",
    "        \n",
    "        while offset <max(y.shape):\n",
    "            if offset%1000==0 :\n",
    "                print(\"epoch :\",epoch,\" batch:\",offset)\n",
    "            else :\n",
    "                a=1\n",
    "            if offset+batch_size >=max(y.shape):\n",
    "                X_batch = X[:,offset:]\n",
    "                y_batch = y[:,offset:]\n",
    "            else :    \n",
    "                X_batch = X[:,offset:offset+batch_size]\n",
    "                y_batch = y[:,offset:offset+batch_size]\n",
    "            \n",
    "            activations = forward_pass_two_layers(X_batch, weights, biases)\n",
    "            gradients = backpropagation_two_layers(X_batch, y_batch, weights, biases, activations)\n",
    "            weights, biases,velocity = update_parameters_with_momentum(weights, biases, gradients, \n",
    "                                                                       lr, velocity, momentum_param)\n",
    "            offset = offset+batch_size\n",
    "        \n",
    "        activations_full = forward_pass_two_layers(X, weights, biases)\n",
    "        y_prob = activations_full[-1]\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "\n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "        \n",
    "        activations_dev = forward_pass_two_layers(X_dev,weights,biases)\n",
    "        y_dev_prob =  activations_dev[-1]\n",
    "        y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "        \n",
    "        loss_dev = get_log_loss(y_dev,y_dev_prob)\n",
    "        accuracy_dev = get_accuracy(y_dev,y_dev_pred)\n",
    "\n",
    "        history[\"weights\"].append(weights)\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(biases)\n",
    "        history[\"velocity\"].append(velocity)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "        history[\"dev_accuracies\"].append(accuracy_dev)\n",
    "        history['dev_loss'].append(loss_dev)\n",
    "\n",
    "        if np.isnan(loss):\n",
    "            break\n",
    "        print(\"loss after epoch: \",epoch,\": \",loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(data_path, epochs, mode, model_name, h1, lr, batch_size, momentum_param):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Set seed for reproducible results\n",
    "    np.random.seed(1252908)\n",
    "    \n",
    "    # Get data\n",
    "    X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test = prep_data(data_path)\n",
    "    \n",
    "    # According to mode\n",
    "    if mode == 'full': \n",
    "        batch_size = max(y_train.shape)\n",
    "    elif mode == 'stochastic':\n",
    "        batch_size = 1\n",
    "    \n",
    "    # Print to check batch size is correct\n",
    "    print(\"The batch size is...{} with mode {}\".format(batch_size, mode))\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    weights, biases = initialize_two_layers(X_train_flattened, h1)\n",
    "    \n",
    "    # Call training\n",
    "    velocity = initialize_velocity(weights, biases)\n",
    "    history = batch_training(batch_size, weights, biases, \n",
    "                             epochs, X_train_flattened, y_train, \n",
    "                             momentum_param, lr, X_dev_flattened, y_dev, velocity)\n",
    "    \n",
    "    # Get best epoch from history object\n",
    "    best_epoch = get_best_epoch(history)\n",
    "    \n",
    "    # Plot loss and save\n",
    "    plot_loss(\"loss_{}.png\".format(model_name), history[\"losses\"][:-2])\n",
    "    \n",
    "    # Plot accuracy and save\n",
    "    plot_loss(\"accuracy_{}.png\".format(model_name), history[\"accuracies\"][:-2], label='Training Accuracy')\n",
    "    \n",
    "    return(history, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(momentum_grid, lr_grid, batch_size_grid, h1):\n",
    "\n",
    "    best_params = {}\n",
    "    best_params['loss'] = np.inf\n",
    "    best_params['momentum'] = 0\n",
    "    best_params['lr'] = 0\n",
    "    best_params['batch_size'] = 0\n",
    "    best_params[\"weights\"] = 0\n",
    "    best_params[\"biases\"] = 0\n",
    "    \n",
    "    for m in momentum_grid:\n",
    "        for l in lr_grid:\n",
    "            for bs in batch_size_grid:\n",
    "                \n",
    "                weights, biases = initialize(X_train_flattened, h1)\n",
    "                velocity = initialize_velocity(weights,biases)\n",
    "                epochs = 50\n",
    "                \n",
    "                history = {\n",
    "                    \"weights\": [weights],\n",
    "                    \"losses\": [], \n",
    "                    \"biases\": [biases],\n",
    "                    \"accuracies\": [],\n",
    "                    \"velocity\":[velocity]\n",
    "                }\n",
    "                \n",
    "                history = batch_training_with_momentum(bs,weights,biases,history,X_train_flattened,y_train,m,l)\n",
    "                best_epoch = get_best_epoch(history)\n",
    "                print(history['accuracies'][best_epoch],best_epoch)\n",
    "                \n",
    "                activations_dev = forward_pass(X_dev_flattened, \n",
    "                                               history[\"weights\"][best_epoch], \n",
    "                                               history[\"biases\"][best_epoch])\n",
    "                y_prob = activations_dev[-1]\n",
    "                dev_loss = get_log_loss(y_dev,y_prob)\n",
    "                \n",
    "                if dev_loss < best_params['loss'] :\n",
    "                    best_params['momentum'] = m\n",
    "                    best_params['lr'] = l\n",
    "                    best_params['batch_size'] = bs\n",
    "                    best_params['loss'] = dev_loss\n",
    "                    best_params[\"weights\"] = history[\"weights\"][best_epoch]\n",
    "                    best_params[\"biases\"] = history[\"biases\"][best_epoch]\n",
    "            \n",
    "    return(best_params)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "modes = ['full', 'stochastic', 'mini']\n",
    "data_path = '../setup/data'\n",
    "batch_size = 10\n",
    "epochs = 100 \n",
    "h1 = 32 \n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '..figs/{}_without_momentum'\n",
    "momentum_param = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch size is...12000 with mode full\n",
      "epoch : 0  batch: 0\n",
      "loss after epoch:  0 :  8271.200264586612\n",
      "epoch : 1  batch: 0\n",
      "loss after epoch:  1 :  8257.007938066472\n",
      "epoch : 2  batch: 0\n",
      "loss after epoch:  2 :  8236.431169220825\n",
      "epoch : 3  batch: 0\n",
      "loss after epoch:  3 :  8209.58810294501\n",
      "epoch : 4  batch: 0\n",
      "loss after epoch:  4 :  8176.261813542554\n",
      "epoch : 5  batch: 0\n",
      "loss after epoch:  5 :  8135.862272257615\n",
      "epoch : 6  batch: 0\n",
      "loss after epoch:  6 :  8087.405748666673\n",
      "epoch : 7  batch: 0\n",
      "loss after epoch:  7 :  8029.513611158557\n",
      "epoch : 8  batch: 0\n",
      "loss after epoch:  8 :  7960.437280698944\n",
      "epoch : 9  batch: 0\n",
      "loss after epoch:  9 :  7878.11498128352\n",
      "epoch : 10  batch: 0\n",
      "loss after epoch:  10 :  7780.256707478431\n",
      "epoch : 11  batch: 0\n",
      "loss after epoch:  11 :  7664.446376669439\n",
      "epoch : 12  batch: 0\n",
      "loss after epoch:  12 :  7528.259619452246\n",
      "epoch : 13  batch: 0\n",
      "loss after epoch:  13 :  7369.427990314881\n",
      "epoch : 14  batch: 0\n",
      "loss after epoch:  14 :  7186.118288951651\n",
      "epoch : 15  batch: 0\n",
      "loss after epoch:  15 :  6977.396028619878\n",
      "epoch : 16  batch: 0\n",
      "loss after epoch:  16 :  6743.853915286975\n",
      "epoch : 17  batch: 0\n",
      "loss after epoch:  17 :  6488.200494043065\n",
      "epoch : 18  batch: 0\n",
      "loss after epoch:  18 :  6215.432970515665\n",
      "epoch : 19  batch: 0\n",
      "loss after epoch:  19 :  5932.303928046322\n",
      "epoch : 20  batch: 0\n",
      "loss after epoch:  20 :  5646.24115066908\n",
      "epoch : 21  batch: 0\n",
      "loss after epoch:  21 :  5364.284966679719\n",
      "epoch : 22  batch: 0\n",
      "loss after epoch:  22 :  5092.390069538233\n",
      "epoch : 23  batch: 0\n",
      "loss after epoch:  23 :  4834.8833378696145\n",
      "epoch : 24  batch: 0\n",
      "loss after epoch:  24 :  4593.912095097337\n",
      "epoch : 25  batch: 0\n",
      "loss after epoch:  25 :  4369.287171985858\n",
      "epoch : 26  batch: 0\n",
      "loss after epoch:  26 :  4159.1220007193515\n",
      "epoch : 27  batch: 0\n",
      "loss after epoch:  27 :  3960.9288362088087\n",
      "epoch : 28  batch: 0\n",
      "loss after epoch:  28 :  3772.4403508078585\n",
      "epoch : 29  batch: 0\n",
      "loss after epoch:  29 :  3591.895169102145\n",
      "epoch : 30  batch: 0\n",
      "loss after epoch:  30 :  3418.126020354644\n",
      "epoch : 31  batch: 0\n",
      "loss after epoch:  31 :  3250.7681471398696\n",
      "epoch : 32  batch: 0\n",
      "loss after epoch:  32 :  3090.4537205480447\n",
      "epoch : 33  batch: 0\n",
      "loss after epoch:  33 :  2938.6634277657604\n",
      "epoch : 34  batch: 0\n",
      "loss after epoch:  34 :  2797.206205861103\n",
      "epoch : 35  batch: 0\n",
      "loss after epoch:  35 :  2667.6549299069243\n",
      "epoch : 36  batch: 0\n",
      "loss after epoch:  36 :  2551.0320587347164\n",
      "epoch : 37  batch: 0\n",
      "loss after epoch:  37 :  2447.7319223357813\n",
      "epoch : 38  batch: 0\n",
      "loss after epoch:  38 :  2357.5094583631526\n",
      "epoch : 39  batch: 0\n",
      "loss after epoch:  39 :  2279.484299887379\n",
      "epoch : 40  batch: 0\n",
      "loss after epoch:  40 :  2212.2430909185196\n",
      "epoch : 41  batch: 0\n",
      "loss after epoch:  41 :  2154.068380450132\n",
      "epoch : 42  batch: 0\n",
      "loss after epoch:  42 :  2103.1967859925235\n",
      "epoch : 43  batch: 0\n",
      "loss after epoch:  43 :  2058.0039138439324\n",
      "epoch : 44  batch: 0\n",
      "loss after epoch:  44 :  2017.107857116914\n",
      "epoch : 45  batch: 0\n",
      "loss after epoch:  45 :  1979.431694007398\n",
      "epoch : 46  batch: 0\n",
      "loss after epoch:  46 :  1944.2320779696042\n",
      "epoch : 47  batch: 0\n",
      "loss after epoch:  47 :  1911.0722223585408\n",
      "epoch : 48  batch: 0\n",
      "loss after epoch:  48 :  1879.7444372773548\n",
      "epoch : 49  batch: 0\n",
      "loss after epoch:  49 :  1850.1835586903885\n",
      "epoch : 50  batch: 0\n",
      "loss after epoch:  50 :  1822.4050321399598\n",
      "epoch : 51  batch: 0\n",
      "loss after epoch:  51 :  1796.4658583400455\n",
      "epoch : 52  batch: 0\n",
      "loss after epoch:  52 :  1772.4302861978363\n",
      "epoch : 53  batch: 0\n",
      "loss after epoch:  53 :  1750.3356714348108\n",
      "epoch : 54  batch: 0\n",
      "loss after epoch:  54 :  1730.1695945633269\n",
      "epoch : 55  batch: 0\n",
      "loss after epoch:  55 :  1711.8663698139756\n",
      "epoch : 56  batch: 0\n",
      "loss after epoch:  56 :  1695.3174750072126\n",
      "epoch : 57  batch: 0\n",
      "loss after epoch:  57 :  1680.3842401946554\n",
      "epoch : 58  batch: 0\n",
      "loss after epoch:  58 :  1666.906882805552\n",
      "epoch : 59  batch: 0\n",
      "loss after epoch:  59 :  1654.712039418128\n",
      "epoch : 60  batch: 0\n",
      "loss after epoch:  60 :  1643.6224008994852\n",
      "epoch : 61  batch: 0\n",
      "loss after epoch:  61 :  1633.4678575472858\n",
      "epoch : 62  batch: 0\n",
      "loss after epoch:  62 :  1624.094314954118\n",
      "epoch : 63  batch: 0\n",
      "loss after epoch:  63 :  1615.3676369431018\n",
      "epoch : 64  batch: 0\n",
      "loss after epoch:  64 :  1607.1737624299262\n",
      "epoch : 65  batch: 0\n",
      "loss after epoch:  65 :  1599.4178295601478\n",
      "epoch : 66  batch: 0\n",
      "loss after epoch:  66 :  1592.0238567494193\n",
      "epoch : 67  batch: 0\n",
      "loss after epoch:  67 :  1584.934292457801\n",
      "epoch : 68  batch: 0\n",
      "loss after epoch:  68 :  1578.108079034374\n",
      "epoch : 69  batch: 0\n",
      "loss after epoch:  69 :  1571.5170814064818\n",
      "epoch : 70  batch: 0\n",
      "loss after epoch:  70 :  1565.142120309192\n",
      "epoch : 71  batch: 0\n",
      "loss after epoch:  71 :  1558.9699241784572\n",
      "epoch : 72  batch: 0\n",
      "loss after epoch:  72 :  1552.9912436187562\n",
      "epoch : 73  batch: 0\n",
      "loss after epoch:  73 :  1547.1994462778268\n",
      "epoch : 74  batch: 0\n",
      "loss after epoch:  74 :  1541.5889824035885\n",
      "epoch : 75  batch: 0\n",
      "loss after epoch:  75 :  1536.1538384047951\n",
      "epoch : 76  batch: 0\n",
      "loss after epoch:  76 :  1530.8865491389251\n",
      "epoch : 77  batch: 0\n",
      "loss after epoch:  77 :  1525.7780751512964\n",
      "epoch : 78  batch: 0\n",
      "loss after epoch:  78 :  1520.8182587109936\n",
      "epoch : 79  batch: 0\n",
      "loss after epoch:  79 :  1515.9963087665099\n",
      "epoch : 80  batch: 0\n",
      "loss after epoch:  80 :  1511.3010309902415\n",
      "epoch : 81  batch: 0\n",
      "loss after epoch:  81 :  1506.7209462504702\n",
      "epoch : 82  batch: 0\n",
      "loss after epoch:  82 :  1502.2445762719733\n",
      "epoch : 83  batch: 0\n",
      "loss after epoch:  83 :  1497.8609536793406\n",
      "epoch : 84  batch: 0\n",
      "loss after epoch:  84 :  1493.5601419144505\n",
      "epoch : 85  batch: 0\n",
      "loss after epoch:  85 :  1489.333521928122\n",
      "epoch : 86  batch: 0\n",
      "loss after epoch:  86 :  1485.1738059180916\n",
      "epoch : 87  batch: 0\n",
      "loss after epoch:  87 :  1481.0749328976372\n",
      "epoch : 88  batch: 0\n",
      "loss after epoch:  88 :  1477.032003650396\n",
      "epoch : 89  batch: 0\n",
      "loss after epoch:  89 :  1473.0412676909436\n",
      "epoch : 90  batch: 0\n",
      "loss after epoch:  90 :  1469.1000612600328\n",
      "epoch : 91  batch: 0\n",
      "loss after epoch:  91 :  1465.206621391761\n",
      "epoch : 92  batch: 0\n",
      "loss after epoch:  92 :  1461.35981245238\n",
      "epoch : 93  batch: 0\n",
      "loss after epoch:  93 :  1457.5588699303241\n",
      "epoch : 94  batch: 0\n",
      "loss after epoch:  94 :  1453.8032316205095\n",
      "epoch : 95  batch: 0\n",
      "loss after epoch:  95 :  1450.0924415638774\n",
      "epoch : 96  batch: 0\n",
      "loss after epoch:  96 :  1446.4260673885547\n",
      "epoch : 97  batch: 0\n",
      "loss after epoch:  97 :  1442.8035990254084\n",
      "epoch : 98  batch: 0\n",
      "loss after epoch:  98 :  1439.224351613007\n",
      "epoch : 99  batch: 0\n",
      "loss after epoch:  99 :  1435.687416430364\n",
      "best accuracy: 0.96275\n",
      "best loss: 1435.687416430364\n",
      "best epoch: 99\n",
      "The batch size is...1 with mode stochastic\n",
      "epoch : 0  batch: 0\n",
      "epoch : 0  batch: 1000\n",
      "epoch : 0  batch: 2000\n",
      "epoch : 0  batch: 3000\n",
      "epoch : 0  batch: 4000\n",
      "epoch : 0  batch: 5000\n",
      "epoch : 0  batch: 6000\n",
      "epoch : 0  batch: 7000\n",
      "epoch : 0  batch: 8000\n",
      "epoch : 0  batch: 9000\n",
      "epoch : 0  batch: 10000\n",
      "epoch : 0  batch: 11000\n",
      "loss after epoch:  0 :  26708.188838221147\n",
      "epoch : 1  batch: 0\n",
      "epoch : 1  batch: 1000\n",
      "epoch : 1  batch: 2000\n",
      "epoch : 1  batch: 3000\n",
      "epoch : 1  batch: 4000\n",
      "epoch : 1  batch: 5000\n",
      "epoch : 1  batch: 6000\n",
      "epoch : 1  batch: 7000\n",
      "epoch : 1  batch: 8000\n",
      "epoch : 1  batch: 9000\n",
      "epoch : 1  batch: 10000\n",
      "epoch : 1  batch: 11000\n",
      "loss after epoch:  1 :  7687.151751531645\n",
      "epoch : 2  batch: 0\n",
      "epoch : 2  batch: 1000\n",
      "epoch : 2  batch: 2000\n",
      "epoch : 2  batch: 3000\n",
      "epoch : 2  batch: 4000\n",
      "epoch : 2  batch: 5000\n",
      "epoch : 2  batch: 6000\n",
      "epoch : 2  batch: 7000\n",
      "epoch : 2  batch: 8000\n",
      "epoch : 2  batch: 9000\n",
      "epoch : 2  batch: 10000\n",
      "epoch : 2  batch: 11000\n",
      "loss after epoch:  2 :  5885.16041109139\n",
      "epoch : 3  batch: 0\n",
      "epoch : 3  batch: 1000\n",
      "epoch : 3  batch: 2000\n",
      "epoch : 3  batch: 3000\n",
      "epoch : 3  batch: 4000\n",
      "epoch : 3  batch: 5000\n",
      "epoch : 3  batch: 6000\n",
      "epoch : 3  batch: 7000\n",
      "epoch : 3  batch: 8000\n",
      "epoch : 3  batch: 9000\n",
      "epoch : 3  batch: 10000\n",
      "epoch : 3  batch: 11000\n",
      "loss after epoch:  3 :  5047.0355577087\n",
      "epoch : 4  batch: 0\n",
      "epoch : 4  batch: 1000\n",
      "epoch : 4  batch: 2000\n",
      "epoch : 4  batch: 3000\n",
      "epoch : 4  batch: 4000\n",
      "epoch : 4  batch: 5000\n",
      "epoch : 4  batch: 6000\n",
      "epoch : 4  batch: 7000\n",
      "epoch : 4  batch: 8000\n",
      "epoch : 4  batch: 9000\n",
      "epoch : 4  batch: 10000\n",
      "epoch : 4  batch: 11000\n",
      "loss after epoch:  4 :  8178.127233185931\n",
      "epoch : 5  batch: 0\n",
      "epoch : 5  batch: 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5  batch: 2000\n",
      "epoch : 5  batch: 3000\n",
      "epoch : 5  batch: 4000\n",
      "epoch : 5  batch: 5000\n",
      "epoch : 5  batch: 6000\n",
      "epoch : 5  batch: 7000\n",
      "epoch : 5  batch: 8000\n",
      "epoch : 5  batch: 9000\n",
      "epoch : 5  batch: 10000\n",
      "epoch : 5  batch: 11000\n",
      "loss after epoch:  5 :  5054.710104310792\n",
      "epoch : 6  batch: 0\n",
      "epoch : 6  batch: 1000\n",
      "epoch : 6  batch: 2000\n",
      "epoch : 6  batch: 3000\n",
      "epoch : 6  batch: 4000\n",
      "epoch : 6  batch: 5000\n",
      "epoch : 6  batch: 6000\n",
      "epoch : 6  batch: 7000\n",
      "epoch : 6  batch: 8000\n",
      "epoch : 6  batch: 9000\n",
      "epoch : 6  batch: 10000\n",
      "epoch : 6  batch: 11000\n",
      "loss after epoch:  6 :  4751.590208894103\n",
      "epoch : 7  batch: 0\n",
      "epoch : 7  batch: 1000\n",
      "epoch : 7  batch: 2000\n",
      "epoch : 7  batch: 3000\n",
      "epoch : 7  batch: 4000\n",
      "epoch : 7  batch: 5000\n",
      "epoch : 7  batch: 6000\n",
      "epoch : 7  batch: 7000\n",
      "epoch : 7  batch: 8000\n",
      "epoch : 7  batch: 9000\n",
      "epoch : 7  batch: 10000\n",
      "epoch : 7  batch: 11000\n",
      "loss after epoch:  7 :  3355.4718624483785\n",
      "epoch : 8  batch: 0\n",
      "epoch : 8  batch: 1000\n",
      "epoch : 8  batch: 2000\n",
      "epoch : 8  batch: 3000\n",
      "epoch : 8  batch: 4000\n",
      "epoch : 8  batch: 5000\n",
      "epoch : 8  batch: 6000\n",
      "epoch : 8  batch: 7000\n",
      "epoch : 8  batch: 8000\n",
      "epoch : 8  batch: 9000\n",
      "epoch : 8  batch: 10000\n",
      "epoch : 8  batch: 11000\n",
      "loss after epoch:  8 :  3833.6875106188077\n",
      "epoch : 9  batch: 0\n",
      "epoch : 9  batch: 1000\n",
      "epoch : 9  batch: 2000\n",
      "epoch : 9  batch: 3000\n",
      "epoch : 9  batch: 4000\n",
      "epoch : 9  batch: 5000\n",
      "epoch : 9  batch: 6000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-329f98a15e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_training_with_momentum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-9007fc332666>\u001b[0m in \u001b[0;36mrun_training_with_momentum\u001b[0;34m(data_path, batch_size, modes, epochs, h1, lr, model_name, momentum_param)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-4c93d00780cf>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(data_path, epochs, mode, model_name, h1, lr, batch_size, momentum_param)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Call training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mvelocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_velocity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     history = batch_training(batch_size, weights, biases, \n\u001b[0m\u001b[1;32m     37\u001b[0m                              \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_flattened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                              momentum_param, lr, X_dev_flattened, y_dev, velocity)\n",
      "\u001b[0;32m<ipython-input-18-39cca4ea23bc>\u001b[0m in \u001b[0;36mbatch_training\u001b[0;34m(batch_size, weights, biases, epochs, X, y, momentum_param, lr, X_dev, y_dev, velocity)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass_two_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackpropagation_two_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             weights, biases,velocity = update_parameters_with_momentum(weights, biases, gradients, \n",
      "\u001b[0;32m<ipython-input-9-19dd7aaf5a7f>\u001b[0m in \u001b[0;36mforward_pass_two_layers\u001b[0;34m(X, weights, biases)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqMklEQVR4nO3deXxU9bnH8c9D2EFl96UsAhVRVNBrilYsru1F5YoVF/Cq4FLcAblWrYJValtqrRWLYkFRUItKK0opSqWKorgQCi6AFQ0IQVEWoQqyJc/943cCY5gkE8jkJDPf9+s1r5mzzTyTA+eZ81vN3RERESmpVtwBiIhI9aQEISIiSSlBiIhIUkoQIiKSlBKEiIgkVTvuACpLixYtvH379nGHISJSo8yfP3+tu7dMti1jEkT79u3Jy8uLOwwRkRrFzD4tbZuKmEREJCklCBERSUoJQkREksqYOohktm/fTkFBAVu2bIk7FKkC9evXp02bNtSpUyfuUEQyQkYniIKCAvbZZx/at2+PmcUdjqSRu7Nu3ToKCgro0KFD3OGIZISMLmLasmULzZs3V3LIAmZG8+bNdbcoUokyOkEASg5ZROdapHJldBGTiEiV2LYNtmyBRo0gJwe+/TY86taFOnXCo1bC7/F162D16t3fp3NnqF0bvvgC1q4Fd9i+PTy2bYMePcAM3nsPFi2CDRvC45ZbwvpKpgSRRuvWrePUU08FYPXq1eTk5NCyZeiw+M4771C3bt1Sj83Ly2PSpEncf//9ZX7G8ccfz9y5cyst5qFDhzJlyhRWrlxJrVoZf4MpmcY9XCi3boUVK2DTJti8OVxct22DI46AAw+EL7+Ef/wjrCu+AH/7LfzkJ3DwwfD++zBmTNheWLjr/W++GQ4/HN5+G+67L3zG8uXw2Wdh+8KF0K0bPPooXHvtd2PLyYH8fGjXDh56CIYP3z3+NWugRQsYPRp+85vdt2/ZAvXqwbhx8MADu9Zffz00bryXf7zdKUGkUfPmzVm4cCEAd9xxB40bN+bGG2/cuX3Hjh3Urp38FOTm5pKbm1vuZ1RmcigqKmLq1Km0bduWV199lZNPPrnS3jtRWd9bskxhYbioAxQVheUGDcIF+/XXwwV+06ZdF+kjjoCuXcMv7Jtugs8/Dxf7L78Mv8rvuw+uvhqWLIGjj9798yZOhEsugY8+gosv3n17x44hQXz5JUybFn755+Ts+nV+5ZXhee1amDcP2rSBH/8Y2rcPF+gDDgjbTzghxJKYgLZtCxd3gL59oVOn3X/177NPeO7ff1f8dersuhMp/n9z881w3XXQpEl41K9foT97qvS/tIoNHDiQ+vXrs2DBAnr06EG/fv0YMmQIW7ZsoUGDBjz66KN07tyZ2bNnc8899zB9+nTuuOMOVqxYQX5+PitWrGDo0KEMHjwYgMaNG/PNN98we/Zs7rjjDlq0aMEHH3zAMcccwxNPPIGZMWPGDIYNG0ajRo3o0aMH+fn5TJ8+fbfYZs+ezeGHH84FF1zA5MmTdyaIL774gquuuor8/HwAxo4dy/HHH8+kSZO45557MDO6du3K448/zsCBA+nduzfnnnvubvGNGDGCpk2b8uGHH/LRRx9x9tlns3LlSrZs2cKQIUMYNGgQAC+++CK33norhYWFtGjRgpdeeonOnTszd+5cWrZsSVFREYcccghvvvnmzjsyicHixeFCnfgL/YADoGfPsP2mm8JFe9Mm+PrrUBTSqxeMGBGSQf364cKZaNgw+P3vwx3AKafs/pm33x4SRO3a8Mor4fPatYPcXGjeHI46KuzXvj1MmhSKfBo2DJ9Vpw4cckjYfswxIUkkFgE1aBD2BTj11JB8SnPmmeFRmq5dw6M0hx4aHqU58sjwKE3btqVvq0TZlSBOOmn3deefD9dcE/6Rn3HG7tsHDgyPtWshuujtNHv2HoVRUFDA3LlzycnJ4T//+Q9z5syhdu3azJo1i1tvvZW//vWvux3z4Ycf8sorr/D111/TuXNnrr766t3a+y9YsIBFixZx4IEH0qNHD9544w1yc3O58soree211+jQoQP9+/cvNa7JkyfTv39/+vTpw6233sr27dupU6cOgwcP5sQTT2Tq1KkUFhbyzTffsGjRIu666y7mzp1LixYtWL9+fbnf+1//+hcffPDBzmaoEyZMoFmzZnz77bd8//vfp2/fvhQVFfHTn/50Z7zr16+nVq1aXHTRRTz55JMMHTqUWbNm0a1bNyWHdCguogF4/HH4+OPwa3r1ali1KlzUJk0K2884Az4tMYxPnz67EsSzz4Zim0aNwi/jpk13FYPUqhXKzevU2fV5ZnDcceF1w4bh/1fDhuH44l/OzZqF5+bNQ/FOaZo0SX6HUKxBg/ALXsqUXQmimjjvvPPIyckBYOPGjQwYMIClS5diZmwv+YsqcuaZZ1KvXj3q1atHq1at+OKLL2jTps139unevfvOdUcddRTLly+ncePGdOzYcedFuX///owbN26399+2bRszZszg3nvvZZ999uHYY49l5syZ9O7dm5dffplJ0UUhJyeH/fbbj0mTJnHeeefRokULAJoV/8ctQ/fu3b/TR+H+++9n6tSpAKxcuZKlS5eyZs0aevbsuXO/4ve97LLL6NOnD0OHDmXChAlceuml5X5e1lu0CDZuDBfYBg1CWfmqVVD8t7vySvjb30K5drGDD4Z33gmvx48PxTzNm0OrVqE4pWPHXfs+/HC4cDdqtOtXePPmu7Z//HHZ8Y0cWfq2WrXgxBMr9HWl8mVXgijrF3/xL5bStGixx3cMJTVq1Gjn6xEjRnDyySczdepUli9fzknJ7nKAesVll4SL9I4dO/Zon9LMnDmTDRs2cGR0W7t582YaNGhA7969U34PgNq1a1NUVASEOo1t27bt3Jb4vWfPns2sWbN48803adiwISeddFKZfRjatm3L/vvvz8svv8w777zDk08+WaG4aqxNm8JFvaAgFHmsWQODBoV/ry+8ANOnw44du1qzfPUVvPFGuFiPHfvdikwIZeAXXxwu7IceuqvMv/hXfOIF/u9/D58T/ZjZzWmnpeMbSzWSXQmiGtq4cSOtW7cG4LHHHqv09+/cuTP5+fksX76c9u3b8/TTTyfdb/LkyTz88MM7i6A2bdpEhw4d2Lx5M6eeeipjx45l6NChO4uYTjnlFH7yk58wbNgwmjdvzvr162nWrBnt27dn/vz5nH/++UybNq3UO6KNGzfStGlTGjZsyIcffshbb70FwHHHHcc111zDsmXLdhYxFd9FXHHFFVx00UVcfPHFO+/AarwtW8Iv+2XLdj0PHRpa2owZE1qnlNS7d/ilv2QJPPNM+LXdpEkowmnRItw1tGgBN94Y9i1uydOmTSijLy6uueGGsmMrrjCVrJXWBGFmvYDRQA7wsLuPKrH9IGAC0BJYD1zk7gXRtkLg/WjXFe5+VjpjjctNN93EgAEDuOuuuzizrEqvPdSgQQMefPBBevXqRaNGjfj+97+/2z6bN2/mxRdf5KGHHtq5rlGjRpxwwgn87W9/Y/To0QwaNIhHHnmEnJwcxo4dyw9+8ANuu+02TjzxRHJycjj66KN57LHH+OlPf0qfPn3o1q3bzs9MplevXjz00EMcdthhdO7cmeOisueWLVsybtw4zjnnHIqKimjVqhUvvfQSAGeddRaXXnppzSte+vrrUNyzZElo5ti/P3TpElrJ9Onz3X3r1YOzzgoJ4oQTQlPH1q3Dxf2AA0JRT9OmYd9hw8KjNO3bh4fInnL3tDwISeEToCNQF3gX6FJinynAgOj1KcDjCdu+qcjnHXPMMV7S4sWLd1uXjb7++mt3dy8qKvKrr77a77333pgj2jPz5s3zE044ocx9qtU5z8tzP+gg91D1Gx61arlPnhy25+e7jxzpPmmS++uvu69a5V5YGGvIkn2APC/luprOO4juwMfung9gZk8BfYDFCft0AYp/Ar0CPJfGeLLW+PHjmThxItu2bePoo4/myuK23DXIqFGjGDt2bPWte/jyy1D2//rr4e7g8svDc48eoc7giCPC8kEHhfoBgA4dQpNPkWrKvLiTSmW/sdm5QC93vyJavhg41t2vS9jnz8Db7j7azM4B/gq0cPd1ZrYDWAjsAEa5+3NJPmMQMAigXbt2x3xaosndkiVLOOyww9Lx9aSaSvs537o1tP1v1y4sX355aI+/bFlYrlcvNItOKK4Tqc7MbL67J+2VG3cl9Y3AGDMbCLwGrAKK+7Uf5O6rzKwj8LKZve/unyQe7O7jgHEAubm5STOdu2sQtyyRlh87BQWh9drrr4c7hCVLQvv5JUvC9tq1oXv30GT0hz8MHbASWpOJ1GTpTBCrgMTufm2idTu5+2fAOQBm1hjo6+4bom2roud8M5sNHE2o00hZ/fr1WbdunYb8zgIezQdRf2+GHFi1KnQCmz4d5swJrYPuvDO0999331BcdPbZ3+0h+6c/7XXsItVVOhPEPKCTmXUgJIZ+wIWJO5hZC2C9uxcBPye0aMLMmgKb3X1rtE8P4O6KBtCmTRsKCgpYs2bN3n0TqRGKZ5SrkM8/h+efD0nhhRfCEBDHHx+ahTZuHHrZX3ddqEPIlKa1IilKW4Jw9x1mdh0wk9CiaYK7LzKzkYRa82nAScBvzMwJRUzFwx8eBvzJzIoIc1aMcvfFu31IOerUqaPZxWQX99Dc9OWXQy/dbt3CeEJXXx2akd58M1x2WehjUCzZgG8iWSJtldRVLTc31/Py8uIOQ6qjlSvhscfCEMzFlcmjRoWEsGVLqGf43vfSMp6+SHVXnSupRdJr27ZQZ7BhQxihc/jwMEpocQey+vW/e8cgIjspQUjm2b4dpkwJPZbr1g1zABx5ZOh3ICIpU4KQzPLJJ2FY9oULw7AUp50Whq4QkQrTnJKSOV54IUwcs2JFmItAo42K7BUlCMkM990XZvg66CDIywtzC4vIXlGCkMxwxBFhnoO5c1XXIFJJVAchNdvWrWFoi9NOU5GSSCXTHYTUXJs2hTqH0aPjjkQkIylBSM11/fWhZ/QRR8QdiUhGUoKQmmnKlNAzevjw0AFORCqdEoTUPGvXwrXXhuKl22+POxqRjKUEITXP66+H0VYnTAjzMYhIWuh/l9Q8Z58dOsM1axZ3JCIZTXcQUnN89RXMmBFeKzmIpJ0ShNQcd94J//M/u4bsFpG0UoKQmmHZMnjwwTChj3pKi1QJJQipGYYPDxXSd94ZdyQiWUMJQqq/f/0L/vxnuOEGOPDAuKMRyRpKEFL9ffYZHH443HRT3JGIZBUlCKn+eveG99+H/faLOxKRrKIEIdVXURE89VSYQtQs7mhEso4ShFRfTz8d5pV+7rm4IxHJSkoQUj1t2xZaLnXtCn37xh2NSFbSUBtSPY0fD/n58Pe/Qy39jhGJg/7nSfXzzTcwciT07Amnnx53NCJZSwlCqp9Vq2D//eG3v1XltEiMVMQk1U/nzvDuu0oOIjHTHYRUL4sWhVFblRxEYqc7CKk+3GHAgPA6Ly/eWEREdxBSjbz1FsyfH0ZsFZHYKUFI9fHHP8K++8Ill8QdiYigBCHVxWefwZQp4e6hceO4oxER0pwgzKyXmf3bzD42s1uSbD/IzP5pZu+Z2Wwza5OwbYCZLY0eA9IZp1QDL74IhYVw7bVxRyIikbQlCDPLAR4ATge6AP3NrEuJ3e4BJrl7V2Ak8Jvo2GbAL4Bjge7AL8ysabpilWrgsstg+XI4+OC4IxGRSKmtmMzsnBSO3+LuM0rZ1h342N3zo/d7CugDLE7YpwswLHr9CvBc9Pq/gZfcfX107EtAL2ByCjFJTbNjR5gtrl27uCMRkQRlNXMdDzwPlNUgvSdQWoJoDaxMWC4g3BEkehc4BxgN/ATYx8yal3Js65IfYGaDgEEA7XRxqZnc4fjj4Uc/gl/9Ku5oRCRBWQniBXcvs72hmT2xl59/IzDGzAYCrwGrgMJUD3b3ccA4gNzcXN/LWCQOr70G8+bBFVfEHYmIlFBqgnD3i8o7uJx9VgFtE5bbROsSj/+McAeBmTUG+rr7BjNbBZxU4tjZ5cUjNdDo0dCsGVxU7j83EalipVZSm1nP6HHcHr73PKCTmXUws7pAP2Baic9oYWbFMfwcmBC9ngn82MyaRpXTP47WSSZZtgyefx6uvBIaNow7GhEpoawipkuj5w3AWxV9Y3ffYWbXES7sOcAEd19kZiOBPHefRrhL+I2ZOaGI6dro2PVm9ktCkgEYWVxhLRlkzJgw5tI118QdiYgkYe5lF92bWY67p1wvEJfc3FzP0/g9NcvKlTBnDlx4YdyRiGQtM5vv7rnJtqXSD2Kpmf0uSR8Gkb3Ttq2Sg0g1lkqC6AZ8BDxsZm+Z2SAz2zfNcUkmc4dBg+D11+OORETKUG6CcPev3X28ux8P3Ezo4fy5mU00M3V7lYqbNWvXnNMiUm2VmyDMLMfMzjKzqcB9wO+BjsDfKL2TnEjp/vhHaNUKLrgg7khEpAypTBi0lDAMxu/cfW7C+r+YWc/0hCUZKz8fpk+H226DevXijkZEypBKgujq7t8k2+Dugys5Hsl0DzwAOTlw1VVxRyIi5UilkvoBM2tSvBB1XptQxv4ipfve92DwYGi929BaIlLNpHoHsaF4wd2/MrOj0xeSZDR1ihOpMVK5g6iVOBdDNFdDKolFZJdvv4XHH4ft2+OORERSlMqF/vfAm2Y2hTD097mAxmWWihk/HoYMgY4doUePuKMRkRSUmyDcfZKZzQdOjlad4+6LyzpG5Du2boW774aePZUcRGqQlIqKokH21gD1AcysnbuvSGtkkjkmToRVq+DRR+OOREQqIJWOcmeZ2VJgGfAqsBx4Ic1xSabYvh1+8xs49lg47bS4oxGRCkilkvqXwHHAR+7eATiVPRj+W7LU559Do0YwfHgY2ltEaoxUipi2u/s6M6tlZrXc/RUzuy/dgUmGaNcOFi4MneNEpEZJ5Q5iQzQd6GvAk2Y2GtiU3rAkIzz/PGzcCLVr6+5BpAZKJUH0ATYDNwAvAp8A/5POoCQDLF4MffvCyJFxRyIie6jMIiYzywGmu/vJQBEwsUqikprNHYYNg8aN4ZZb4o5GRPZQmQnC3QvNrMjM9nP3jVUVlNRwU6fCzJlw773QsmXc0YjIHkqlkvob4H0ze4mEugeN5CpJLVsGl10GxxwD114bdzQishdSSRDPRg+R8tWvDyedFO4e6taNOxoR2QupDLWhegdJjTsccAA891zckYhIJUilJ/UyM8sv+aiK4KQGmTgRfvQjWL8+7khEpJKkUsSUm/C6PnAe0Cw94UiN4x4G4rvlFjj55NBySUQyQrl3EO6+LuGxyt3vA85Mf2hS7RUWhoroW26Bfv3ghRdU7yCSQcq9gzCz/0pYrEW4o9CEQQI33ABjx8JNN4UB+Wql0u9SRGqKVCcMKraDMKrr+ekJR2qEoqKQDK68Erp2hSuuiDsiEUmDVFoxnVzePpIltmyBESPgyy9DpfThh4eHiGSkVFox/drMmiQsNzWzu9IalVQ/CxZAbi7ccw80bAg7dsQdkYikWSqFxqe7+4biBXf/CjgjbRFJ9bJxY6hjOPbY0IR1xoxQ71Bb1VAimS6VBJFjZvWKF8ysAVCvjP2lplu+HN54I7yuWxfGjIFzz4UPPoDTT481NBGpOqkkiCeBf5rZ5WZ2OfASKY7qama9zOzfZvaxme02rKeZtTOzV8xsgZm9Z2ZnROvbm9m3ZrYwejxUkS8le+HJJ6FbtzCeUmEhNGgQ6hz+/Gdopu4vItkklUrq35rZu0DxhMK/dPeZ5R0XDRX+APAjoACYZ2bT3H1xwm7DgWfcfayZdQFmAO2jbZ+4+1EpfxPZO2vXwuDBMHky9OgBTzyxaxY4dX4TyUqp9IPoAMx29xej5QZm1t7dl5dzaHfgY3fPj457ijD5UGKCcGDf6PV+wGcVC18qxaefhtFXN26EX/4ydHxTHYNI1kuliGkKYbKgYoXRuvK0BlYmLBdE6xLdAVxkZgWEu4frE7Z1iIqeXjWzHyb7ADMbZGZ5Zpa3Zs2aFEKS7/jqq/Dcrl3oy7BgAQwfruQgIkBqCaK2u28rXoheV9Z4Cv2Bx9y9DaFl1ONmVgv4HGjn7kcDw4A/m9m+JQ9293HunuvuuS01MU3q1q+H668PiWHFijBf9KhRcMQRcUcmItVIKglijZmdVbxgZn2AtSkctwpom7DcJlqX6HLgGQB3f5MwGGALd9/q7uui9fMJ82AfksJnSlncYcIE6NQJHnwQBgxQ/YKIlCqVBHEVcKuZrTCzlcDNwKAUjpsHdDKzDmZWF+gHTCuxzwrgVAAzO4yQINaYWcuokhsz6wh0AjTE+N7YsQPOPBMuvzz0fl64MDRfVcskESlFKq2YPgGOM7PG0fI3ZvZ9wq/6so7bYWbXATOBHGCCuy8ys5FAnrtPA/4PGG9mNxAqrAe6u5tZT2CkmW0n1H9c5e6aaGBv1K4NRx4JZ5wB11yjgfVEpFzm7qntGJqh9ifcCWx099xyDqlSubm5npeXF3cY1cuKFXDVVXD77XDccXFHIyLVkJnNL+16XuYdhJm1JySF/sB24CAgN4UmrhK3KVNCcVJRUegZrQQhIhVUajmDmb0J/J2QRPq6+zHA10oONcCnn8LAgdClC7z/fpjMR0SkgsoqiP4C2AfYHyhuQ5paeZTEa8iQ8PzMM9ChQ7yxiEiNVWoRk7ufbWb7AecAd5hZJ6CJmXV393eqLEKpmMLCcOdw4omhn4OIyB6qSCV1K8JMcv0JndjalnNIlVIltYhIxZVVSZ1yW0d3/9Ldx7h7D+CESotOKs/o0TCz3HEURURSskeN4d3908oORPbShx/CjTeG0VhFRCqBektlAvcwtlKjRnD33XFHIyIZQsN2ZoK//hVmzYL774dWreKORkQyRCrzQdyfZPVGwnAZz1d+SFIhmzbBsGHQtStcfXXc0YhIBknlDqI+cCi75oDoCywDupnZye4+NE2xSSrq1w9zOBxxhOZxEJFKlcoVpSvQw90LAcxsLDCH0JLp/TTGJqnIyYFBqQyuKyJSMalUUjcFEicNaAQ0ixLG1rREJakZOBAefTTuKEQkQ6VyB3E3sNDMZgMG9AR+bWaNgFlpjE3K8uKLMHFimNtBRCQNUupJbWYHAN2jxXnu/llao9oDWdWTetu2UCldVAQffAB1K2sGWBHJNns83HeCWsCaaP+Dzexgd3+tsgKUChozBv79b5g+XclBRNImlWauvwUuABYRZneDMKqrEkQc1q+HO++E008PU4iKiKRJKncQZwOd3V0V0tVB06bw7LMaqVVE0i6VBJEP1EEtlqoHMzj11LijEJEskEqC2ExoxfRPEpKEuw9OW1SS3L33wsqVcM89of+DiEgapZIgpkUPidOmTfDrX8Oxxyo5iEiVKDdBuPvEqghEyjFuHKxbB7fdFnckIpIlSk0QZvaMu59vZu+TZC5qd++a1shkl61bQ7HSiSfC8cfHHY2IZImy7iCGRM+9qyIQKcOoUfDZZ6HntIhIFSk1Qbj759GzZo+L28CBYTKg006LOxIRySLlDtZnZueY2VIz22hm/zGzr83sP1URXNZbvz4Mp3HQQWE6URGRKpTKaK53A2e5+37uvq+77+Pu+6Y7sKy3dSv893/DgAFxRyIiWSqVBPGFuy9JeySyi3uY4yEvD/r2jTsaEclSqfSDyDOzp4Hn+G5HuWfTFVTWu/12mDQpjLl09tlxRyMiWSqVBLEvoTf1jxPWOaAEkQ4PPwx33QVXXAEjRsQdjYhksVQ6yl1aFYFI5PDD4aKL4MEHw7hLIiIxKauj3E3ufreZ/ZHkHeXKHYvJzHoBo4Ec4GF3H1VieztgItAk2ucWd58Rbfs5cDlQCAx295mpfqka7Qc/CA8RkZiVdQdRXDG9R9O0mVkO8ADwI6AAmGdm09x9ccJuw4Fn3H2smXUBZgDto9f9gMOBA4FZZnZINA92Zpo+HWbNCsVLjRuXv7+ISJqV1VHub9Hznnbf7Q587O75AGb2FNAHSEwQTqjjANgPKJ7KtA/wVDQHxTIz+zh6vzf3MJbqzR3uuAM2bAhDaoiIVAOpzCjXErgZ6ALUL17v7qeUc2hrYGXCcgFwbIl97gD+YWbXA42A4q7CrYG3ShzbOklsg4BBAO1q8gQ6//gHzJ8P48dD7VRngRURSa9U+kE8SShu6gDcCSwH5lXS5/cHHnP3NsAZwONmlkpMALj7OHfPdffcli1bVlJIMfjVr6BNG7jkkrgjERHZKZWLcXN3fwTY7u6vuvtlQHl3DwCrgLYJy22idYkuB54BcPc3CXcoLVI8NjPMmRMeP/sZ1K0bdzQiIjulkiC2R8+fm9mZZnY00CyF4+YBncysg5nVJVQ6l5x4aAVwKoCZHUZIEGui/fqZWT0z6wB0At5J4TNrngMOgCuvDP0eRESqkVQKvO8ys/2A/wP+SKhUvqG8g9x9h5ldB8wkNGGd4O6LzGwkkOfu06L3HG9mNxAqrAe6uwOLzOwZQoX2DuDajG3BdPDB8NBDcUchIrIbC9fjUjaGpqqD3f0PVRfSnsnNzfW8vD1qkRuPTz6BIUNCh7iaXMEuIjWamc1399xk28osYop+tfdPS1TZrKgILr881D3USrlOXkSkSqVSxPSGmY0BngY2Fa9093+lLapM9+CD8Oqr8MgjofWSiEg1VNZQG/9w9x8DR0WrRiZsdlJrySQlffQR3Hwz9OoFl2qYKxGpvsq6g2gJ4O4nV1Es2WHECKhXL9w9aDA+EanGykoQ+5nZOaVt1HwQe2j8eFiyBA48MO5IRETKVGaCAHoDyX7maj6IisrPD0lh333h2JIjjoiIVD9lJYhPo17Tsre2boXTT4fvfQ9mzIg7GhGRlJTVxlIF5JVl9OhQOT243Ck0RESqjbISxEVVFkUmW706zPHQu3douSQiUkOUVcQ03cwcWOPuKjTfU7fdBlu2wO9/H3ckIiIVUtaEQR2qMpCMtHUrfPBBKFo65JC4oxERqRDNTpNO9erBm2/Ctm1xRyIiUmGl1kGYWblDaaSyT9ZatAjWrQtjLdWvX/7+IiLVTFl3EIeZ2XtlbDdCXwkpyR0uvji8nj9fPaZFpEYqK0EcmsLxmTlHw956/nlYsAAmTlRyEJEaq6xK6k+rMpCMUVQEv/gFdOoEF14YdzQiIntMldSV7dln4b334IknoLb+vCJSc2m2msr29ttw2GHQr1/ckYiI7BUliMr2u9/BO+9ATk7ckYiI7BUliMqyeTMsXhxeN24cbywiIpVACaKy3HsvdO0ahvUWEckAShCVYfVqGDUK+vSBjh3jjkZEpFIoQVSGESPCcBq//W3ckYiIVBoliL313nthfunrroODD447GhGRSqMEsbfmz4dWrWD48LgjERGpVEoQe+vSS2H5cmjWLO5IREQqlRLE3vjww/Cs0VpFJAMpQeyppUvh8MNhzJi4IxERSQsliD01ahTUrQvnnRd3JCIiaaEEsSeWL4dJk2DQINh//7ijERFJCyWIPXHnnWGeh5/9LO5IRETSRgmiojZsgJkzYdgwaNMm7mhERNImrRMWmFkvYDSQAzzs7qNKbP8DcHK02BBo5e5Nom2FwPvRthXuflY6Y01ZkyawZInmehCRjJe2q5yZ5QAPAD8CCoB5ZjbN3RcX7+PuNyTsfz1wdMJbfOvuR6Urvj3y7rvQpQvsp6m4RSTzpbOIqTvwsbvnu/s24CmgTxn79wcmpzGevbNmDZx0Elx7bdyRiIhUiXQmiNbAyoTlgmjdbszsIKAD8HLC6vpmlmdmb5nZ2aUcNyjaJ2/NmjWVFHYSRUUhMXzzDdxwQ/n7i4hkgOpSSd0P+Iu7FyasO8jdc4ELgfvM7HslD3L3ce6e6+65LVu2TE9k7iEpTJkCv/51mE5URCQLpDNBrALaJiy3idYl048SxUvuvip6zgdm8936iarzhz/A/feHVks33hhLCCIicUhnU5x5QCcz60BIDP0IdwPfYWaHAk2BNxPWNQU2u/tWM2sB9ADuTmOspevde9eEQGaxhCAiEoe0JQh332Fm1wEzCc1cJ7j7IjMbCeS5+7Ro137AU+7uCYcfBvzJzIoIdzmjEls/ValDDoG748lNIiJxsu9el2uu3Nxcz8vLq7w3nDYNHn44PFq1qrz3FRGpRsxsflTfu5vqUkld/Tz6KOTlaZ4HEclaShDJrF0Lf/87XHihekyLSNZSgkjmqadg+3a45JK4IxERiY0SRDKTJkG3btC1a9yRiIjERuUnJRUWQp8+0K5d3JGIiMRKCaKknBy47ba4oxARiZ2KmBK5w/PPw5YtcUciIhI7JYhE+flw9tkwcWLckYiIxE4JItGcOeH5hBPijUNEpBpQgkg0Z07oGKcRW0VElCC+Y86ccPdQS38WERFdCYutXg1Ll0LPnnFHIiJSLaiZa7FWrcKc0+maeEhEpIZRgihWq5Z6TouIJFARU7Ff/hJefrn8/UREsoQSBMCGDfCLX+xq5ioiIkoQAMydG3pRq4JaRGQnJQgIdw516sCxx8YdiYhItaEEAfDaa3DMMdCwYdyRiIhUG0oQhYVhBrkf/jDuSEREqhU1c83JgX//O8wgJyIiO+kOolidOnFHICJSrShBiIhIUkoQIiKSlBKEiIgkpQQhIiJJKUGIiEhSShAiIpKUEoSIiCSlBCEiIkmZu8cdQ6UwszXApxU8rAWwNg3hVHfZ+r1B313fPbuk8r0PcvekU2lmTILYE2aW5+65ccdR1bL1e4O+u757dtnb760iJhERSUoJQkREksr2BDEu7gBikq3fG/Tds1W2fve9+t5ZXQchIiKly/Y7CBERKYUShIiIJJWVCcLMepnZv83sYzO7Je540snM2prZK2a22MwWmdmQaH0zM3vJzJZGz03jjjUdzCzHzBaY2fRouYOZvR2d+6fNrG7cMaaDmTUxs7+Y2YdmtsTMfpBF5/yG6N/6B2Y22czqZ+p5N7MJZvalmX2QsC7pebbg/uhv8J6Z/Vd57591CcLMcoAHgNOBLkB/M+sSb1RptQP4P3fvAhwHXBt931uAf7p7J+Cf0XImGgIsSVj+LfAHdz8Y+Aq4PJao0m808KK7Hwp0I/wNMv6cm1lrYDCQ6+5HADlAPzL3vD8G9CqxrrTzfDrQKXoMAsaW9+ZZlyCA7sDH7p7v7tuAp4A+MceUNu7+ubv/K3r9NeFC0ZrwnSdGu00Ezo4lwDQyszbAmcDD0bIBpwB/iXbJ1O+9H9ATeATA3be5+way4JxHagMNzKw20BD4nAw97+7+GrC+xOrSznMfYJIHbwFNzOyAst4/GxNEa2BlwnJBtC7jmVl74GjgbWB/d/882rQa2D+uuNLoPuAmoChabg5scPcd0XKmnvsOwBrg0ah47WEza0QWnHN3XwXcA6wgJIaNwHyy47wXK+08V/jal40JIiuZWWPgr8BQd/9P4jYPbZ0zqr2zmfUGvnT3+XHHEoPawH8BY939aGATJYqTMvGcA0Tl7X0ISfJAoBG7F8Fkjb09z9mYIFYBbROW20TrMpaZ1SEkhyfd/dlo9RfFt5fR85dxxZcmPYCzzGw5oRjxFEK5fJOo6AEy99wXAAXu/na0/BdCwsj0cw5wGrDM3de4+3bgWcK/hWw478VKO88VvvZlY4KYB3SKWjXUJVRgTYs5prSJyt0fAZa4+70Jm6YBA6LXA4Dnqzq2dHL3n7t7G3dvTzjHL7v7/wKvAOdGu2Xc9wZw99XASjPrHK06FVhMhp/zyArgODNrGP3bL/7uGX/eE5R2nqcBl0StmY4DNiYURSWVlT2pzewMQvl0DjDB3X8Vb0TpY2YnAHOA99lVFn8roR7iGaAdYZj08929ZGVXRjCzk4Ab3b23mXUk3FE0AxYAF7n71hjDSwszO4pQOV8XyAcuJfwgzPhzbmZ3AhcQWvAtAK4glLVn3Hk3s8nASYRhvb8AfgE8R5LzHCXMMYQit83Ape6eV+b7Z2OCEBGR8mVjEZOIiKRACUJERJJSghARkaSUIEREJCklCBERSUoJQqQCzKzQzBYmPCptwDsza584KqdI3GqXv4uIJPjW3Y+KOwiRqqA7CJFKYGbLzexuM3vfzN4xs4Oj9e3N7OVo/P1/mlm7aP3+ZjbVzN6NHsdHb5VjZuOj+Qz+YWYNYvtSkvWUIEQqpkGJIqYLErZtdPcjCb1V74vW/RGY6O5dgSeB+6P19wOvuns3wjhJi6L1nYAH3P1wYAPQN63fRqQM6kktUgFm9o27N06yfjlwirvnR4Mjrnb35ma2FjjA3bdH6z939xZmtgZokzjcQzQc+0vRRC+Y2c1AHXe/qwq+mshudAchUnm8lNcVkTg+UCGqJ5QYKUGIVJ4LEp7fjF7PJYwmC/C/hIETIUwFeTXsnDd7v6oKUiRV+nUiUjENzGxhwvKL7l7c1LWpmb1HuAvoH627njCz288Is7xdGq0fAowzs8sJdwpXE2ZAE6k2VAchUgmiOohcd18bdywilUVFTCIikpTuIEREJCndQYiISFJKECIikpQShIiIJKUEISIiSSlBiIhIUv8P5MebHm1vLOkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_without_momentum = {'full':'', 'stochastic':'', 'batch': ''}\n",
    "    \n",
    "for mode in modes:\n",
    "    fig_name = model_name.format(mode)\n",
    "    results_without_momentum[mode] = run_training(data_path, \n",
    "                                                  epochs, mode, model_name, h1, \n",
    "                                                  lr, batch_size, momentum_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "model_name = '../figs/{}_without_momentum'\n",
    "momentum_param = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_momentum = {'full':'', 'stochastic':'', 'batch': ''}\n",
    "    \n",
    "for mode in modes:\n",
    "    fig_name = model_name.format(mode)\n",
    "    results_with_momentum[mode] = run_training(data_path, \n",
    "                                                  epochs, mode, model_name, h1, \n",
    "                                                  lr, batch_size, momentum_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store grids for grid search\n",
    "momentum = np.arange(0.001,0.01,0.001)\n",
    "lr = np.arange(0.01,0.1,0.01)\n",
    "batch_size = [16,64,256,1024,4096]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
