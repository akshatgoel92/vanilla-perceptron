{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "%reset -f\n",
    "from helpers import load_all_data, vectorized_flatten, sigmoid, get_log_loss, get_accuracy \n",
    "from helpers import sigmoid_derivative, gradient_update, plot_loss, prep_data, get_best_results\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X, h1, h2): \n",
    "    '''\n",
    "    --------------------\n",
    "    Parameter Initialization\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X [n = 12000])\n",
    "    --------------------\n",
    "    Output: \n",
    "    weights: Weight terms initialized as random normals\n",
    "    biases: Bias terms initialized to zero\n",
    "    --------------------\n",
    "    '''\n",
    "    dim1 = 1/np.sqrt(X.shape[0])\n",
    "    W1 = dim1 * np.random.randn(h1, 28**2)\n",
    "    \n",
    "    dim2 = 1/np.sqrt(W1.shape[1])\n",
    "    W2 = dim2 * np.random.randn(h2, h1)\n",
    "    \n",
    "    dim3 = 1/np.sqrt(W2.shape[1])\n",
    "    W3 = dim3 * np.random.randn(1, h2)\n",
    "\n",
    "    b1 = np.zeros((h1, 1))\n",
    "    b2 = np.zeros((h2, 1))\n",
    "    b3 = np.zeros((1, 1))\n",
    "    \n",
    "    weights = (W1, W2, W3)\n",
    "    biases = (b1, b2, b3)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, weights, biases):\n",
    "    '''\n",
    "    ----------------------------------\n",
    "    Forward propogation:\n",
    "    Send inputs through the network to\n",
    "    generate output\n",
    "    ----------------------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    weights: Binary (1/0) training label (shape = n X 1)\n",
    "    biases:\n",
    "    --------------------\n",
    "    Output: \n",
    "    activations: vector of results from passing\n",
    "    inputs through each neuron\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    \n",
    "    z1 = W1 @ X + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    activations = (z1, a1, z2, a2, z3, a3)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, weights, biases, activations):\n",
    "    '''\n",
    "    --------------------\n",
    "    Backpropagation\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    activations: Current set of activations\n",
    "    --------------------\n",
    "    Output: \n",
    "    Derivatives required\n",
    "    for optimization update\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    z1, a1, z2, a2, z3, a3 = activations\n",
    "    m = max(y.shape)\n",
    "    # print(m)\n",
    "    \n",
    "    dz3 = (a3 - y)/m\n",
    "    # print(\"dz3\", dz3.shape)\n",
    "    \n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    # print(\"dW3\", dW3.shape)\n",
    "    \n",
    "    db3 = np.sum(dz3, axis=1).reshape(-1, 1)\n",
    "    # print(\"db3\", db3.shape)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    # print(\"da2\", da2.shape)\n",
    "    \n",
    "    dz2 = da2 * sigmoid_derivative(z2)\n",
    "    # print(\"dz2\", dz2.shape)\n",
    "    \n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    # print(\"dW2\", dW2.shape)\n",
    "    \n",
    "    db2 = np.sum(dz2, axis=1).reshape(-1, 1)\n",
    "    # print(\"db2\", db2.shape)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    # print(\"da1\", da1.shape)\n",
    "    \n",
    "    dz1 = da1 * sigmoid_derivative(z1)\n",
    "    # print(\"dz1\", dz1.shape)\n",
    "    \n",
    "    dW1 = np.dot(dz1, X.T)\n",
    "    # print(\"dW1\", dW1.shape)\n",
    "    \n",
    "    db1 = np.sum(dz1, axis=1).reshape(-1, 1)\n",
    "    # print(\"db1\", db1.shape)\n",
    "    \n",
    "    return db1, dW1, db2, dW2, db3, dW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    '''\n",
    "    --------------------\n",
    "    Update parameters\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2, db3, dW3 = gradients\n",
    "    \n",
    "    W1 = gradient_update(W1, learning_rate, dW1)\n",
    "    W2 = gradient_update(W2, learning_rate, dW2)\n",
    "    W3 = gradient_update(W3, learning_rate, dW3)\n",
    "   \n",
    "    b1 = gradient_update(b1, learning_rate, db1)\n",
    "    b2 = gradient_update(b2, learning_rate, db2)\n",
    "    b3 = gradient_update(b3, learning_rate, db3)\n",
    "    \n",
    "    weights = (W1, W2, W3)\n",
    "    biases = (b1, b2, b3)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_differences(example, truth, weights, biases, delta_h=1e-9):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    \n",
    "    I, J = W3.shape # Change here\n",
    "    \n",
    "    deltaW = np.zeros((I, J))\n",
    "    \n",
    "    activations = forward_pass(example, weights, biases)\n",
    "    db1, dW1, db2, dW2, db3, dW3 = backpropagation(example, truth, weights, biases, activations)\n",
    "    \n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "    \n",
    "            W_plus = np.copy(W3) # Change here\n",
    "            W_minus = np.copy(W3) # Change here\n",
    "            \n",
    "            W_plus[i][j] += delta_h\n",
    "            W_minus[i][j] -= delta_h\n",
    "            \n",
    "            weights_plus = [W1, W2, W_plus] # Change here\n",
    "            weights_minus = [W1, W2, W_minus] # Change here\n",
    "            \n",
    "            activations_plus = forward_pass(example, weights_plus, biases)\n",
    "            activations_minus = forward_pass(example, weights_minus, biases)\n",
    "\n",
    "            loss_plus = get_log_loss(truth, activations_plus[-1])\n",
    "            loss_minus =  get_log_loss(truth, activations_minus[-1])\n",
    "\n",
    "            deltaW[i][j] = (loss_plus - loss_minus)/(2 * delta_h)\n",
    "\n",
    "    difference = np.linalg.norm(dW3 - deltaW) # Change here\n",
    "    \n",
    "    return dW3, deltaW, difference, activations_plus[-1], activations_minus[-1] # Change here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finite_differences(X_train_flattened, y_train, w, b, idx=10):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    dW, deltaW, difference, activations_plus, activations_minus = finite_differences(X_train_flattened[:, idx].reshape(-1, 1), \n",
    "                                                                  y_train[:, idx].reshape(-1, 1), w, b)\n",
    "    \n",
    "    print(\"dW\", dW)\n",
    "    print(\"deltaW\", deltaW)\n",
    "    print(\"difference\", difference)\n",
    "    print(activations_plus, activations_minus)\n",
    "    \n",
    "    return(dW, deltaW, difference, activations_plus, activations_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, w, b, h1, h2, lr, epochs, tolerance=1e-1):    \n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Initialize history dictionary\n",
    "    history = {\n",
    "        \"weights\": [w],\n",
    "        \"losses\": [], \n",
    "        \"biases\": [b],\n",
    "        \"accuracies\": []\n",
    "    }\n",
    "    \n",
    "    convergence_counter = 0\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    # Do this for the specified epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Get weights and bias\n",
    "        w = history['weights'][epoch]\n",
    "        b = history['biases'][epoch]\n",
    "        \n",
    "        # Forward pass to get activations\n",
    "        activations = forward_pass(X, w, b)\n",
    "        \n",
    "        # Backward pass to get gradients\n",
    "        gradients = backpropagation(X, y, w, b, activations)\n",
    "        \n",
    "        # Gradient descent update\n",
    "        w, b = update_parameters(w, b, gradients, lr)\n",
    "    \n",
    "        # Get last layer output\n",
    "        y_prob = activations[-1]\n",
    "        \n",
    "        # Threshold\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    \n",
    "        # Get loss and accuracy results\n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "        \n",
    "        # Check convergence, keeps a counter of how many epochs it has been without an improvement\n",
    "        # Counter resets whenever there's an improvent            \n",
    "        if loss < best_loss - tolerance:\n",
    "            best_loss = loss\n",
    "            convergence_counter = 0\n",
    "        else:\n",
    "            convergence_counter += 1\n",
    "\n",
    "        # Append results to history\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(b)\n",
    "        history[\"weights\"].append(w)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "        \n",
    "        # 10 epochs without an improvment is considered to have converged\n",
    "        if convergence_counter == 10:\n",
    "            break\n",
    "        \n",
    "        # Display loss for monitoring\n",
    "        print(loss)\n",
    "        \n",
    "        # Stop training if numerical loss underflows\n",
    "        if np.isnan(loss): break\n",
    "        \n",
    "        # Store loss for next epoch\n",
    "        previous_accuracy = accuracy\n",
    "    \n",
    "    # Return statement\n",
    "    return(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(X_dev, y_dev, history, best_epoch, label=\"dev\"):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    w = history[\"weights\"][best_epoch]\n",
    "    b = history[\"biases\"][best_epoch]\n",
    "    activations = forward_pass(X_dev, w, b)\n",
    "\n",
    "    y_dev_prob = activations[-1]\n",
    "    y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "\n",
    "    loss = get_log_loss(y_dev, y_dev_prob)\n",
    "    accuracy = get_accuracy(y_dev, y_dev_pred)\n",
    "    print(f\"{label} set accuracy: {accuracy}\")\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(data_path, idx, h1, h2, lr, epochs):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Set seed for reproducible results\n",
    "    np.random.seed(1252908)\n",
    "\n",
    "    # Get data\n",
    "    X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test = prep_data(data_path)\n",
    "\n",
    "    # Initialize weights\n",
    "    weights, biases = initialize(X_train_flattened, h1, h2)\n",
    "    \n",
    "    # Check finite difference\n",
    "    dW, deltaW, difference, activations_plus, activations_minus = run_finite_differences(X_train_flattened, y_train, weights, biases, idx=10)\n",
    "    \n",
    "    # Now enter training loop\n",
    "    training_history = train(X_train_flattened, y_train, weights, biases, h1, h2, lr, epochs)\n",
    "    \n",
    "    # Display plots to monitor whether loss functions are correct shape\n",
    "    plot_loss(\"loss.png\", training_history[\"losses\"][:-2])\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plot_loss(\"accuracy.png\", training_history[\"accuracies\"][:-2], label='Training Accuracy')\n",
    "    \n",
    "    # Get weights and biases from best training epoch\n",
    "    best_training_epoch, best_training_accuracy, best_training_loss = get_best_results(training_history)\n",
    "    \n",
    "    # Get dev results\n",
    "    get_results(X_dev_flattened, y_dev, training_history, best_training_epoch)\n",
    "    \n",
    "    # Get test results\n",
    "    get_results(X_test_flattened, y_test, training_history, best_training_epoch, label=\"test\")\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to data\n",
    "data_path = '../setup/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set no. of observations to use for checking finite differences\n",
    "idx = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set perceptron parameters: architecture, learning rate, and no. of training epochs\n",
    "h1 = 8\n",
    "h2 = 4\n",
    "lr = 0.1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW [[-0.18303023 -0.17351476 -0.17373708 -0.1751518 ]]\n",
      "deltaW [[-0.1830302  -0.17351479 -0.17373697 -0.1751517 ]]\n",
      "difference 1.558757383345652e-07\n",
      "[[0.64527729]] [[0.64527729]]\n",
      "8856.98510033028\n",
      "8804.025224598206\n",
      "8756.259330743445\n",
      "8713.189539991632\n",
      "8674.362979977468\n",
      "8639.368290840051\n",
      "8607.832282727755\n",
      "8579.416765249127\n",
      "8553.81556174796\n",
      "8530.751715035882\n",
      "8509.974886243115\n",
      "8491.258944588493\n",
      "8474.39974296122\n",
      "8459.213072094506\n",
      "8445.532784652509\n",
      "8433.209079621134\n",
      "8422.10693687984\n",
      "8412.104691643206\n",
      "8403.092738519073\n",
      "8394.972355171076\n",
      "8387.654635945051\n",
      "8381.059526279885\n",
      "8375.114949240951\n",
      "8369.756016063284\n",
      "8364.924313152345\n",
      "8360.567258548535\n",
      "8356.637521406701\n",
      "8353.092498566457\n",
      "8349.89384278809\n",
      "8347.00703769924\n",
      "8344.4010149374\n",
      "8342.04780938249\n",
      "8339.922248751976\n",
      "8338.001674179652\n",
      "8336.265688718722\n",
      "8334.695931002567\n",
      "8333.275871563286\n",
      "8331.990629551085\n",
      "8330.82680781839\n",
      "8329.772344532652\n",
      "8328.816379663283\n",
      "8327.949134852144\n",
      "8327.16180532537\n",
      "8326.446462638078\n",
      "8325.795967164477\n",
      "8325.20388935462\n",
      "8324.664438877378\n",
      "8324.172400857475\n",
      "8323.72307849416\n",
      "8323.312241420626\n",
      "8322.936079227871\n",
      "8322.591159634623\n",
      "8322.274390837167\n",
      "8321.98298761984\n",
      "8321.714440849135\n",
      "8321.46649001232\n",
      "8321.237098495603\n",
      "8321.024431327514\n",
      "8320.826835140795\n",
      "8320.642820130859\n",
      "8320.471043811198\n",
      "8320.310296386135\n",
      "8320.159487579353\n",
      "8320.017634772868\n",
      "8319.883852325638\n",
      "8319.75734195413\n",
      "8319.63738406896\n",
      "8319.523329972306\n",
      "8319.414594830356\n",
      "8319.310651343552\n",
      "8319.211024045242\n",
      "8319.115284166108\n",
      "8319.0230450082\n",
      "8318.933957777794\n",
      "8318.847707831537\n",
      "8318.76401129477\n",
      "8318.682612015076\n",
      "8318.60327881774\n",
      "8318.525803033166\n",
      "8318.449996269253\n",
      "8318.375688404407\n",
      "8318.302725779307\n",
      "8318.23096956774\n",
      "8318.160294308693\n",
      "8318.090586583778\n",
      "8318.021743825519\n",
      "8317.953673243606\n",
      "8317.886290857357\n",
      "8317.819520623922\n",
      "8317.753293652695\n",
      "8317.68754749747\n",
      "8317.622225518548\n",
      "8317.557276307953\n",
      "8317.492653171472\n",
      "8317.428313661885\n",
      "8317.364219158353\n",
      "8317.300334487363\n",
      "8317.23662758114\n",
      "8317.17306916981\n",
      "8317.109632503982\n",
      "8317.046293104717\n",
      "8316.983028538223\n",
      "8316.919818212777\n",
      "8316.856643195704\n",
      "8316.793486048438\n",
      "8316.730330677852\n",
      "8316.667162202264\n",
      "8316.603966830668\n",
      "8316.540731753876\n",
      "8316.477445046394\n",
      "8316.414095577991\n",
      "8316.350672933953\n",
      "8316.287167343238\n",
      "8316.223569613656\n",
      "8316.159871073496\n",
      "8316.096063518838\n",
      "8316.032139166093\n",
      "8315.968090609185\n",
      "8315.903910780955\n",
      "8315.83959291835\n",
      "8315.775130531034\n",
      "8315.710517373067\n",
      "8315.645747417373\n",
      "8315.580814832703\n",
      "8315.515713962845\n",
      "8315.45043930787\n",
      "8315.384985507211\n",
      "8315.319347324386\n",
      "8315.253519633206\n",
      "8315.187497405328\n",
      "8315.121275698997\n",
      "8315.054849648912\n",
      "8314.988214457024\n",
      "8314.921365384253\n",
      "8314.85429774299\n",
      "8314.787006890298\n",
      "8314.719488221795\n",
      "8314.651737166085\n",
      "8314.583749179725\n",
      "8314.515519742676\n",
      "8314.447044354156\n",
      "8314.378318528896\n",
      "8314.309337793733\n",
      "8314.24009768452\n",
      "8314.1705937433\n",
      "8314.100821515753\n",
      "8314.030776548861\n",
      "8313.960454388773\n",
      "8313.889850578862\n",
      "8313.818960657944\n",
      "8313.747780158643\n",
      "8313.67630460591\n",
      "8313.604529515622\n",
      "8313.532450393339\n",
      "8313.460062733124\n",
      "8313.38736201647\n",
      "8313.314343711281\n",
      "8313.24100327097\n",
      "8313.167336133563\n",
      "8313.093337720918\n",
      "8313.019003437945\n",
      "8312.944328671909\n",
      "8312.869308791756\n",
      "8312.793939147487\n",
      "8312.718215069544\n",
      "8312.642131868262\n",
      "8312.565684833313\n",
      "8312.488869233184\n",
      "8312.411680314697\n",
      "8312.334113302515\n",
      "8312.256163398684\n",
      "8312.177825782193\n",
      "8312.099095608532\n",
      "8312.01996800927\n",
      "8311.940438091657\n",
      "8311.860500938197\n",
      "8311.780151606275\n",
      "8311.699385127758\n",
      "8311.618196508614\n",
      "8311.53658072853\n",
      "8311.454532740547\n",
      "8311.372047470679\n",
      "8311.289119817557\n",
      "8311.205744652048\n",
      "8311.12191681691\n",
      "8311.03763112641\n",
      "8310.952882365973\n",
      "8310.867665291815\n",
      "8310.781974630585\n",
      "8310.695805078994\n",
      "8310.609151303466\n",
      "8310.522007939751\n",
      "8310.434369592578\n",
      "8310.34623083528\n",
      "8310.257586209413\n",
      "8310.168430224408\n",
      "8310.078757357172\n",
      "8309.98856205172\n",
      "8309.897838718798\n",
      "8309.806581735496\n",
      "8309.71478544486\n",
      "8309.622444155502\n",
      "8309.529552141214\n",
      "8309.436103640553\n",
      "8309.342092856466\n",
      "8309.247513955863\n",
      "8309.152361069213\n",
      "8309.056628290136\n",
      "8308.960309674985\n",
      "8308.863399242413\n",
      "8308.765890972954\n",
      "8308.667778808598\n",
      "8308.569056652337\n",
      "8308.469718367734\n",
      "8308.369757778471\n",
      "8308.2691686679\n",
      "8308.167944778583\n",
      "8308.066079811826\n",
      "8307.96356742721\n",
      "8307.860401242116\n",
      "8307.756574831245\n",
      "8307.652081726117\n",
      "8307.546915414601\n",
      "8307.441069340392\n",
      "8307.334536902512\n",
      "8307.227311454797\n",
      "8307.119386305367\n",
      "8307.010754716115\n",
      "8306.901409902151\n",
      "8306.79134503127\n",
      "8306.680553223407\n",
      "8306.56902755006\n",
      "8306.456761033747\n",
      "8306.343746647412\n",
      "8306.229977313858\n",
      "8306.115445905149\n",
      "8306.000145242007\n",
      "8305.884068093223\n",
      "8305.767207175008\n",
      "8305.649555150405\n",
      "8305.53110462863\n",
      "8305.411848164438\n",
      "8305.291778257462\n",
      "8305.170887351565\n",
      "8305.04916783415\n",
      "8304.926612035484\n",
      "8304.80321222802\n",
      "8304.678960625666\n",
      "8304.553849383094\n",
      "8304.427870595006\n",
      "8304.301016295401\n",
      "8304.17327845683\n",
      "8304.044648989628\n",
      "8303.915119741163\n",
      "8303.78468249504\n",
      "8303.653328970311\n",
      "8303.521050820676\n",
      "8303.387839633664\n",
      "8303.253686929791\n",
      "8303.11858416174\n",
      "8302.982522713484\n",
      "8302.845493899427\n",
      "8302.707488963524\n",
      "8302.568499078381\n",
      "8302.428515344349\n",
      "8302.287528788594\n",
      "8302.14553036417\n",
      "8302.002510949054\n",
      "8301.858461345193\n",
      "8301.713372277507\n",
      "8301.567234392907\n",
      "8301.420038259275\n",
      "8301.271774364433\n",
      "8301.12243311511\n",
      "8300.972004835865\n",
      "8300.820479768034\n",
      "8300.66784806861\n",
      "8300.514099809163\n",
      "8300.359224974685\n",
      "8300.203213462468\n",
      "8300.046055080933\n",
      "8299.887739548447\n",
      "8299.728256492133\n",
      "8299.567595446653\n",
      "8299.405745852975\n",
      "8299.24269705711\n",
      "8299.078438308858\n",
      "8298.9129587605\n",
      "8298.746247465493\n",
      "8298.57829337715\n",
      "8298.409085347266\n",
      "8298.23861212477\n",
      "8298.06686235432\n",
      "8297.8938245749\n",
      "8297.719487218368\n",
      "8297.543838608028\n",
      "8297.366866957123\n",
      "8297.188560367369\n",
      "8297.008906827403\n",
      "8296.827894211257\n",
      "8296.64551027679\n",
      "8296.46174266409\n",
      "8296.27657889388\n",
      "8296.090006365854\n",
      "8295.90201235704\n",
      "8295.71258402011\n",
      "8295.52170838166\n",
      "8295.329372340499\n",
      "8295.135562665859\n",
      "8294.940265995643\n",
      "8294.743468834593\n",
      "8294.545157552468\n",
      "8294.34531838217\n",
      "8294.14393741787\n",
      "8293.941000613086\n",
      "8293.736493778735\n",
      "8293.53040258118\n",
      "8293.322712540215\n",
      "8293.113409027053\n",
      "8292.902477262269\n",
      "8292.689902313721\n",
      "8292.475669094434\n",
      "8292.259762360467\n",
      "8292.04216670874\n",
      "8291.822866574841\n",
      "8291.601846230787\n",
      "8291.379089782786\n",
      "8291.154581168918\n",
      "8290.928304156841\n",
      "8290.70024234143\n",
      "8290.470379142393\n",
      "8290.238697801855\n",
      "8290.00518138192\n",
      "8289.769812762177\n",
      "8289.532574637205\n",
      "8289.29344951402\n",
      "8289.05241970949\n",
      "8288.809467347735\n",
      "8288.564574357479\n",
      "8288.31772246936\n",
      "8288.068893213227\n",
      "8287.818067915388\n",
      "8287.565227695823\n",
      "8287.310353465356\n",
      "8287.053425922817\n",
      "8286.794425552138\n",
      "8286.533332619423\n",
      "8286.27012716999\n",
      "8286.004789025359\n",
      "8285.737297780224\n",
      "8285.467632799364\n",
      "8285.195773214538\n",
      "8284.92169792132\n",
      "8284.645385575914\n",
      "8284.366814591922\n",
      "8284.08596313707\n",
      "8283.802809129898\n",
      "8283.517330236413\n",
      "8283.229503866694\n",
      "8282.939307171462\n",
      "8282.646717038613\n",
      "8282.35171008969\n",
      "8282.054262676349\n",
      "8281.754350876745\n",
      "8281.451950491895\n",
      "8281.147037042012\n",
      "8280.839585762755\n",
      "8280.529571601486\n",
      "8280.216969213443\n",
      "8279.90175295789\n",
      "8279.583896894219\n",
      "8279.263374778002\n",
      "8278.940160057013\n",
      "8278.614225867179\n",
      "8278.28554502851\n",
      "8277.954090040977\n",
      "8277.619833080335\n",
      "8277.282745993905\n",
      "8276.942800296314\n",
      "8276.599967165179\n",
      "8276.254217436752\n",
      "8275.905521601508\n",
      "8275.553849799697\n",
      "8275.199171816832\n",
      "8274.841457079143\n",
      "8274.48067464897\n",
      "8274.116793220117\n",
      "8273.749781113143\n",
      "8273.379606270624\n",
      "8273.006236252335\n",
      "8272.629638230403\n",
      "8272.24977898441\n",
      "8271.86662489642\n",
      "8271.48014194599\n",
      "8271.090295705093\n",
      "8270.697051333016\n",
      "8270.300373571183\n",
      "8269.900226737942\n",
      "8269.496574723285\n",
      "8269.089380983529\n",
      "8268.678608535916\n",
      "8268.264219953193\n",
      "8267.846177358097\n",
      "8267.424442417829\n",
      "8266.998976338422\n",
      "8266.569739859098\n",
      "8266.136693246535\n",
      "8265.699796289095\n",
      "8265.259008290976\n",
      "8264.814288066338\n",
      "8264.365593933326\n",
      "8263.912883708073\n",
      "8263.456114698613\n",
      "8262.995243698755\n",
      "8262.530226981888\n",
      "8262.061020294723\n",
      "8261.587578850967\n",
      "8261.109857324955\n",
      "8260.62780984519\n",
      "8260.141389987852\n",
      "8259.650550770211\n",
      "8259.155244643998\n",
      "8258.655423488699\n",
      "8258.151038604788\n",
      "8257.64204070689\n",
      "8257.128379916889\n",
      "8256.61000575693\n",
      "8256.086867142414\n",
      "8255.558912374872\n",
      "8255.02608913478\n",
      "8254.48834447433\n",
      "8253.945624810089\n",
      "8253.397875915616\n",
      "8252.845042913996\n",
      "8252.287070270302\n",
      "8251.723901783971\n",
      "8251.15548058112\n",
      "8250.58174910678\n",
      "8250.00264911705\n",
      "8249.418121671177\n",
      "8248.82810712355\n",
      "8248.232545115618\n",
      "8247.631374567733\n",
      "8247.024533670894\n",
      "8246.411959878431\n",
      "8245.793589897581\n",
      "8245.169359680998\n",
      "8244.539204418157\n",
      "8243.903058526692\n",
      "8243.260855643624\n",
      "8242.612528616515\n",
      "8241.95800949453\n",
      "8241.297229519385\n",
      "8240.630119116227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8239.956607884413\n",
      "8239.276624588176\n",
      "8238.590097147211\n",
      "8237.896952627161\n",
      "8237.197117229993\n",
      "8236.490516284264\n",
      "8235.777074235319\n",
      "8235.056714635339\n",
      "8234.329360133315\n",
      "8233.594932464906\n",
      "8232.85335244217\n",
      "8232.104539943208\n",
      "8231.348413901687\n",
      "8230.584892296249\n",
      "8229.813892139788\n",
      "8229.03532946864\n",
      "8228.249119331633\n",
      "8227.455175779025\n",
      "8226.653411851312\n",
      "8225.843739567918\n",
      "8225.026069915766\n",
      "8224.200312837711\n",
      "8223.366377220847\n",
      "8222.524170884695\n",
      "8221.67360056924\n",
      "8220.81457192286\n",
      "8219.946989490098\n",
      "8219.070756699308\n",
      "8218.185775850177\n",
      "8217.291948101083\n",
      "8216.389173456324\n",
      "8215.477350753223\n",
      "8214.556377649065\n",
      "8213.62615060791\n",
      "8212.686564887244\n",
      "8211.73751452451\n",
      "8210.778892323462\n",
      "8209.810589840406\n",
      "8208.832497370257\n",
      "8207.844503932483\n",
      "8206.84649725687\n",
      "8205.83836376917\n",
      "8204.819988576557\n",
      "8203.79125545298\n",
      "8202.75204682433\n",
      "8201.702243753476\n",
      "8200.641725925148\n",
      "8199.570371630669\n",
      "8198.48805775254\n",
      "8197.394659748876\n",
      "8196.290051637694\n",
      "8195.174105981063\n",
      "8194.046693869099\n",
      "8192.907684903823\n",
      "8191.756947182878\n",
      "8190.594347283104\n",
      "8189.419750243975\n",
      "8188.233019550902\n",
      "8187.034017118409\n",
      "8185.82260327317\n",
      "8184.598636736922\n",
      "8183.361974609263\n",
      "8182.112472350313\n",
      "8180.8499837632835\n",
      "8179.574360976911\n",
      "8178.285454427796\n",
      "8176.983112842646\n",
      "8175.6671832203965\n",
      "8174.3375108142745\n",
      "8172.993939113745\n",
      "8171.636309826395\n",
      "8170.264462859739\n",
      "8168.878236302955\n",
      "8167.477466408567\n",
      "8166.061987574067\n",
      "8164.631632323499\n",
      "8163.186231289003\n",
      "8161.725613192331\n",
      "8160.249604826354\n",
      "8158.758031036541\n",
      "8157.250714702454\n",
      "8155.727476719254\n",
      "8154.188135979215\n",
      "8152.632509353285\n",
      "8151.060411672689\n",
      "8149.471655710576\n",
      "8147.8660521637485\n",
      "8146.243409634472\n",
      "8144.603534612369\n",
      "8142.946231456438\n",
      "8141.271302377179\n",
      "8139.578547418865\n",
      "8137.867764441971\n",
      "8136.1387491057685\n",
      "8134.391294851095\n",
      "8132.625192883343\n",
      "8130.840232155655\n",
      "8129.036199352364\n",
      "8127.212878872686\n",
      "8125.370052814681\n",
      "8123.507500959524\n",
      "8121.625000756067\n",
      "8119.722327305752\n",
      "8117.799253347876\n",
      "8115.855549245225\n",
      "8113.890982970113\n",
      "8111.9053200908365\n",
      "8109.898323758583\n",
      "8107.869754694785\n",
      "8105.819371178997\n",
      "8103.7469290372555\n",
      "8101.652181631009\n",
      "8099.534879846586\n",
      "8097.394772085283\n",
      "8095.231604254054\n",
      "8093.045119756854\n",
      "8090.8350594866515\n",
      "8088.601161818148\n",
      "8086.343162601228\n",
      "8084.060795155162\n",
      "8081.75379026361\n",
      "8079.42187617043\n",
      "8077.064778576343\n",
      "8074.68222063649\n",
      "8072.273922958864\n",
      "8069.83960360373\n",
      "8067.378978083981\n",
      "8064.891759366514\n",
      "8062.377657874651\n",
      "8059.836381491615\n",
      "8057.2676355651165\n",
      "8054.671122913078\n",
      "8052.046543830519\n",
      "8049.393596097643\n",
      "8046.711974989172\n",
      "8044.00137328492\n",
      "8041.261481281703\n",
      "8038.491986806544\n",
      "8035.692575231279\n",
      "8032.862929488527\n",
      "8030.002730089117\n",
      "8027.111655140967\n",
      "8024.18938036945\n",
      "8021.235579139304\n",
      "8018.2499224780695\n",
      "8015.232079101139\n",
      "8012.181715438413\n",
      "8009.098495662591\n",
      "8005.982081719146\n",
      "8002.832133357995\n",
      "7999.648308166892\n",
      "7996.430261606578\n",
      "7993.177647047701\n",
      "7989.890115809536\n",
      "7986.567317200526\n",
      "7983.208898560672\n",
      "7979.8145053057615\n",
      "7976.383780973505\n",
      "7972.916367271542\n",
      "7969.411904127379\n",
      "7965.870029740228\n",
      "7962.290380634802\n",
      "7958.672591717042\n",
      "7955.0162963318\n",
      "7951.321126322475\n",
      "7947.586712092633\n",
      "7943.812682669562\n",
      "7939.998665769814\n",
      "7936.1442878667085\n",
      "7932.2491742597795\n",
      "7928.312949146198\n",
      "7924.335235694114\n",
      "7920.3156561179585\n",
      "7916.253831755637\n",
      "7912.149383147658\n",
      "7908.00193011812\n",
      "7903.811091857586\n",
      "7899.576487007793\n",
      "7895.297733748184\n",
      "7890.97444988424\n",
      "7886.606252937563\n",
      "7882.192760237698\n",
      "7877.733589015661\n",
      "7873.228356499118\n",
      "7868.676680009201\n",
      "7864.078177058898\n",
      "7859.432465452999\n",
      "7854.739163389524\n",
      "7849.99788956262\n",
      "7845.208263266857\n",
      "7840.3699045028725\n",
      "7835.482434084327\n",
      "7830.5454737461105\n",
      "7825.558646253736\n",
      "7820.521575513874\n",
      "7815.433886685976\n",
      "7810.2952062949025\n",
      "7805.105162344524\n",
      "7799.8633844322085\n",
      "7794.569503864153\n",
      "7789.223153771476\n",
      "7783.823969227004\n",
      "7778.371587362705\n",
      "7772.865647487689\n",
      "7767.305791206683\n",
      "7761.691662538959\n",
      "7756.022908037576\n",
      "7750.299176908948\n",
      "7744.520121132567\n",
      "7738.685395580882\n",
      "7732.79465813922\n",
      "7726.847569825677\n",
      "7720.8437949108975\n",
      "7714.783001037666\n",
      "7708.66485934022\n",
      "7702.489044563212\n",
      "7696.25523518022\n",
      "7689.963113511723\n",
      "7683.612365842466\n",
      "7677.202682538082\n",
      "7670.7337581609445\n",
      "7664.205291585081\n",
      "7657.6169861101025\n",
      "7650.968549574023\n",
      "7644.259694464879\n",
      "7637.490138031035\n",
      "7630.659602390064\n",
      "7623.767814636122\n",
      "7616.814506945663\n",
      "7609.799416681408\n",
      "7602.722286494443\n",
      "7595.582864424336\n",
      "7588.380903997119\n",
      "7581.11616432106\n",
      "7573.788410180061\n",
      "7566.39741212456\n",
      "7558.94294655983\n",
      "7551.424795831503\n",
      "7543.842748308232\n",
      "7536.196598461316\n",
      "7528.486146941166\n",
      "7520.711200650498\n",
      "7512.871572814066\n",
      "7504.967083044849\n",
      "7496.9975574065265\n",
      "7488.962828472102\n",
      "7480.862735378554\n",
      "7472.6971238773785\n",
      "7464.465846380872\n",
      "7456.168762004045\n",
      "7447.805736602031\n",
      "7439.376642802854\n",
      "7430.881360035454\n",
      "7422.319774552845\n",
      "7413.691779450275\n",
      "7404.997274678318\n",
      "7396.236167050758\n",
      "7387.408370247199\n",
      "7378.513804810296\n",
      "7369.55239813753\n",
      "7360.524084467459\n",
      "7351.428804860374\n",
      "7342.266507173314\n",
      "7333.037146029372\n",
      "7323.740682781289\n",
      "7314.377085469282\n",
      "7304.946328773102\n",
      "7295.448393958324\n",
      "7285.883268816868\n",
      "7276.250947601771\n",
      "7266.551430956268\n",
      "7256.78472583718\n",
      "7246.950845432724\n",
      "7237.049809074776\n",
      "7227.081642145702\n",
      "7217.04637597984\n",
      "7206.94404775976\n",
      "7196.774700407436\n",
      "7186.538382470472\n",
      "7176.2351480035295\n",
      "7165.865056445174\n",
      "7155.428172490281\n",
      "7144.924565958251\n",
      "7134.354311657247\n",
      "7123.71748924467\n",
      "7113.014183084172\n",
      "7102.244482099444\n",
      "7091.408479625072\n",
      "7080.506273254783\n",
      "7069.537964687383\n",
      "7058.503659570712\n",
      "7047.403467344004\n",
      "7036.23750107898\n",
      "7025.005877320057\n",
      "7013.708715924113\n",
      "7002.346139900149\n",
      "6990.918275249334\n",
      "6979.425250805838\n",
      "6967.867198078915\n",
      "6956.244251096704\n",
      "6944.55654625222\n",
      "6932.804222152033\n",
      "6920.987419468142\n",
      "6909.106280793536\n",
      "6897.160950501995\n",
      "6885.15157461265\n",
      "6873.078300659847\n",
      "6860.941277568867\n",
      "6848.740655538072\n",
      "6836.476585928029\n",
      "6824.149221158184\n",
      "6811.75871461168\n",
      "6799.305220548882\n",
      "6786.7888940301855\n",
      "6774.209890848714\n",
      "6761.568367473465\n",
      "6748.8644810034775\n",
      "6736.09838913364\n",
      "6723.27025013264\n",
      "6710.380222833682\n",
      "6697.428466638474\n",
      "6684.415141535061\n",
      "6671.340408130016\n",
      "6658.204427695484\n",
      "6645.007362231609\n",
      "6631.749374544774\n",
      "6618.4306283421365\n",
      "6605.051288342862\n",
      "6591.611520406448\n",
      "6578.111491678524\n",
      "6564.551370754412\n",
      "6550.9313278607915\n",
      "6537.25153505567\n",
      "6523.512166446892\n",
      "6509.7133984293405\n",
      "6495.855409940946\n",
      "6481.938382737529\n",
      "6467.9625016865275\n",
      "6453.927955079511\n",
      "6439.83493496337\n",
      "6425.683637490001\n",
      "6411.474263284243\n",
      "6397.207017829704\n",
      "6382.88211187213\n",
      "6368.499761839811\n",
      "6354.060190280486\n",
      "6339.563626314115\n",
      "6325.010306100823\n",
      "6310.400473323202\n",
      "6295.734379682124\n",
      "6281.0122854050915\n",
      "6266.234459766106\n",
      "6251.401181615904\n",
      "6236.512739921402\n",
      "6221.569434313033\n",
      "6206.5715756386335\n",
      "6191.519486522445\n",
      "6176.41350192772\n",
      "6161.253969721364\n",
      "6146.041251238972\n",
      "6130.775721848555\n",
      "6115.45777151121\n",
      "6100.087805336938\n",
      "6084.666244133756\n",
      "6069.193524948229\n",
      "6053.670101595518\n",
      "6038.096445177019\n",
      "6022.47304458365\n",
      "6006.800406982867\n",
      "5991.079058287435\n",
      "5975.309543604086\n",
      "5959.492427660127\n",
      "5943.628295206161\n",
      "5927.717751393111\n",
      "5911.761422121755\n",
      "5895.759954363096\n",
      "5879.7140164479415\n",
      "5863.624298324118\n",
      "5847.491511779885\n",
      "5831.316390632181\n",
      "5815.099690878439\n",
      "5798.842190810839\n",
      "5782.54469109196\n",
      "5766.208014790946\n",
      "5749.83300737942\n",
      "5733.420536686498\n",
      "5716.971492812422\n",
      "5700.486788000477\n",
      "5683.967356466949\n",
      "5667.414154189115\n",
      "5650.828158651306\n",
      "5634.210368549315\n",
      "5617.561803453515\n",
      "5600.88350343121\n",
      "5584.1765286288955\n",
      "5567.441958815225\n",
      "5550.68089288562\n",
      "5533.8944483296\n",
      "5517.083760662004\n",
      "5500.249982819431\n",
      "5483.3942845233\n",
      "5466.517851611057\n",
      "5449.621885337156\n",
      "5432.7076016454885\n",
      "5415.776230415091\n",
      "5398.829014680932\n",
      "5381.867209831749\n",
      "5364.892082786862\n",
      "5347.904911154019\n",
      "5330.906982370281\n",
      "5313.899592828075\n",
      "5296.8840469884635\n",
      "5279.861656483774\n",
      "5262.833739211661\n",
      "5245.801618422729\n",
      "5228.766621803776\n",
      "5211.7300805587265\n",
      "5194.693328489282\n",
      "5177.657701077262\n",
      "5160.62453457061\n",
      "5143.595165074918\n",
      "5126.570927652339\n",
      "5109.553155429632\n",
      "5092.543178717069\n",
      "5075.542324139808\n",
      "5058.551913783313\n",
      "5041.573264354275\n",
      "5024.607686358444\n",
      "5007.656483296665\n",
      "4990.720950880334\n",
      "4973.8023762674275\n",
      "4956.902037320109\n",
      "4940.021201884879\n",
      "4923.161127096129\n",
      "4906.323058703847\n",
      "4889.508230426152\n",
      "4872.717863327254\n",
      "4855.953165221301\n",
      "4839.215330102564\n",
      "4822.505537602234\n",
      "4805.824952472108\n",
      "4789.174724095301\n",
      "4772.555986024072\n",
      "4755.969855544761\n",
      "4739.417433269794\n",
      "4722.899802756596\n",
      "4706.418030153233\n",
      "4689.973163870523\n",
      "4673.56623428029\n",
      "4657.198253439404\n",
      "4640.870214839191\n",
      "4624.5830931797445\n",
      "4608.337844168624\n",
      "4592.135404343423\n",
      "4575.976690917612\n",
      "4559.862601649049\n",
      "4543.794014730532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4527.771788701733\n",
      "4511.796762381841\n",
      "4495.869754822212\n",
      "4479.991565278324\n",
      "4464.162973200335\n",
      "4448.384738241486\n",
      "4432.657600283666\n",
      "4416.982279479365\n",
      "4401.359476309318\n",
      "4385.7898716550935\n",
      "4370.274126885923\n",
      "4354.812883959025\n",
      "4339.406765532763\n",
      "4324.05637509189\n",
      "4308.762297084244\n",
      "4293.525097068183\n",
      "4278.345321870128\n",
      "4263.223499751561\n",
      "4248.160140584858\n",
      "4233.1557360373445\n",
      "4218.210759762982\n",
      "4203.325667601122\n",
      "4188.5008977817615\n",
      "4173.73687113677\n",
      "4159.033991316577\n",
      "4144.392645011818\n",
      "4129.813202179462\n",
      "4115.296016272961\n",
      "4100.841424475999\n",
      "4086.4497479393835\n",
      "4072.121292020734\n",
      "4057.8563465265297\n",
      "4043.6551859561896\n",
      "4029.5180697478286\n",
      "4015.445242525351\n",
      "4001.436934346588\n",
      "3987.493360952173\n",
      "3973.614724014878\n",
      "3959.8012113891464\n",
      "3946.0529973605867\n",
      "3932.3702428951747\n",
      "3918.7530958879597\n",
      "3905.201691411069\n",
      "3891.716151960807\n",
      "3878.296587703687\n",
      "3864.9430967212056\n",
      "3851.6557652532338\n",
      "3838.434667939843\n",
      "3825.279868061469\n",
      "3812.191417777259\n",
      "3799.169358361506\n",
      "3786.213720438055\n",
      "3773.3245242125886\n",
      "3760.5017797027017\n",
      "3747.745486965683\n",
      "3735.0556363239284\n",
      "3722.432208587925\n",
      "3709.8751752767394\n",
      "3697.3844988359588\n",
      "3684.9601328530343\n",
      "3672.6020222699885\n",
      "3660.3101035934437\n",
      "3648.0843051019424\n",
      "3635.924547050532\n",
      "3623.8307418725844\n",
      "3611.802794378835\n",
      "3599.840601953631\n",
      "3587.9440547483564\n",
      "3576.1130358720548\n",
      "3564.347421579213\n",
      "3552.6470814547247\n",
      "3541.0118785960194\n",
      "3529.441669792369\n",
      "3517.9363057013657\n",
      "3506.4956310225944\n",
      "3495.119484668481\n",
      "3483.807699932357\n",
      "3472.5601046537345\n",
      "3461.3765213808024\n",
      "3450.2567675301752\n",
      "3439.2006555438993\n",
      "3428.207993043731\n",
      "3417.278582982718\n",
      "3406.412223794093\n",
      "3395.608709537505\n",
      "3384.8678300426177\n",
      "3374.1893710500763\n",
      "3363.5731143498906\n",
      "3353.018837917248\n",
      "best accuracy: 0.9484166666666667\n",
      "best loss: 3353.018837917248\n",
      "best epoch: 999\n",
      "dev set accuracy: 0.958\n",
      "test set accuracy: 0.502\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute pipeline\n",
    "execute_pipeline(data_path, idx, h1, h2, lr, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
