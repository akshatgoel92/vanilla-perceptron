{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "%reset -f\n",
    "from helpers import load_all_data, vectorized_flatten, sigmoid, get_log_loss, get_accuracy \n",
    "from helpers import sigmoid_derivative, gradient_update, plot_loss, prep_data, get_best_results\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X, h1, h2): \n",
    "    '''\n",
    "    --------------------\n",
    "    Parameter Initialization\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X [n = 12000])\n",
    "    --------------------\n",
    "    Output: \n",
    "    weights: Weight terms initialized as random normals\n",
    "    biases: Bias terms initialized to zero\n",
    "    --------------------\n",
    "    '''\n",
    "    dim1 = 1/np.sqrt(X.shape[0])\n",
    "    W1 = dim1 * np.random.randn(h1, 28**2)\n",
    "    \n",
    "    dim2 = 1/np.sqrt(W1.shape[1])\n",
    "    W2 = dim2 * np.random.randn(h2, h1)\n",
    "    \n",
    "    dim3 = 1/np.sqrt(W2.shape[1])\n",
    "    W3 = dim3 * np.random.randn(1, h2)\n",
    "\n",
    "    b1 = np.zeros((h1, 1))\n",
    "    b2 = np.zeros((h2, 1))\n",
    "    b3 = np.zeros((1, 1))\n",
    "    \n",
    "    weights = [W1, W2, W3]\n",
    "    biases = [b1, b2, b3]\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, weights, biases):\n",
    "    '''\n",
    "    ----------------------------------\n",
    "    Forward propogation:\n",
    "    Send inputs through the network to\n",
    "    generate output\n",
    "    ----------------------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    weights: Binary (1/0) training label (shape = n X 1)\n",
    "    biases:\n",
    "    --------------------\n",
    "    Output: \n",
    "    activations: vector of results from passing\n",
    "    inputs through each neuron\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    \n",
    "    z1 = W1 @ X + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    activations = [z1, a1, z2, a2, z3, a3]\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, weights, biases, activations):\n",
    "    '''\n",
    "    --------------------\n",
    "    Backpropagation\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = 784 X n)\n",
    "    y: Binary (1/0) training label (shape = n X 1)\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    activations: Current set of activations\n",
    "    --------------------\n",
    "    Output: \n",
    "    Derivatives required\n",
    "    for optimization update\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    z1, a1, z2, a2, z3, a3 = activations\n",
    "    m = max(y.shape)\n",
    "    # print(m)\n",
    "    \n",
    "    dz3 = (a3 - y)/m\n",
    "    # print(\"dz3\", dz3.shape)\n",
    "    \n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    # print(\"dW3\", dW3.shape)\n",
    "    \n",
    "    db3 = np.sum(dz3, axis=1).reshape(-1, 1)\n",
    "    # print(\"db3\", db3.shape)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    # print(\"da2\", da2.shape)\n",
    "    \n",
    "    dz2 = da2 * sigmoid_derivative(z2)\n",
    "    # print(\"dz2\", dz2.shape)\n",
    "    \n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    # print(\"dW2\", dW2.shape)\n",
    "    \n",
    "    db2 = np.sum(dz2, axis=1).reshape(-1, 1)\n",
    "    # print(\"db2\", db2.shape)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    # print(\"da1\", da1.shape)\n",
    "    \n",
    "    dz1 = da1 * sigmoid_derivative(z1)\n",
    "    # print(\"dz1\", dz1.shape)\n",
    "    \n",
    "    dW1 = np.dot(dz1, X.T)\n",
    "    # print(\"dW1\", dW1.shape)\n",
    "    \n",
    "    db1 = np.sum(dz1, axis=1).reshape(-1, 1)\n",
    "    # print(\"db1\", db1.shape)\n",
    "    \n",
    "    return db1, dW1, db2, dW2, db3, dW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    '''\n",
    "    --------------------\n",
    "    Update parameters\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2, db3, dW3 = gradients\n",
    "    \n",
    "    W1 = gradient_update(W1, learning_rate, dW1)\n",
    "    W2 = gradient_update(W2, learning_rate, dW2)\n",
    "    W3 = gradient_update(W3, learning_rate, dW3)\n",
    "   \n",
    "    b1 = gradient_update(b1, learning_rate, db1)\n",
    "    b2 = gradient_update(b2, learning_rate, db2)\n",
    "    b3 = gradient_update(b3, learning_rate, db3)\n",
    "    \n",
    "    weights = [W1, W2, W3]\n",
    "    biases = [b1, b2, b3]\n",
    "    \n",
    "    return [weights, biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_differences(example, truth, weights, biases, delta_h=1e-7, tolerance=1e-7):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    \n",
    "    activations = forward_pass(example, weights, biases)\n",
    "    db1, dW1, db2, dW2, db3, dW3 = backpropagation(example, truth, weights, biases, activations)\n",
    "    \n",
    "    \n",
    "    weight_gradients = [dW1, dW2, dW3]\n",
    "    bias_gradients = [db1, db2, db3]\n",
    "    \n",
    "    deltaW_lst = []\n",
    "    weight_differences = []\n",
    "    for l in range(len(weights)):\n",
    "        I, J = weights[l].shape \n",
    "        \n",
    "\n",
    "        deltaW_lst.append(np.zeros((I, J)))\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "\n",
    "                weights[l][i][j] += delta_h\n",
    "                activations_plus = forward_pass(example, weights, biases)\n",
    "                \n",
    "                weights[l][i][j] -= 2* delta_h\n",
    "                activations_minus = forward_pass(example, weights, biases)\n",
    "                \n",
    "                weights[l][i][j] += delta_h\n",
    "\n",
    "#                 W_minus[i][j] -= delta_h\n",
    "\n",
    "#                 weights_plus = [W1, W2, W_plus] # Change here\n",
    "#                 weights_minus = [W1, W2, W_minus] # Change here\n",
    "\n",
    "\n",
    "\n",
    "#                 weights[l][i][j] += delta_h\n",
    "\n",
    "                loss_plus = get_log_loss(truth, activations_plus[-1])\n",
    "                loss_minus =  get_log_loss(truth, activations_minus[-1])\n",
    "\n",
    "                deltaW_lst[l][i][j] = (loss_plus - loss_minus)/(2 * delta_h)\n",
    "\n",
    "        difference = np.linalg.norm(weight_gradients[l] - deltaW_lst[l]) # Change here\n",
    "        weight_differences.append(difference)\n",
    "        \n",
    "        \n",
    "    deltab_lst = []\n",
    "    bias_differences = []\n",
    "    for l in range(len(biases)):\n",
    "        I, J = biases[l].shape \n",
    "        \n",
    "\n",
    "        deltab_lst.append(np.zeros((I, J)))\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "\n",
    "                biases[l][i][j] += delta_h\n",
    "                activations_plus = forward_pass(example, weights, biases)\n",
    "                \n",
    "                biases[l][i][j] -= 2* delta_h\n",
    "                activations_minus = forward_pass(example, weights, biases)\n",
    "                \n",
    "                biases[l][i][j] += delta_h\n",
    "\n",
    "#                 W_minus[i][j] -= delta_h\n",
    "\n",
    "#                 weights_plus = [W1, W2, W_plus] # Change here\n",
    "#                 weights_minus = [W1, W2, W_minus] # Change here\n",
    "\n",
    "\n",
    "\n",
    "#                 weights[l][i][j] += delta_h\n",
    "\n",
    "                loss_plus = get_log_loss(truth, activations_plus[-1])\n",
    "                loss_minus =  get_log_loss(truth, activations_minus[-1])\n",
    "\n",
    "                deltab_lst[l][i][j] = (loss_plus - loss_minus)/(2 * delta_h)\n",
    "\n",
    "        difference = np.linalg.norm(bias_gradients[l] - deltab_lst[l]) # Change here\n",
    "        bias_differences.append(difference)\n",
    "        \n",
    "    return weight_differences, bias_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finite_differences(X_train_flattened, y_train, weights, biases, idx=10):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    weight_differences, bias_differences = finite_differences(X_train_flattened[:, idx].reshape(-1, 1), y_train[:, idx].reshape(-1, 1), weights, biases)\n",
    "    \n",
    "    print(\"weight_differences\", weight_differences)\n",
    "    print(\"bias_differences\", bias_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, w, b, h1, h2, lr, epochs, tolerance=1e-1):    \n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Initialize history dictionary\n",
    "    history = {\n",
    "        \"weights\": [w],\n",
    "        \"losses\": [], \n",
    "        \"biases\": [b],\n",
    "        \"accuracies\": []\n",
    "    }\n",
    "    \n",
    "    convergence_counter = 0\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    # Do this for the specified epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Get weights and bias\n",
    "        w = history['weights'][epoch]\n",
    "        b = history['biases'][epoch]\n",
    "        \n",
    "        # Forward pass to get activations\n",
    "        activations = forward_pass(X, w, b)\n",
    "        \n",
    "        # Backward pass to get gradients\n",
    "        gradients = backpropagation(X, y, w, b, activations)\n",
    "        \n",
    "        # Gradient descent update\n",
    "        w, b = update_parameters(w, b, gradients, lr)\n",
    "    \n",
    "        # Get last layer output\n",
    "        y_prob = activations[-1]\n",
    "        \n",
    "        # Threshold\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    \n",
    "        # Get loss and accuracy results\n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "        \n",
    "        # Check convergence, keeps a counter of how many epochs it has been without an improvement\n",
    "        # Counter resets whenever there's an improvent            \n",
    "        if loss < best_loss - tolerance:\n",
    "            best_loss = loss\n",
    "            convergence_counter = 0\n",
    "        else:\n",
    "            convergence_counter += 1\n",
    "\n",
    "        # Append results to history\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"biases\"].append(b)\n",
    "        history[\"weights\"].append(w)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "        \n",
    "        # 10 epochs without an improvment is considered to have converged\n",
    "        if convergence_counter == 10:\n",
    "            break\n",
    "        \n",
    "        # Display loss for monitoring\n",
    "        print(loss)\n",
    "        \n",
    "        # Stop training if numerical loss underflows\n",
    "        if np.isnan(loss): break\n",
    "        \n",
    "        # Store loss for next epoch\n",
    "        previous_accuracy = accuracy\n",
    "    \n",
    "    # Return statement\n",
    "    return(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(X_dev, y_dev, history, best_epoch, label=\"dev\"):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    w = history[\"weights\"][best_epoch]\n",
    "    b = history[\"biases\"][best_epoch]\n",
    "    activations = forward_pass(X_dev, w, b)\n",
    "\n",
    "    y_dev_prob = activations[-1]\n",
    "    y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "\n",
    "    loss = get_log_loss(y_dev, y_dev_prob)\n",
    "    accuracy = get_accuracy(y_dev, y_dev_pred)\n",
    "    print(f\"{label} set accuracy: {accuracy}\")\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(data_path, idx, h1, h2, lr, epochs):\n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    # Set seed for reproducible results\n",
    "    np.random.seed(1252908)\n",
    "\n",
    "    # Get data\n",
    "    X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test = prep_data(data_path)\n",
    "\n",
    "    # Initialize weights\n",
    "    weights, biases = initialize(X_train_flattened, h1, h2)\n",
    "    \n",
    "    # Check finite difference\n",
    "    run_finite_differences(X_train_flattened, y_train, weights, biases, idx=10)\n",
    "    \n",
    "    # Now enter training loop\n",
    "    training_history = train(X_train_flattened, y_train, weights, biases, h1, h2, lr, epochs)\n",
    "    \n",
    "    # Display plots to monitor whether loss functions are correct shape\n",
    "    plot_loss(\"loss.png\", training_history[\"losses\"][:-2])\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plot_loss(\"accuracy.png\", training_history[\"accuracies\"][:-2], label='Training Accuracy')\n",
    "    \n",
    "    # Get weights and biases from best training epoch\n",
    "    best_training_epoch, best_training_accuracy, best_training_loss = get_best_results(training_history)\n",
    "    \n",
    "    # Get dev results\n",
    "    get_results(X_dev_flattened, y_dev, training_history, best_training_epoch)\n",
    "    \n",
    "    # Get test results\n",
    "    get_results(X_test_flattened, y_test, training_history, best_training_epoch, label=\"test\")\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to data\n",
    "data_path = '../setup/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set perceptron parameters: architecture, learning rate, and no. of training epochs\n",
    "h1 = 8\n",
    "h2 = 4\n",
    "lr = 0.1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set no. of observations to use for checking finite differences\n",
    "idx = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_differences [6.65688346243071e-08, 1.846319562773607e-08, 5.509821962198041e-08]\n",
      "bias_differences [2.510028947421227e-09, 1.1331843861559304e-08, 5.5933531362128974e-08]\n",
      "8856.98510033028\n",
      "8804.025224598205\n",
      "8756.259330743445\n",
      "8713.189539991632\n",
      "8674.362979977468\n",
      "8639.368290840051\n",
      "8607.832282727755\n",
      "8579.416765249127\n",
      "8553.81556174796\n",
      "8530.751715035882\n",
      "8509.974886243115\n",
      "8491.258944588493\n",
      "8474.399742961223\n",
      "8459.213072094506\n",
      "8445.532784652509\n",
      "8433.209079621134\n",
      "8422.10693687984\n",
      "8412.104691643206\n",
      "8403.092738519073\n",
      "8394.972355171076\n",
      "8387.654635945051\n",
      "8381.059526279885\n",
      "8375.114949240951\n",
      "8369.756016063284\n",
      "8364.924313152345\n",
      "8360.567258548535\n",
      "8356.637521406701\n",
      "8353.092498566457\n",
      "8349.89384278809\n",
      "8347.00703769924\n",
      "8344.4010149374\n",
      "8342.04780938249\n",
      "8339.922248751976\n",
      "8338.001674179652\n",
      "8336.265688718722\n",
      "8334.695931002567\n",
      "8333.275871563284\n",
      "8331.990629551085\n",
      "8330.82680781839\n",
      "8329.772344532652\n",
      "8328.816379663283\n",
      "8327.949134852144\n",
      "8327.16180532537\n",
      "8326.44646263808\n",
      "8325.795967164477\n",
      "8325.20388935462\n",
      "8324.664438877378\n",
      "8324.172400857475\n",
      "8323.72307849416\n",
      "8323.312241420626\n",
      "8322.936079227871\n",
      "8322.591159634623\n",
      "8322.274390837167\n",
      "8321.98298761984\n",
      "8321.714440849135\n",
      "8321.46649001232\n",
      "8321.237098495603\n",
      "8321.024431327514\n",
      "8320.826835140795\n",
      "8320.642820130859\n",
      "8320.4710438112\n",
      "8320.310296386135\n",
      "8320.159487579353\n",
      "8320.017634772868\n",
      "8319.883852325638\n",
      "8319.75734195413\n",
      "8319.63738406896\n",
      "8319.523329972306\n",
      "8319.414594830356\n",
      "8319.310651343552\n",
      "8319.211024045242\n",
      "8319.115284166108\n",
      "8319.0230450082\n",
      "8318.933957777794\n",
      "8318.847707831537\n",
      "8318.76401129477\n",
      "8318.682612015076\n",
      "8318.60327881774\n",
      "8318.525803033166\n",
      "8318.449996269253\n",
      "8318.375688404407\n",
      "8318.302725779307\n",
      "8318.23096956774\n",
      "8318.160294308695\n",
      "8318.090586583778\n",
      "8318.021743825519\n",
      "8317.953673243606\n",
      "8317.886290857357\n",
      "8317.819520623922\n",
      "8317.753293652695\n",
      "8317.68754749747\n",
      "8317.622225518548\n",
      "8317.557276307953\n",
      "8317.492653171472\n",
      "8317.428313661885\n",
      "8317.364219158353\n",
      "8317.300334487363\n",
      "8317.23662758114\n",
      "8317.17306916981\n",
      "8317.109632503982\n",
      "8317.046293104717\n",
      "8316.983028538223\n",
      "8316.919818212777\n",
      "8316.856643195704\n",
      "8316.793486048438\n",
      "8316.730330677852\n",
      "8316.667162202264\n",
      "8316.603966830668\n",
      "8316.540731753876\n",
      "8316.477445046394\n",
      "8316.414095577991\n",
      "8316.350672933953\n",
      "8316.287167343238\n",
      "8316.223569613656\n",
      "8316.159871073496\n",
      "8316.096063518838\n",
      "8316.032139166093\n",
      "8315.968090609185\n",
      "8315.903910780955\n",
      "8315.83959291835\n",
      "8315.775130531034\n",
      "8315.710517373067\n",
      "8315.645747417373\n",
      "8315.580814832703\n",
      "8315.515713962845\n",
      "8315.45043930787\n",
      "8315.384985507211\n",
      "8315.319347324388\n",
      "8315.253519633206\n",
      "8315.187497405328\n",
      "8315.121275698997\n",
      "8315.054849648912\n",
      "8314.988214457024\n",
      "8314.921365384253\n",
      "8314.85429774299\n",
      "8314.787006890298\n",
      "8314.719488221795\n",
      "8314.651737166085\n",
      "8314.583749179725\n",
      "8314.515519742676\n",
      "8314.447044354156\n",
      "8314.378318528896\n",
      "8314.309337793733\n",
      "8314.24009768452\n",
      "8314.1705937433\n",
      "8314.100821515753\n",
      "8314.030776548861\n",
      "8313.960454388773\n",
      "8313.889850578862\n",
      "8313.818960657944\n",
      "8313.747780158643\n",
      "8313.67630460591\n",
      "8313.604529515622\n",
      "8313.532450393339\n",
      "8313.460062733124\n",
      "8313.38736201647\n",
      "8313.314343711281\n",
      "8313.24100327097\n",
      "8313.167336133563\n",
      "8313.093337720918\n",
      "8313.019003437945\n",
      "8312.944328671909\n",
      "8312.869308791756\n",
      "8312.793939147487\n",
      "8312.718215069544\n",
      "8312.642131868262\n",
      "8312.565684833311\n",
      "8312.488869233184\n",
      "8312.411680314697\n",
      "8312.334113302515\n",
      "8312.256163398684\n",
      "8312.177825782193\n",
      "8312.099095608532\n",
      "8312.01996800927\n",
      "8311.940438091657\n",
      "8311.860500938197\n",
      "8311.780151606274\n",
      "8311.699385127758\n",
      "8311.618196508614\n",
      "8311.53658072853\n",
      "8311.454532740547\n",
      "8311.372047470679\n",
      "8311.289119817557\n",
      "8311.205744652048\n",
      "8311.12191681691\n",
      "8311.03763112641\n",
      "8310.952882365973\n",
      "8310.867665291815\n",
      "8310.781974630583\n",
      "8310.695805078994\n",
      "8310.609151303464\n",
      "8310.522007939751\n",
      "8310.434369592578\n",
      "8310.34623083528\n",
      "8310.257586209413\n",
      "8310.168430224408\n",
      "8310.07875735717\n",
      "8309.98856205172\n",
      "8309.897838718798\n",
      "8309.806581735496\n",
      "8309.71478544486\n",
      "8309.622444155502\n",
      "8309.529552141214\n",
      "8309.436103640553\n",
      "8309.342092856466\n",
      "8309.247513955863\n",
      "8309.152361069213\n",
      "8309.056628290136\n",
      "8308.960309674983\n",
      "8308.863399242413\n",
      "8308.765890972954\n",
      "8308.667778808598\n",
      "8308.569056652337\n",
      "8308.469718367733\n",
      "8308.369757778471\n",
      "8308.2691686679\n",
      "8308.167944778583\n",
      "8308.066079811826\n",
      "8307.96356742721\n",
      "8307.860401242116\n",
      "8307.756574831245\n",
      "8307.652081726117\n",
      "8307.546915414601\n",
      "8307.441069340392\n",
      "8307.334536902512\n",
      "8307.227311454797\n",
      "8307.119386305367\n",
      "8307.010754716115\n",
      "8306.901409902151\n",
      "8306.79134503127\n",
      "8306.680553223407\n",
      "8306.56902755006\n",
      "8306.456761033747\n",
      "8306.343746647413\n",
      "8306.229977313858\n",
      "8306.115445905149\n",
      "8306.000145242007\n",
      "8305.884068093223\n",
      "8305.767207175008\n",
      "8305.649555150405\n",
      "8305.53110462863\n",
      "8305.411848164438\n",
      "8305.291778257462\n",
      "8305.170887351565\n",
      "8305.04916783415\n",
      "8304.926612035484\n",
      "8304.803212228018\n",
      "8304.678960625666\n",
      "8304.553849383094\n",
      "8304.427870595006\n",
      "8304.301016295401\n",
      "8304.17327845683\n",
      "8304.044648989628\n",
      "8303.915119741163\n",
      "8303.78468249504\n",
      "8303.653328970311\n",
      "8303.521050820676\n",
      "8303.387839633664\n",
      "8303.253686929791\n",
      "8303.11858416174\n",
      "8302.982522713484\n",
      "8302.845493899427\n",
      "8302.707488963524\n",
      "8302.568499078381\n",
      "8302.428515344349\n",
      "8302.287528788594\n",
      "8302.14553036417\n",
      "8302.002510949054\n",
      "8301.858461345193\n",
      "8301.713372277507\n",
      "8301.567234392907\n",
      "8301.420038259275\n",
      "8301.271774364433\n",
      "8301.12243311511\n",
      "8300.972004835865\n",
      "8300.820479768034\n",
      "8300.66784806861\n",
      "8300.514099809163\n",
      "8300.359224974685\n",
      "8300.203213462468\n",
      "8300.046055080933\n",
      "8299.887739548447\n",
      "8299.728256492133\n",
      "8299.567595446653\n",
      "8299.405745852975\n",
      "8299.24269705711\n",
      "8299.078438308858\n",
      "8298.912958760498\n",
      "8298.746247465493\n",
      "8298.57829337715\n",
      "8298.409085347266\n",
      "8298.238612124771\n",
      "8298.06686235432\n",
      "8297.8938245749\n",
      "8297.719487218368\n",
      "8297.543838608028\n",
      "8297.366866957123\n",
      "8297.188560367369\n",
      "8297.008906827403\n",
      "8296.827894211257\n",
      "8296.64551027679\n",
      "8296.46174266409\n",
      "8296.27657889388\n",
      "8296.090006365854\n",
      "8295.90201235704\n",
      "8295.71258402011\n",
      "8295.521708381662\n",
      "8295.329372340499\n",
      "8295.135562665859\n",
      "8294.940265995643\n",
      "8294.743468834593\n",
      "8294.545157552468\n",
      "8294.34531838217\n",
      "8294.14393741787\n",
      "8293.941000613086\n",
      "8293.736493778737\n",
      "8293.53040258118\n",
      "8293.322712540215\n",
      "8293.113409027053\n",
      "8292.902477262269\n",
      "8292.689902313721\n",
      "8292.475669094434\n",
      "8292.259762360467\n",
      "8292.04216670874\n",
      "8291.822866574841\n",
      "8291.601846230787\n",
      "8291.379089782784\n",
      "8291.154581168918\n",
      "8290.928304156841\n",
      "8290.70024234143\n",
      "8290.470379142393\n",
      "8290.238697801855\n",
      "8290.00518138192\n",
      "8289.769812762177\n",
      "8289.532574637207\n",
      "8289.29344951402\n",
      "8289.05241970949\n",
      "8288.809467347737\n",
      "8288.564574357479\n",
      "8288.31772246936\n",
      "8288.068893213229\n",
      "8287.818067915388\n",
      "8287.565227695823\n",
      "8287.310353465356\n",
      "8287.053425922819\n",
      "8286.794425552138\n",
      "8286.533332619423\n",
      "8286.27012716999\n",
      "8286.004789025359\n",
      "8285.737297780224\n",
      "8285.467632799364\n",
      "8285.195773214538\n",
      "8284.92169792132\n",
      "8284.645385575914\n",
      "8284.366814591922\n",
      "8284.08596313707\n",
      "8283.802809129898\n",
      "8283.517330236413\n",
      "8283.229503866694\n",
      "8282.939307171462\n",
      "8282.646717038613\n",
      "8282.35171008969\n",
      "8282.054262676349\n",
      "8281.754350876743\n",
      "8281.451950491895\n",
      "8281.147037042012\n",
      "8280.839585762755\n",
      "8280.529571601486\n",
      "8280.216969213443\n",
      "8279.90175295789\n",
      "8279.583896894219\n",
      "8279.263374778004\n",
      "8278.940160057013\n",
      "8278.614225867179\n",
      "8278.28554502851\n",
      "8277.954090040977\n",
      "8277.619833080335\n",
      "8277.282745993905\n",
      "8276.942800296314\n",
      "8276.59996716518\n",
      "8276.254217436752\n",
      "8275.905521601508\n",
      "8275.553849799697\n",
      "8275.199171816832\n",
      "8274.841457079143\n",
      "8274.48067464897\n",
      "8274.116793220117\n",
      "8273.749781113143\n",
      "8273.379606270624\n",
      "8273.006236252335\n",
      "8272.629638230403\n",
      "8272.24977898441\n",
      "8271.86662489642\n",
      "8271.48014194599\n",
      "8271.090295705093\n",
      "8270.697051333016\n",
      "8270.300373571183\n",
      "8269.900226737942\n",
      "8269.496574723285\n",
      "8269.089380983529\n",
      "8268.678608535916\n",
      "8268.264219953193\n",
      "8267.846177358098\n",
      "8267.424442417829\n",
      "8266.998976338422\n",
      "8266.569739859098\n",
      "8266.136693246535\n",
      "8265.699796289095\n",
      "8265.259008290977\n",
      "8264.814288066338\n",
      "8264.365593933328\n",
      "8263.912883708072\n",
      "8263.456114698613\n",
      "8262.995243698755\n",
      "8262.530226981888\n",
      "8262.061020294723\n",
      "8261.587578850967\n",
      "8261.109857324955\n",
      "8260.62780984519\n",
      "8260.141389987852\n",
      "8259.650550770211\n",
      "8259.155244643998\n",
      "8258.655423488699\n",
      "8258.151038604788\n",
      "8257.64204070689\n",
      "8257.128379916889\n",
      "8256.61000575693\n",
      "8256.086867142414\n",
      "8255.558912374872\n",
      "8255.02608913478\n",
      "8254.488344474332\n",
      "8253.945624810087\n",
      "8253.397875915616\n",
      "8252.845042913996\n",
      "8252.287070270302\n",
      "8251.723901783971\n",
      "8251.15548058112\n",
      "8250.58174910678\n",
      "8250.00264911705\n",
      "8249.418121671177\n",
      "8248.82810712355\n",
      "8248.232545115618\n",
      "8247.631374567733\n",
      "8247.024533670894\n",
      "8246.411959878431\n",
      "8245.793589897585\n",
      "8245.169359681\n",
      "8244.539204418157\n",
      "8243.903058526692\n",
      "8243.260855643624\n",
      "8242.612528616515\n",
      "8241.95800949453\n",
      "8241.297229519385\n",
      "8240.630119116227\n",
      "8239.956607884413\n",
      "8239.276624588176\n",
      "8238.590097147211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8237.896952627161\n",
      "8237.197117229993\n",
      "8236.490516284264\n",
      "8235.777074235319\n",
      "8235.056714635339\n",
      "8234.329360133315\n",
      "8233.594932464906\n",
      "8232.85335244217\n",
      "8232.104539943208\n",
      "8231.348413901687\n",
      "8230.584892296249\n",
      "8229.813892139788\n",
      "8229.03532946864\n",
      "8228.249119331633\n",
      "8227.455175779025\n",
      "8226.653411851312\n",
      "8225.843739567918\n",
      "8225.026069915766\n",
      "8224.200312837711\n",
      "8223.366377220847\n",
      "8222.524170884695\n",
      "8221.67360056924\n",
      "8220.81457192286\n",
      "8219.946989490098\n",
      "8219.070756699308\n",
      "8218.185775850177\n",
      "8217.291948101083\n",
      "8216.389173456324\n",
      "8215.477350753223\n",
      "8214.556377649065\n",
      "8213.62615060791\n",
      "8212.686564887244\n",
      "8211.73751452451\n",
      "8210.778892323464\n",
      "8209.810589840406\n",
      "8208.832497370257\n",
      "8207.844503932482\n",
      "8206.84649725687\n",
      "8205.83836376917\n",
      "8204.819988576557\n",
      "8203.79125545298\n",
      "8202.75204682433\n",
      "8201.702243753476\n",
      "8200.641725925148\n",
      "8199.570371630669\n",
      "8198.48805775254\n",
      "8197.394659748876\n",
      "8196.290051637694\n",
      "8195.174105981063\n",
      "8194.046693869099\n",
      "8192.907684903823\n",
      "8191.756947182878\n",
      "8190.594347283104\n",
      "8189.419750243975\n",
      "8188.233019550902\n",
      "8187.034017118409\n",
      "8185.82260327317\n",
      "8184.598636736922\n",
      "8183.361974609263\n",
      "8182.112472350313\n",
      "8180.8499837632835\n",
      "8179.574360976911\n",
      "8178.285454427796\n",
      "8176.983112842646\n",
      "8175.6671832203965\n",
      "8174.3375108142745\n",
      "8172.993939113745\n",
      "8171.636309826395\n",
      "8170.264462859739\n",
      "8168.878236302955\n",
      "8167.477466408567\n",
      "8166.061987574067\n",
      "8164.631632323499\n",
      "8163.186231289003\n",
      "8161.725613192331\n",
      "8160.249604826354\n",
      "8158.758031036541\n",
      "8157.250714702454\n",
      "8155.727476719254\n",
      "8154.188135979215\n",
      "8152.632509353285\n",
      "8151.060411672689\n",
      "8149.471655710576\n",
      "8147.8660521637485\n",
      "8146.243409634471\n",
      "8144.603534612369\n",
      "8142.946231456438\n",
      "8141.271302377179\n",
      "8139.578547418865\n",
      "8137.867764441971\n",
      "8136.1387491057685\n",
      "8134.391294851095\n",
      "8132.625192883344\n",
      "8130.840232155655\n",
      "8129.036199352364\n",
      "8127.212878872686\n",
      "8125.370052814682\n",
      "8123.507500959524\n",
      "8121.625000756067\n",
      "8119.7223273057525\n",
      "8117.799253347876\n",
      "8115.855549245226\n",
      "8113.890982970114\n",
      "8111.905320090837\n",
      "8109.8983237585835\n",
      "8107.869754694785\n",
      "8105.819371178997\n",
      "8103.746929037257\n",
      "8101.652181631009\n",
      "8099.534879846586\n",
      "8097.394772085284\n",
      "8095.231604254054\n",
      "8093.045119756854\n",
      "8090.8350594866515\n",
      "8088.601161818148\n",
      "8086.343162601228\n",
      "8084.060795155163\n",
      "8081.75379026361\n",
      "8079.42187617043\n",
      "8077.064778576344\n",
      "8074.682220636489\n",
      "8072.273922958864\n",
      "8069.83960360373\n",
      "8067.378978083981\n",
      "8064.891759366514\n",
      "8062.377657874651\n",
      "8059.836381491615\n",
      "8057.2676355651165\n",
      "8054.671122913078\n",
      "8052.046543830519\n",
      "8049.393596097643\n",
      "8046.71197498917\n",
      "8044.00137328492\n",
      "8041.261481281703\n",
      "8038.491986806544\n",
      "8035.692575231279\n",
      "8032.862929488527\n",
      "8030.002730089117\n",
      "8027.111655140967\n",
      "8024.18938036945\n",
      "8021.235579139304\n",
      "8018.2499224780695\n",
      "8015.232079101139\n",
      "8012.181715438413\n",
      "8009.098495662591\n",
      "8005.982081719146\n",
      "8002.832133357995\n",
      "7999.648308166892\n",
      "7996.430261606578\n",
      "7993.177647047701\n",
      "7989.890115809536\n",
      "7986.567317200526\n",
      "7983.208898560672\n",
      "7979.8145053057615\n",
      "7976.383780973505\n",
      "7972.916367271542\n",
      "7969.411904127379\n",
      "7965.870029740228\n",
      "7962.290380634802\n",
      "7958.672591717042\n",
      "7955.0162963318\n",
      "7951.321126322475\n",
      "7947.586712092633\n",
      "7943.812682669562\n",
      "7939.998665769814\n",
      "7936.1442878667085\n",
      "7932.24917425978\n",
      "7928.312949146198\n",
      "7924.335235694114\n",
      "7920.315656117959\n",
      "7916.253831755637\n",
      "7912.149383147658\n",
      "7908.00193011812\n",
      "7903.811091857586\n",
      "7899.576487007793\n",
      "7895.297733748184\n",
      "7890.974449884241\n",
      "7886.606252937563\n",
      "7882.192760237698\n",
      "7877.733589015662\n",
      "7873.228356499118\n",
      "7868.676680009201\n",
      "7864.078177058899\n",
      "7859.432465453\n",
      "7854.739163389524\n",
      "7849.997889562621\n",
      "7845.208263266857\n",
      "7840.369904502872\n",
      "7835.482434084327\n",
      "7830.545473746111\n",
      "7825.558646253736\n",
      "7820.521575513874\n",
      "7815.433886685976\n",
      "7810.2952062949025\n",
      "7805.105162344524\n",
      "7799.8633844322085\n",
      "7794.569503864153\n",
      "7789.223153771477\n",
      "7783.823969227004\n",
      "7778.371587362706\n",
      "7772.86564748769\n",
      "7767.305791206684\n",
      "7761.691662538959\n",
      "7756.022908037577\n",
      "7750.299176908948\n",
      "7744.520121132567\n",
      "7738.685395580882\n",
      "7732.79465813922\n",
      "7726.847569825677\n",
      "7720.8437949108975\n",
      "7714.783001037666\n",
      "7708.66485934022\n",
      "7702.489044563212\n",
      "7696.25523518022\n",
      "7689.963113511723\n",
      "7683.612365842466\n",
      "7677.2026825380835\n",
      "7670.733758160945\n",
      "7664.205291585082\n",
      "7657.6169861101025\n",
      "7650.968549574023\n",
      "7644.259694464879\n",
      "7637.490138031035\n",
      "7630.659602390064\n",
      "7623.767814636122\n",
      "7616.814506945663\n",
      "7609.799416681408\n",
      "7602.722286494443\n",
      "7595.582864424337\n",
      "7588.380903997119\n",
      "7581.11616432106\n",
      "7573.788410180061\n",
      "7566.39741212456\n",
      "7558.94294655983\n",
      "7551.424795831503\n",
      "7543.842748308233\n",
      "7536.196598461316\n",
      "7528.486146941166\n",
      "7520.711200650498\n",
      "7512.871572814067\n",
      "7504.96708304485\n",
      "7496.9975574065265\n",
      "7488.962828472102\n",
      "7480.862735378554\n",
      "7472.6971238773785\n",
      "7464.465846380872\n",
      "7456.168762004045\n",
      "7447.805736602031\n",
      "7439.376642802854\n",
      "7430.881360035454\n",
      "7422.319774552845\n",
      "7413.691779450275\n",
      "7404.997274678318\n",
      "7396.236167050758\n",
      "7387.408370247199\n",
      "7378.513804810296\n",
      "7369.55239813753\n",
      "7360.524084467459\n",
      "7351.428804860374\n",
      "7342.266507173314\n",
      "7333.037146029372\n",
      "7323.740682781289\n",
      "7314.377085469282\n",
      "7304.946328773102\n",
      "7295.448393958326\n",
      "7285.883268816868\n",
      "7276.250947601771\n",
      "7266.551430956268\n",
      "7256.78472583718\n",
      "7246.950845432724\n",
      "7237.049809074776\n",
      "7227.081642145703\n",
      "7217.04637597984\n",
      "7206.94404775976\n",
      "7196.774700407436\n",
      "7186.538382470472\n",
      "7176.23514800353\n",
      "7165.865056445175\n",
      "7155.428172490281\n",
      "7144.924565958253\n",
      "7134.354311657247\n",
      "7123.71748924467\n",
      "7113.014183084174\n",
      "7102.244482099445\n",
      "7091.408479625072\n",
      "7080.506273254783\n",
      "7069.537964687383\n",
      "7058.5036595707115\n",
      "7047.403467344005\n",
      "7036.23750107898\n",
      "7025.005877320058\n",
      "7013.708715924113\n",
      "7002.34613990015\n",
      "6990.918275249334\n",
      "6979.425250805838\n",
      "6967.867198078915\n",
      "6956.244251096704\n",
      "6944.55654625222\n",
      "6932.804222152033\n",
      "6920.987419468143\n",
      "6909.106280793536\n",
      "6897.160950501995\n",
      "6885.15157461265\n",
      "6873.078300659847\n",
      "6860.941277568867\n",
      "6848.740655538072\n",
      "6836.476585928029\n",
      "6824.149221158184\n",
      "6811.75871461168\n",
      "6799.305220548882\n",
      "6786.7888940301855\n",
      "6774.209890848715\n",
      "6761.568367473465\n",
      "6748.8644810034775\n",
      "6736.098389133641\n",
      "6723.270250132642\n",
      "6710.380222833683\n",
      "6697.428466638474\n",
      "6684.415141535062\n",
      "6671.340408130016\n",
      "6658.204427695484\n",
      "6645.0073622316095\n",
      "6631.749374544774\n",
      "6618.430628342137\n",
      "6605.051288342862\n",
      "6591.61152040645\n",
      "6578.1114916785245\n",
      "6564.551370754412\n",
      "6550.9313278607915\n",
      "6537.25153505567\n",
      "6523.512166446892\n",
      "6509.713398429343\n",
      "6495.855409940947\n",
      "6481.9383827375295\n",
      "6467.962501686528\n",
      "6453.927955079511\n",
      "6439.83493496337\n",
      "6425.683637490001\n",
      "6411.474263284243\n",
      "6397.207017829704\n",
      "6382.88211187213\n",
      "6368.499761839811\n",
      "6354.060190280486\n",
      "6339.563626314117\n",
      "6325.010306100824\n",
      "6310.400473323203\n",
      "6295.734379682124\n",
      "6281.012285405093\n",
      "6266.234459766107\n",
      "6251.4011816159045\n",
      "6236.512739921402\n",
      "6221.569434313034\n",
      "6206.5715756386335\n",
      "6191.519486522445\n",
      "6176.41350192772\n",
      "6161.253969721364\n",
      "6146.041251238972\n",
      "6130.775721848555\n",
      "6115.45777151121\n",
      "6100.087805336938\n",
      "6084.666244133756\n",
      "6069.19352494823\n",
      "6053.670101595518\n",
      "6038.096445177019\n",
      "6022.47304458365\n",
      "6006.800406982868\n",
      "5991.079058287436\n",
      "5975.309543604087\n",
      "5959.492427660127\n",
      "5943.628295206162\n",
      "5927.717751393113\n",
      "5911.7614221217555\n",
      "5895.759954363097\n",
      "5879.714016447942\n",
      "5863.624298324118\n",
      "5847.4915117798855\n",
      "5831.316390632181\n",
      "5815.09969087844\n",
      "5798.84219081084\n",
      "5782.54469109196\n",
      "5766.208014790947\n",
      "5749.833007379422\n",
      "5733.420536686499\n",
      "5716.971492812423\n",
      "5700.486788000477\n",
      "5683.96735646695\n",
      "5667.414154189116\n",
      "5650.828158651306\n",
      "5634.210368549315\n",
      "5617.561803453515\n",
      "5600.883503431211\n",
      "5584.176528628896\n",
      "5567.441958815225\n",
      "5550.680892885621\n",
      "5533.8944483296\n",
      "5517.083760662004\n",
      "5500.249982819431\n",
      "5483.3942845233\n",
      "5466.517851611057\n",
      "5449.621885337156\n",
      "5432.707601645489\n",
      "5415.776230415093\n",
      "5398.829014680933\n",
      "5381.867209831749\n",
      "5364.892082786864\n",
      "5347.904911154019\n",
      "5330.906982370281\n",
      "5313.899592828075\n",
      "5296.884046988464\n",
      "5279.861656483775\n",
      "5262.833739211661\n",
      "5245.801618422729\n",
      "5228.766621803777\n",
      "5211.730080558727\n",
      "5194.693328489282\n",
      "5177.657701077263\n",
      "5160.624534570611\n",
      "5143.595165074919\n",
      "5126.570927652339\n",
      "5109.553155429633\n",
      "5092.543178717071\n",
      "5075.542324139809\n",
      "5058.5519137833135\n",
      "5041.573264354276\n",
      "5024.607686358446\n",
      "5007.656483296665\n",
      "4990.720950880335\n",
      "4973.802376267429\n",
      "4956.902037320109\n",
      "4940.021201884879\n",
      "4923.161127096129\n",
      "4906.323058703847\n",
      "4889.508230426153\n",
      "4872.717863327254\n",
      "4855.953165221303\n",
      "4839.215330102565\n",
      "4822.505537602234\n",
      "4805.824952472109\n",
      "4789.174724095303\n",
      "4772.555986024073\n",
      "4755.969855544761\n",
      "4739.417433269795\n",
      "4722.899802756597\n",
      "4706.418030153234\n",
      "4689.973163870524\n",
      "4673.566234280291\n",
      "4657.198253439405\n",
      "4640.870214839193\n",
      "4624.5830931797445\n",
      "4608.3378441686245\n",
      "4592.135404343424\n",
      "4575.976690917613\n",
      "4559.86260164905\n",
      "4543.794014730533\n",
      "4527.771788701733\n",
      "4511.796762381842\n",
      "4495.869754822212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4479.991565278326\n",
      "4464.162973200336\n",
      "4448.384738241488\n",
      "4432.657600283667\n",
      "4416.982279479367\n",
      "4401.359476309319\n",
      "4385.789871655095\n",
      "4370.2741268859245\n",
      "4354.812883959026\n",
      "4339.4067655327635\n",
      "4324.056375091891\n",
      "4308.762297084244\n",
      "4293.525097068185\n",
      "4278.345321870129\n",
      "4263.2234997515625\n",
      "4248.16014058486\n",
      "4233.155736037345\n",
      "4218.210759762983\n",
      "4203.325667601124\n",
      "4188.500897781762\n",
      "4173.736871136771\n",
      "4159.033991316577\n",
      "4144.39264501182\n",
      "4129.813202179463\n",
      "4115.296016272963\n",
      "4100.841424475999\n",
      "4086.449747939384\n",
      "4072.1212920207345\n",
      "4057.85634652653\n",
      "4043.6551859561905\n",
      "4029.51806974783\n",
      "4015.445242525351\n",
      "4001.436934346589\n",
      "3987.4933609521745\n",
      "3973.6147240148784\n",
      "3959.8012113891477\n",
      "3946.052997360588\n",
      "3932.3702428951756\n",
      "3918.7530958879606\n",
      "3905.20169141107\n",
      "3891.7161519608085\n",
      "3878.2965877036877\n",
      "3864.9430967212074\n",
      "3851.6557652532347\n",
      "3838.434667939844\n",
      "3825.27986806147\n",
      "3812.1914177772605\n",
      "3799.169358361507\n",
      "3786.213720438055\n",
      "3773.3245242125895\n",
      "3760.5017797027026\n",
      "3747.7454869656835\n",
      "3735.0556363239293\n",
      "3722.432208587926\n",
      "3709.8751752767407\n",
      "3697.38449883596\n",
      "3684.9601328530357\n",
      "3672.60202226999\n",
      "3660.310103593444\n",
      "3648.084305101943\n",
      "3635.9245470505334\n",
      "3623.8307418725853\n",
      "3611.802794378836\n",
      "3599.8406019536314\n",
      "3587.944054748357\n",
      "3576.1130358720557\n",
      "3564.3474215792144\n",
      "3552.6470814547256\n",
      "3541.0118785960203\n",
      "3529.441669792369\n",
      "3517.936305701367\n",
      "3506.4956310225944\n",
      "3495.119484668481\n",
      "3483.8076999323575\n",
      "3472.560104653735\n",
      "3461.3765213808024\n",
      "3450.256767530176\n",
      "3439.2006555439\n",
      "3428.207993043732\n",
      "3417.278582982719\n",
      "3406.412223794093\n",
      "3395.608709537506\n",
      "3384.867830042618\n",
      "3374.1893710500767\n",
      "3363.573114349892\n",
      "3353.0188379172496\n",
      "best accuracy: 0.9484166666666667\n",
      "best loss: 3353.0188379172496\n",
      "best epoch: 999\n",
      "dev set accuracy: 0.958\n",
      "test set accuracy: 0.502\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute pipeline\n",
    "execute_pipeline(data_path, idx, h1, h2, lr, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
