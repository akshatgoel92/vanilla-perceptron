{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "%reset -f\n",
    "from helpers import load_all_data, vectorized_flatten, sigmoid, get_log_loss, get_accuracy \n",
    "from helpers import sigmoid_derivative, gradient_update, plot_loss, prep_data, get_best_results, get_results\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X, h1, h2): \n",
    "    '''\n",
    "    --------------------\n",
    "    Parameter Initialization\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = nxm)\n",
    "    --------------------\n",
    "    Output: \n",
    "    weights: Weight terms initialized as random normals\n",
    "    biases: Bias terms initialized to zero\n",
    "    --------------------\n",
    "    '''\n",
    "    dim1 = 1/np.sqrt(X.shape[0])\n",
    "    W1 = dim1 * np.random.randn(h1, 28**2)\n",
    "    \n",
    "    dim2 = 1/np.sqrt(W1.shape[1])\n",
    "    W2 = dim2 * np.random.randn(h2, h1)\n",
    "    \n",
    "    dim3 = 1/np.sqrt(W2.shape[1])\n",
    "    W3 = dim3 * np.random.randn(1, h2)\n",
    "\n",
    "    b1 = np.zeros((h1, 1))\n",
    "    b2 = np.zeros((h2, 1))\n",
    "    b3 = np.zeros((1, 1))\n",
    "    \n",
    "    weights = [W1, W2, W3]\n",
    "    biases = [b1, b2, b3]\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, weights, biases):\n",
    "    '''\n",
    "    ----------------------------------\n",
    "    Forward propogation:\n",
    "    Send inputs through the network to\n",
    "    generate output\n",
    "    ----------------------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = nxm)\n",
    "    weights: List of weights \n",
    "    biases: List of biases\n",
    "    --------------------\n",
    "    Output: \n",
    "    activations: List of forward pass terms\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    \n",
    "    z1 = W1 @ X + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    activations = [z1, a1, z2, a2, z3, a3]\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, weights, biases, activations):\n",
    "    '''\n",
    "    --------------------\n",
    "    Backpropagation\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Numpy array of training features (shape = nxm)\n",
    "    y: Binary (1/0) training label (shape = 1xm)\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    activations: Current set of activations\n",
    "    --------------------\n",
    "    Output: \n",
    "    Gradients required\n",
    "    for optimization update\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    z1, a1, z2, a2, z3, a3 = activations\n",
    "    m = max(y.shape)\n",
    "    # print(m)\n",
    "    \n",
    "    dz3 = (a3 - y)/m\n",
    "    # print(\"dz3\", dz3.shape)\n",
    "    \n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    # print(\"dW3\", dW3.shape)\n",
    "    \n",
    "    db3 = np.sum(dz3, axis=1).reshape(-1, 1)\n",
    "    # print(\"db3\", db3.shape)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    # print(\"da2\", da2.shape)\n",
    "    \n",
    "    dz2 = da2 * sigmoid_derivative(z2)\n",
    "    # print(\"dz2\", dz2.shape)\n",
    "    \n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    # print(\"dW2\", dW2.shape)\n",
    "    \n",
    "    db2 = np.sum(dz2, axis=1).reshape(-1, 1)\n",
    "    # print(\"db2\", db2.shape)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    # print(\"da1\", da1.shape)\n",
    "    \n",
    "    dz1 = da1 * sigmoid_derivative(z1)\n",
    "    # print(\"dz1\", dz1.shape)\n",
    "    \n",
    "    dW1 = np.dot(dz1, X.T)\n",
    "    # print(\"dW1\", dW1.shape)\n",
    "    \n",
    "    db1 = np.sum(dz1, axis=1).reshape(-1, 1)\n",
    "    # print(\"db1\", db1.shape)\n",
    "    \n",
    "    return db1, dW1, db2, dW2, db3, dW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    '''\n",
    "    --------------------\n",
    "    Update parameters\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    gradients: Current set of gradients\n",
    "    learning_rate: parameter to guide SGD step size\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    \n",
    "    db1, dW1, db2, dW2, db3, dW3 = gradients\n",
    "    \n",
    "    W1 = gradient_update(W1, learning_rate, dW1)\n",
    "    W2 = gradient_update(W2, learning_rate, dW2)\n",
    "    W3 = gradient_update(W3, learning_rate, dW3)\n",
    "   \n",
    "    b1 = gradient_update(b1, learning_rate, db1)\n",
    "    b2 = gradient_update(b2, learning_rate, db2)\n",
    "    b3 = gradient_update(b3, learning_rate, db3)\n",
    "    \n",
    "    weights = [W1, W2, W3]\n",
    "    biases = [b1, b2, b3]\n",
    "    \n",
    "    return [weights, biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_differences(example, truth, weights, biases, delta_h=1e-7):\n",
    "    '''\n",
    "    --------------------\n",
    "    Calculate gradients using finite differences. \n",
    "    Calculate norm of differences between finite difference gradients and backpropogation gradients.\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    example: Some training example\n",
    "    truth: Label of that training example\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    delta_h: Finite differences epsilon term\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    W1, W2, W3 = weights\n",
    "    b1, b2, b3 = biases\n",
    "    \n",
    "    activations = forward_pass(example, weights, biases)\n",
    "    db1, dW1, db2, dW2, db3, dW3 = backpropagation(example, truth, weights, biases, activations)\n",
    "    \n",
    "    \n",
    "    weight_gradients = [dW1, dW2, dW3]\n",
    "    bias_gradients = [db1, db2, db3]\n",
    "    \n",
    "    deltaW_lst = []\n",
    "    weight_differences = []\n",
    "    for l in range(len(weights)):\n",
    "        I, J = weights[l].shape \n",
    "        \n",
    "\n",
    "        deltaW_lst.append(np.zeros((I, J)))\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "\n",
    "                weights[l][i][j] += delta_h\n",
    "                activations_plus = forward_pass(example, weights, biases)\n",
    "                \n",
    "                weights[l][i][j] -= 2* delta_h\n",
    "                activations_minus = forward_pass(example, weights, biases)\n",
    "                \n",
    "                weights[l][i][j] += delta_h\n",
    "\n",
    "#                 W_minus[i][j] -= delta_h\n",
    "\n",
    "#                 weights_plus = [W1, W2, W_plus] # Change here\n",
    "#                 weights_minus = [W1, W2, W_minus] # Change here\n",
    "\n",
    "\n",
    "\n",
    "#                 weights[l][i][j] += delta_h\n",
    "\n",
    "                loss_plus = get_log_loss(truth, activations_plus[-1])\n",
    "                loss_minus =  get_log_loss(truth, activations_minus[-1])\n",
    "\n",
    "                deltaW_lst[l][i][j] = (loss_plus - loss_minus)/(2 * delta_h)\n",
    "\n",
    "        difference = np.linalg.norm(weight_gradients[l] - deltaW_lst[l]) # Change here\n",
    "        weight_differences.append(difference)\n",
    "        \n",
    "        \n",
    "    deltab_lst = []\n",
    "    bias_differences = []\n",
    "    for l in range(len(biases)):\n",
    "        I, J = biases[l].shape \n",
    "        \n",
    "\n",
    "        deltab_lst.append(np.zeros((I, J)))\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "\n",
    "                biases[l][i][j] += delta_h\n",
    "                activations_plus = forward_pass(example, weights, biases)\n",
    "                \n",
    "                biases[l][i][j] -= 2* delta_h\n",
    "                activations_minus = forward_pass(example, weights, biases)\n",
    "                \n",
    "                biases[l][i][j] += delta_h\n",
    "\n",
    "#                 W_minus[i][j] -= delta_h\n",
    "\n",
    "#                 weights_plus = [W1, W2, W_plus] # Change here\n",
    "#                 weights_minus = [W1, W2, W_minus] # Change here\n",
    "\n",
    "\n",
    "\n",
    "#                 weights[l][i][j] += delta_h\n",
    "\n",
    "                loss_plus = get_log_loss(truth, activations_plus[-1])\n",
    "                loss_minus =  get_log_loss(truth, activations_minus[-1])\n",
    "\n",
    "                deltab_lst[l][i][j] = (loss_plus - loss_minus)/(2 * delta_h)\n",
    "\n",
    "        difference = np.linalg.norm(bias_gradients[l] - deltab_lst[l]) # Change here\n",
    "        bias_differences.append(difference)\n",
    "        \n",
    "    print(\"W3 finite difference vs gradient\")\n",
    "    print(deltaW_lst[-1])\n",
    "    print(dW3)\n",
    "    \n",
    "    print(\"b3 finite difference vs gradient\")\n",
    "    print(deltab_lst[-1])\n",
    "    print(db3)\n",
    "    \n",
    "    print(\"W2 finite difference vs gradient\")\n",
    "    print(deltaW_lst[-2])\n",
    "    print(dW2)\n",
    "    \n",
    "    print(\"b2 finite difference vs gradient\")\n",
    "    print(deltab_lst[-2])\n",
    "    print(db2)\n",
    "    \n",
    "    print(\"W1 finite difference vs gradient\")\n",
    "    print(deltaW_lst[-3])\n",
    "    print(dW1)\n",
    "    \n",
    "    print(\"b1 finite difference vs gradient\")\n",
    "    print(deltab_lst[-3])\n",
    "    print(db1)\n",
    "    \n",
    "    return weight_differences, bias_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finite_differences(X_train_flattened, y_train, weights, biases, idx=10):\n",
    "    '''\n",
    "    --------------------\n",
    "    Run finite differerences\n",
    "    --------------------\n",
    "    Parameters:\n",
    "    weights: Current set of weights\n",
    "    biases: Current set of biases\n",
    "    --------------------\n",
    "    Output: \n",
    "    Updated weights and biases\n",
    "    --------------------\n",
    "    '''\n",
    "    weight_differences, bias_differences = finite_differences(X_train_flattened[:, idx].reshape(-1, 1), y_train[:, idx].reshape(-1, 1), weights, biases)\n",
    "    \n",
    "    print(\"weight_differences\", weight_differences)\n",
    "    print(\"bias_differences\", bias_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, X_val, y_val, w, b, h1, h2, lr, epochs, tolerance=1e-1):    \n",
    "    '''\n",
    "    --------------------\n",
    "    Prepare data\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X: Training set features\n",
    "    y: Training set labels\n",
    "    X_val: Validation set features\n",
    "    y_val: Validation set labels\n",
    "    w: Initialized list of weight matrices\n",
    "    b: Initialized list of bias matrices\n",
    "    h1: Number of neurons in 1st layer\n",
    "    h2: Number of neurons in 2nd layer\n",
    "    lr: Learning rate\n",
    "    epochs: Max epochs\n",
    "    tolerance: Minimum reduction in loss over 10 epochs, used for convergence check\n",
    "    --------------------\n",
    "    Output: \n",
    "    Training History containing metrics and weights/biases at each epoch\n",
    "    --------------------\n",
    "    '''\n",
    "    # Initialize history dictionary\n",
    "    history = {\n",
    "        \"weights\": [w],\n",
    "        \"losses\": [],\n",
    "        \"val_losses\": [],\n",
    "        \"biases\": [b],\n",
    "        \"accuracies\": [],\n",
    "        \"val_accuracies\": []\n",
    "    }\n",
    "    \n",
    "    convergence_counter = 0\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    # Do this for the specified epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Get weights and bias\n",
    "        w = history['weights'][epoch]\n",
    "        b = history['biases'][epoch]\n",
    "        \n",
    "        # Forward pass to get activations\n",
    "        activations = forward_pass(X, w, b)\n",
    "        \n",
    "        # Forward pass to get validation activations\n",
    "        val_activations = forward_pass(X_val, w, b)\n",
    "              \n",
    "        # Backward pass to get gradients\n",
    "        gradients = backpropagation(X, y, w, b, activations)\n",
    "        \n",
    "        # Gradient descent update\n",
    "        w, b = update_parameters(w, b, gradients, lr)\n",
    "    \n",
    "        # Get last layer output\n",
    "        y_prob = activations[-1]\n",
    "        y_val_prob = val_activations[-1]\n",
    "        \n",
    "        # Threshold\n",
    "        y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "        y_val_pred = np.where(y_val_prob > 0.5, 1, 0)\n",
    "    \n",
    "        # Get loss and accuracy results\n",
    "        loss = get_log_loss(y, y_prob)\n",
    "        accuracy = get_accuracy(y, y_pred)\n",
    "        \n",
    "        # Get loss and accuracy results for validation set\n",
    "        val_loss = get_log_loss(y_val, y_val_prob)\n",
    "        val_accuracy = get_accuracy(y_val, y_val_pred)\n",
    "         \n",
    "        # Check convergence, keeps a counter of how many epochs it has been without an improvement\n",
    "        # Counter resets whenever there's an improvent            \n",
    "        if loss < best_loss - tolerance:\n",
    "            best_loss = loss\n",
    "            convergence_counter = 0\n",
    "        else:\n",
    "            convergence_counter += 1\n",
    "\n",
    "        # Append results to history\n",
    "        history[\"losses\"].append(loss)\n",
    "        history[\"val_losses\"].append(val_loss)\n",
    "        history[\"biases\"].append(b)\n",
    "        history[\"weights\"].append(w)\n",
    "        history[\"accuracies\"].append(accuracy)\n",
    "        history[\"val_accuracies\"].append(val_accuracy)\n",
    "        \n",
    "        # 10 epochs without an improvment is considered to have converged\n",
    "        if convergence_counter == 10:\n",
    "            break\n",
    "        \n",
    "        # Display loss for monitoring\n",
    "        print(loss)\n",
    "        \n",
    "        # Stop training if numerical loss underflows\n",
    "        if np.isnan(loss): break\n",
    "        \n",
    "        # Store loss for next epoch\n",
    "        previous_accuracy = accuracy\n",
    "    \n",
    "    # Return statement\n",
    "    return(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(X_dev, y_dev, history, best_epoch, label=\"dev\"):\n",
    "    '''\n",
    "    --------------------\n",
    "    Get metrics given epoch, history and set\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    X_dev: Features matrix\n",
    "    y_dev: labels vector\n",
    "    history: History containing metrics collected during training\n",
    "    best_epoch: Epoch number to index history\n",
    "    label: Specify type of set ie. test, train, dev\n",
    "    --------------------\n",
    "    Output: \n",
    "    Accuracy at specified epoch\n",
    "    --------------------\n",
    "    '''\n",
    "    w = history[\"weights\"][best_epoch]\n",
    "    b = history[\"biases\"][best_epoch]\n",
    "    activations = forward_pass(X_dev, w, b)\n",
    "\n",
    "    y_dev_prob = activations[-1]\n",
    "    y_dev_pred = np.where(y_dev_prob > 0.5, 1, 0)\n",
    "\n",
    "    loss = get_log_loss(y_dev, y_dev_prob)\n",
    "    accuracy = get_accuracy(y_dev, y_dev_pred)\n",
    "    print(f\"{label} set accuracy at best epoch: {accuracy}\")\n",
    "    print(f\"{label} set loss at best epoch: {loss}\")\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(data_path, h1, h2, lr, epochs):\n",
    "    '''\n",
    "    --------------------\n",
    "    Execute Pipeline\n",
    "    --------------------\n",
    "    Parameters: \n",
    "    data_path: Folder where data is stored\n",
    "    h1: Number of neurons in 1st layer\n",
    "    h2: Number of neurons in 2nd layer\n",
    "    lr: Learning rate\n",
    "    epochs: Max number of epochs\n",
    "    --------------------\n",
    "    Output: \n",
    "    Training History containing metrics and weights/biases at each epoch\n",
    "    --------------------\n",
    "    '''\n",
    "    # Set seed for reproducible results\n",
    "    np.random.seed(1252908)\n",
    "\n",
    "    # Get data\n",
    "    X_train_flattened, X_dev_flattened, X_test_flattened, y_train, y_dev, y_test = prep_data(data_path)\n",
    "\n",
    "    # Initialize weights\n",
    "    weights, biases = initialize(X_train_flattened, h1, h2)\n",
    "\n",
    "    # Check finite difference\n",
    "    idx = np.random.choice(range(X_train_flattened.shape[1]))\n",
    "    run_finite_differences(X_train_flattened, y_train, weights, biases, idx=idx)\n",
    "    \n",
    "#     # Check finite difference\n",
    "    idx = np.random.choice(range(X_train_flattened.shape[1]))\n",
    "    run_finite_differences(X_train_flattened, y_train, weights, biases, idx=idx)\n",
    "    \n",
    "    # Now enter training loop\n",
    "    training_history = train(X_train_flattened, y_train, X_dev_flattened, y_dev, weights, biases, h1, h2, lr, epochs)\n",
    "    \n",
    "    # Display plots to monitor whether loss functions are correct shape\n",
    "    plot_loss(\"loss.png\", training_history[\"losses\"][:-2])\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plot_loss(\"accuracy.png\", training_history[\"accuracies\"][:-2], label='Training Accuracy')\n",
    "    \n",
    "    # Display plots to monitor whether loss functions are correct shape\n",
    "    plot_loss(\"val_loss.png\", training_history[\"val_losses\"][:-2], label='Validation Loss')\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plot_loss(\"val_accuracy.png\", training_history[\"val_accuracies\"][:-2], label='Validation Accuracy')\n",
    "    \n",
    "    # Get weights and biases from best training epoch\n",
    "    best_training_epoch, best_training_accuracy, best_training_loss = get_best_results(training_history, metric='val_accuracies')\n",
    "    \n",
    "    # Get dev results\n",
    "    get_results(X_dev_flattened, y_dev, training_history, best_training_epoch)\n",
    "    \n",
    "    # Get test results\n",
    "    get_results(X_test_flattened, y_test, training_history, best_training_epoch, label=\"test\")\n",
    "    \n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to data\n",
    "data_path = '../setup/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set perceptron parameters: architecture, learning rate, and no. of training epochs\n",
    "h1 = 8\n",
    "h2 = 4\n",
    "lr = 0.1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute pipeline\n",
    "history = execute_pipeline(data_path, h1, h2, lr, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
